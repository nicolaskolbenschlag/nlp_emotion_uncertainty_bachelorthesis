MuSe-LSTM-Attention-baseline-model/emotion_recognition/main.py --feature_set bert-4 --emo_dim_set valence --epochs 100 --refresh --n_seeds 20 --seed 314 --predict --attn --rnn_bi --loss tiltedCCC --uncertainty_approach quantile_regression --log_extensive
Constructing dataset and data loader ...
Constructing data from scratch ...
Samples in partitions: (3122, 62, 64)
Input feature dim: 768.
****************************************************************************************************
Using seed "314"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 314]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.6 | Training loss: 0.2497
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.9 | Training loss: 0.2456
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.2348
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.2293
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2431
Epoch:  1 |   [Val] | Loss: 0.1745 | [CCC]:  0.1752 [' 0.1752'] | PCC: 0.1859 ['0.1859'] | RMSE: 0.4540 ['0.4540']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_314_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.7 | Training loss: 0.2249
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.0 | Training loss: 0.2154
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 82.7 | Training loss: 0.1958
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.2125
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2121
Epoch:  2 |   [Val] | Loss: 0.1473 | [CCC]:  0.2207 [' 0.2207'] | PCC: 0.2314 ['0.2314'] | RMSE: 0.5010 ['0.5010']
Epoch:  2 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_314_None_None].pth"!
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.1 | Training loss: 0.1790
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.2 | Training loss: 0.1784
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.8 | Training loss: 0.1946
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1733
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.1839
Epoch:  3 |   [Val] | Loss: 0.1250 | [CCC]:  0.2789 [' 0.2789'] | PCC: 0.2829 ['0.2829'] | RMSE: 0.4659 ['0.4659']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_314_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.0 | Training loss: 0.1590
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 81.3 | Training loss: 0.1606
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 80.9 | Training loss: 0.1578
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.1307
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1587
Epoch:  4 |   [Val] | Loss: 0.1041 | [CCC]:  0.3295 [' 0.3295'] | PCC: 0.3320 ['0.3320'] | RMSE: 0.4463 ['0.4463']
Epoch:  4 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_314_None_None].pth"!
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.2 | Training loss: 0.1434
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.1 | Training loss: 0.1602
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.4 | Training loss: 0.1895
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1360
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1639
Epoch:  5 |   [Val] | Loss: 0.1150 | [CCC]:  0.2990 [' 0.2990'] | PCC: 0.3383 ['0.3383'] | RMSE: 0.4823 ['0.4823']
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.2 | Training loss: 0.1434
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.3 | Training loss: 0.1616
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.4 | Training loss: 0.1371
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1682
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1477
Epoch:  6 |   [Val] | Loss: 0.0966 | [CCC]:  0.3425 [' 0.3425'] | PCC: 0.3500 ['0.3500'] | RMSE: 0.4599 ['0.4599']
Epoch:  6 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_314_None_None].pth"!
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.5 | Training loss: 0.1374
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.1 | Training loss: 0.1341
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.0 | Training loss: 0.1480
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1247
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1396
Epoch:  7 |   [Val] | Loss: 0.1008 | [CCC]:  0.3347 [' 0.3347'] | PCC: 0.3623 ['0.3623'] | RMSE: 0.4142 ['0.4142']
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.5 | Training loss: 0.1339
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 81.4 | Training loss: 0.1263
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 82.2 | Training loss: 0.1273
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1261
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1291
Epoch:  8 |   [Val] | Loss: 0.0910 | [CCC]:  0.3556 [' 0.3556'] | PCC: 0.3594 ['0.3594'] | RMSE: 0.4441 ['0.4441']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_314_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.2 | Training loss: 0.1205
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.1248
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.2 | Training loss: 0.1130
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1078
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1193
Epoch:  9 |   [Val] | Loss: 0.1079 | [CCC]:  0.3089 [' 0.3089'] | PCC: 0.3465 ['0.3465'] | RMSE: 0.4896 ['0.4896']
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.6 | Training loss: 0.1172
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.6 | Training loss: 0.1409
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.2 | Training loss: 0.1176
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1655
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1259
Epoch: 10 |   [Val] | Loss: 0.1213 | [CCC]:  0.2756 [' 0.2756'] | PCC: 0.3441 ['0.3441'] | RMSE: 0.5353 ['0.5353']
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.6 | Training loss: 0.1165
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.6 | Training loss: 0.1144
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.2 | Training loss: 0.1172
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0844
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1155
Epoch: 11 |   [Val] | Loss: 0.1093 | [CCC]:  0.3016 [' 0.3016'] | PCC: 0.3561 ['0.3561'] | RMSE: 0.5819 ['0.5819']
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.8 | Training loss: 0.1168
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.5 | Training loss: 0.0995
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.5 | Training loss: 0.1140
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1176
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1102
Epoch: 12 |   [Val] | Loss: 0.0804 | [CCC]:  0.3934 [' 0.3934'] | PCC: 0.4004 ['0.4004'] | RMSE: 0.4852 ['0.4852']
Epoch: 12 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_314_None_None].pth"!
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.2 | Training loss: 0.0973
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.3 | Training loss: 0.1127
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.8 | Training loss: 0.1068
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0953
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1055
Epoch: 13 |   [Val] | Loss: 0.0824 | [CCC]:  0.3771 [' 0.3771'] | PCC: 0.3801 ['0.3801'] | RMSE: 0.4315 ['0.4315']
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.6 | Training loss: 0.1011
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.0 | Training loss: 0.1142
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.6 | Training loss: 0.0973
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0941
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1040
Epoch: 14 |   [Val] | Loss: 0.0844 | [CCC]:  0.3916 [' 0.3916'] | PCC: 0.3923 ['0.3923'] | RMSE: 0.4612 ['0.4612']
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.1 | Training loss: 0.0953
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.3 | Training loss: 0.1009
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.9 | Training loss: 0.0954
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1021
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.0973
Epoch: 15 |   [Val] | Loss: 0.0865 | [CCC]:  0.3790 [' 0.3790'] | PCC: 0.3891 ['0.3891'] | RMSE: 0.4158 ['0.4158']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.8 | Training loss: 0.1045
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.6 | Training loss: 0.0889
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.2 | Training loss: 0.0877
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1201
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.0941
Epoch: 16 |   [Val] | Loss: 0.0785 | [CCC]:  0.4062 [' 0.4062'] | PCC: 0.4125 ['0.4125'] | RMSE: 0.4703 ['0.4703']
Epoch: 16 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_314_None_None].pth"!
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.7 | Training loss: 0.0942
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.8 | Training loss: 0.0871
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.5 | Training loss: 0.0831
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0691
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.0878
Epoch: 17 |   [Val] | Loss: 0.0886 | [CCC]:  0.3909 [' 0.3909'] | PCC: 0.4084 ['0.4084'] | RMSE: 0.4851 ['0.4851']
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.4 | Training loss: 0.0933
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.9 | Training loss: 0.0921
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.9 | Training loss: 0.0890
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0563
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.0909
Epoch: 18 |   [Val] | Loss: 0.0852 | [CCC]:  0.3760 [' 0.3760'] | PCC: 0.3815 ['0.3815'] | RMSE: 0.4319 ['0.4319']
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.1 | Training loss: 0.0882
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.8 | Training loss: 0.0825
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.0 | Training loss: 0.0774
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0963
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.0829
Epoch: 19 |   [Val] | Loss: 0.0904 | [CCC]:  0.3732 [' 0.3732'] | PCC: 0.4105 ['0.4105'] | RMSE: 0.4749 ['0.4749']
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.0 | Training loss: 0.0875
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.3 | Training loss: 0.0755
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.7 | Training loss: 0.0812
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0405
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0807
Epoch: 20 |   [Val] | Loss: 0.1225 | [CCC]:  0.2772 [' 0.2772'] | PCC: 0.3726 ['0.3726'] | RMSE: 0.6379 ['0.6379']
Epoch: 21 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.0 | Training loss: 0.1159
Epoch: 21 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.9 | Training loss: 0.1045
Epoch: 21 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.6 | Training loss: 0.0804
Epoch: 21 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0887
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.1001
Epoch: 21 |   [Val] | Loss: 0.0920 | [CCC]:  0.3193 [' 0.3193'] | PCC: 0.3616 ['0.3616'] | RMSE: 0.4250 ['0.4250']
Epoch: 22 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.4 | Training loss: 0.0953
Epoch: 22 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.6 | Training loss: 0.0918
Epoch: 22 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.9 | Training loss: 0.0747
Epoch: 22 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0710
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0870
Epoch: 22 |   [Val] | Loss: 0.0792 | [CCC]:  0.4208 [' 0.4208'] | PCC: 0.4347 ['0.4347'] | RMSE: 0.5004 ['0.5004']
Epoch: 22 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_314_None_None].pth"!
Epoch: 23 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.3 | Training loss: 0.0803
Epoch: 23 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.2 | Training loss: 0.0764
Epoch: 23 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.2 | Training loss: 0.0740
Epoch: 23 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0845
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0770
Epoch: 23 |   [Val] | Loss: 0.0709 | [CCC]:  0.4287 [' 0.4287'] | PCC: 0.4492 ['0.4492'] | RMSE: 0.4144 ['0.4144']
Epoch: 23 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_314_None_None].pth"!
Epoch: 24 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.8 | Training loss: 0.0815
Epoch: 24 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.8 | Training loss: 0.0703
Epoch: 24 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.6 | Training loss: 0.0780
Epoch: 24 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0635
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0764
Epoch: 24 |   [Val] | Loss: 0.0837 | [CCC]:  0.4232 [' 0.4232'] | PCC: 0.4308 ['0.4308'] | RMSE: 0.4709 ['0.4709']
Epoch: 25 | Batch:   1 | Lr: 0.00500 | Time used(s): 94.5 | Training loss: 0.0767
Epoch: 25 | Batch:   2 | Lr: 0.00500 | Time used(s): 90.8 | Training loss: 0.0731
Epoch: 25 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.3 | Training loss: 0.0745
Epoch: 25 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0721
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0747
Epoch: 25 |   [Val] | Loss: 0.0792 | [CCC]:  0.3907 [' 0.3907'] | PCC: 0.4022 ['0.4022'] | RMSE: 0.4078 ['0.4078']
Epoch: 26 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.4 | Training loss: 0.0782
Epoch: 26 | Batch:   2 | Lr: 0.00500 | Time used(s): 90.5 | Training loss: 0.0681
Epoch: 26 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.4 | Training loss: 0.0620
Epoch: 26 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1019
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0700
Epoch: 26 |   [Val] | Loss: 0.0852 | [CCC]:  0.3914 [' 0.3914'] | PCC: 0.4073 ['0.4073'] | RMSE: 0.4841 ['0.4841']
Epoch: 27 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.9 | Training loss: 0.0644
Epoch: 27 | Batch:   2 | Lr: 0.00500 | Time used(s): 90.8 | Training loss: 0.0653
Epoch: 27 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.7 | Training loss: 0.0685
Epoch: 27 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1007
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0667
Epoch: 27 |   [Val] | Loss: 0.0879 | [CCC]:  0.3476 [' 0.3476'] | PCC: 0.3843 ['0.3843'] | RMSE: 0.4664 ['0.4664']
Epoch: 28 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.3 | Training loss: 0.0664
Epoch: 28 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.9 | Training loss: 0.0558
Epoch: 28 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.4 | Training loss: 0.0708
Epoch: 28 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0662
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0644
Epoch: 28 |   [Val] | Loss: 0.0790 | [CCC]:  0.3863 [' 0.3863'] | PCC: 0.4041 ['0.4041'] | RMSE: 0.4735 ['0.4735']
Epoch: 29 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.1 | Training loss: 0.0590
Epoch: 29 | Batch:   2 | Lr: 0.00500 | Time used(s): 91.6 | Training loss: 0.0621
Epoch: 29 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.1 | Training loss: 0.0661
Epoch: 29 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0210
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0617
Epoch: 29 |   [Val] | Loss: 0.0723 | [CCC]:  0.4007 [' 0.4007'] | PCC: 0.4047 ['0.4047'] | RMSE: 0.4184 ['0.4184']
Epoch    29: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 30 | Batch:   1 | Lr: 0.00250 | Time used(s): 92.1 | Training loss: 0.0563
Epoch: 30 | Batch:   2 | Lr: 0.00250 | Time used(s): 89.8 | Training loss: 0.0552
Epoch: 30 | Batch:   3 | Lr: 0.00250 | Time used(s): 91.4 | Training loss: 0.0529
Epoch: 30 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0512
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0547
Epoch: 30 |   [Val] | Loss: 0.0779 | [CCC]:  0.3938 [' 0.3938'] | PCC: 0.4066 ['0.4066'] | RMSE: 0.4618 ['0.4618']
Epoch: 31 | Batch:   1 | Lr: 0.00250 | Time used(s): 92.1 | Training loss: 0.0575
Epoch: 31 | Batch:   2 | Lr: 0.00250 | Time used(s): 88.9 | Training loss: 0.0537
Epoch: 31 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.3 | Training loss: 0.0545
Epoch: 31 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0486
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0551
Epoch: 31 |   [Val] | Loss: 0.0741 | [CCC]:  0.4128 [' 0.4128'] | PCC: 0.4148 ['0.4148'] | RMSE: 0.4240 ['0.4240']
Epoch: 32 | Batch:   1 | Lr: 0.00250 | Time used(s): 90.6 | Training loss: 0.0503
Epoch: 32 | Batch:   2 | Lr: 0.00250 | Time used(s): 91.2 | Training loss: 0.0485
Epoch: 32 | Batch:   3 | Lr: 0.00250 | Time used(s): 89.9 | Training loss: 0.0568
Epoch: 32 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0603
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0520
Epoch: 32 |   [Val] | Loss: 0.0788 | [CCC]:  0.3955 [' 0.3955'] | PCC: 0.4062 ['0.4062'] | RMSE: 0.4381 ['0.4381']
Epoch: 33 | Batch:   1 | Lr: 0.00250 | Time used(s): 90.3 | Training loss: 0.0539
Epoch: 33 | Batch:   2 | Lr: 0.00250 | Time used(s): 91.1 | Training loss: 0.0546
Epoch: 33 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.8 | Training loss: 0.0506
Epoch: 33 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0641
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0532
Epoch: 33 |   [Val] | Loss: 0.0739 | [CCC]:  0.4017 [' 0.4017'] | PCC: 0.4019 ['0.4019'] | RMSE: 0.4429 ['0.4429']
Epoch: 34 | Batch:   1 | Lr: 0.00250 | Time used(s): 87.6 | Training loss: 0.0417
Epoch: 34 | Batch:   2 | Lr: 0.00250 | Time used(s): 88.2 | Training loss: 0.0523
Epoch: 34 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.3 | Training loss: 0.0503
Epoch: 34 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0156
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0476
Epoch: 34 |   [Val] | Loss: 0.0822 | [CCC]:  0.3639 [' 0.3639'] | PCC: 0.3823 ['0.3823'] | RMSE: 0.4759 ['0.4759']
Epoch: 35 | Batch:   1 | Lr: 0.00250 | Time used(s): 92.4 | Training loss: 0.0600
Epoch: 35 | Batch:   2 | Lr: 0.00250 | Time used(s): 88.8 | Training loss: 0.0487
Epoch: 35 | Batch:   3 | Lr: 0.00250 | Time used(s): 89.8 | Training loss: 0.0447
Epoch: 35 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0597
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0513
Epoch: 35 |   [Val] | Loss: 0.0807 | [CCC]:  0.3792 [' 0.3792'] | PCC: 0.4122 ['0.4122'] | RMSE: 0.4530 ['0.4530']
Epoch    35: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 36 | Batch:   1 | Lr: 0.00125 | Time used(s): 91.9 | Training loss: 0.0595
Epoch: 36 | Batch:   2 | Lr: 0.00125 | Time used(s): 89.0 | Training loss: 0.0512
Epoch: 36 | Batch:   3 | Lr: 0.00125 | Time used(s): 89.8 | Training loss: 0.0448
Epoch: 36 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0467
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0517
Epoch: 36 |   [Val] | Loss: 0.0767 | [CCC]:  0.3918 [' 0.3918'] | PCC: 0.4154 ['0.4154'] | RMSE: 0.4678 ['0.4678']
Epoch: 37 | Batch:   1 | Lr: 0.00125 | Time used(s): 87.9 | Training loss: 0.0572
Epoch: 37 | Batch:   2 | Lr: 0.00125 | Time used(s): 86.7 | Training loss: 0.0355
Epoch: 37 | Batch:   3 | Lr: 0.00125 | Time used(s): 88.7 | Training loss: 0.0476
Epoch: 37 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0230
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0464
Epoch: 37 |   [Val] | Loss: 0.0815 | [CCC]:  0.4172 [' 0.4172'] | PCC: 0.4271 ['0.4271'] | RMSE: 0.4339 ['0.4339']
Epoch: 38 | Batch:   1 | Lr: 0.00125 | Time used(s): 91.9 | Training loss: 0.0456
Epoch: 38 | Batch:   2 | Lr: 0.00125 | Time used(s): 89.3 | Training loss: 0.0404
Epoch: 38 | Batch:   3 | Lr: 0.00125 | Time used(s): 89.8 | Training loss: 0.0348
Epoch: 38 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0471
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0404
Epoch: 38 |   [Val] | Loss: 0.0802 | [CCC]:  0.4128 [' 0.4128'] | PCC: 0.4165 ['0.4165'] | RMSE: 0.4319 ['0.4319']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 314 | Best [Val CCC]: 0.4287 [' 0.4287']| Loss: 0.0709 | PCC: 0.4492 ['0.4492'] | RMSE: 0.4144 ['0.4144']
On Test: CCC  0.5558 | PCC  0.5624 | RMSE  0.3872
****************************************************************************************************
Seed "314" over!
****************************************************************************************************
****************************************************************************************************
Using seed "315"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 315]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.1 | Training loss: 0.2501
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.5 | Training loss: 0.2441
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.1 | Training loss: 0.2314
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2334
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2417
Epoch:  1 |   [Val] | Loss: 0.2138 | [CCC]:  0.0634 [' 0.0634'] | PCC: 0.1727 ['0.1727'] | RMSE: 0.6374 ['0.6374']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_315_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.2 | Training loss: 0.2244
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.8 | Training loss: 0.2085
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.5 | Training loss: 0.1912
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1632
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2073
Epoch:  2 |   [Val] | Loss: 0.2358 | [CCC]:  0.0281 [' 0.0281'] | PCC: 0.1374 ['0.1374'] | RMSE: 0.6880 ['0.6880']
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.3 | Training loss: 0.2316
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.3 | Training loss: 0.2307
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.1 | Training loss: 0.2128
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1883
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.2245
Epoch:  3 |   [Val] | Loss: 0.1569 | [CCC]:  0.2137 [' 0.2137'] | PCC: 0.2460 ['0.2460'] | RMSE: 0.4216 ['0.4216']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_315_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.6 | Training loss: 0.1886
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.5 | Training loss: 0.1767
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.7 | Training loss: 0.1713
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1272
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1781
Epoch:  4 |   [Val] | Loss: 0.1704 | [CCC]:  0.1767 [' 0.1767'] | PCC: 0.2670 ['0.2670'] | RMSE: 0.6174 ['0.6174']
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.7 | Training loss: 0.1923
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 92.5 | Training loss: 0.1590
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 95.3 | Training loss: 0.1481
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1505
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1662
Epoch:  5 |   [Val] | Loss: 0.1118 | [CCC]:  0.2946 [' 0.2946'] | PCC: 0.3036 ['0.3036'] | RMSE: 0.5436 ['0.5436']
Epoch:  5 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_315_None_None].pth"!
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 95.1 | Training loss: 0.1426
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 93.2 | Training loss: 0.1453
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.3 | Training loss: 0.1421
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1635
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1437
Epoch:  6 |   [Val] | Loss: 0.1100 | [CCC]:  0.3071 [' 0.3071'] | PCC: 0.3304 ['0.3304'] | RMSE: 0.4741 ['0.4741']
Epoch:  6 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_315_None_None].pth"!
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 95.3 | Training loss: 0.1340
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 93.9 | Training loss: 0.1377
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 96.5 | Training loss: 0.1386
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1401
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1368
Epoch:  7 |   [Val] | Loss: 0.0955 | [CCC]:  0.3343 [' 0.3343'] | PCC: 0.3624 ['0.3624'] | RMSE: 0.4740 ['0.4740']
Epoch:  7 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_315_None_None].pth"!
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 96.1 | Training loss: 0.1264
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 94.2 | Training loss: 0.1338
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 95.3 | Training loss: 0.1231
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1453
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1281
Epoch:  8 |   [Val] | Loss: 0.0937 | [CCC]:  0.3446 [' 0.3446'] | PCC: 0.3652 ['0.3652'] | RMSE: 0.4467 ['0.4467']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_315_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 94.8 | Training loss: 0.1183
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 91.5 | Training loss: 0.1234
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.2 | Training loss: 0.1234
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1355
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1219
Epoch:  9 |   [Val] | Loss: 0.1045 | [CCC]:  0.3108 [' 0.3108'] | PCC: 0.3712 ['0.3712'] | RMSE: 0.5114 ['0.5114']
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.9 | Training loss: 0.1304
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 90.0 | Training loss: 0.1081
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 95.0 | Training loss: 0.1395
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1232
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1260
Epoch: 10 |   [Val] | Loss: 0.1087 | [CCC]:  0.3018 [' 0.3018'] | PCC: 0.3709 ['0.3709'] | RMSE: 0.4383 ['0.4383']
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 94.7 | Training loss: 0.1294
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 93.0 | Training loss: 0.1333
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.9 | Training loss: 0.1204
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1048
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1274
Epoch: 11 |   [Val] | Loss: 0.1091 | [CCC]:  0.2996 [' 0.2996'] | PCC: 0.3680 ['0.3680'] | RMSE: 0.5708 ['0.5708']
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 94.6 | Training loss: 0.1315
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 92.2 | Training loss: 0.1075
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.2 | Training loss: 0.1302
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1074
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1228
Epoch: 12 |   [Val] | Loss: 0.0841 | [CCC]:  0.3676 [' 0.3676'] | PCC: 0.3968 ['0.3968'] | RMSE: 0.3989 ['0.3989']
Epoch: 12 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_315_None_None].pth"!
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.4 | Training loss: 0.1131
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 92.0 | Training loss: 0.1207
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 93.8 | Training loss: 0.1231
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1093
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1188
Epoch: 13 |   [Val] | Loss: 0.0927 | [CCC]:  0.3755 [' 0.3755'] | PCC: 0.4111 ['0.4111'] | RMSE: 0.4750 ['0.4750']
Epoch: 13 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_315_None_None].pth"!
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.7 | Training loss: 0.1025
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 92.8 | Training loss: 0.1098
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 93.5 | Training loss: 0.1035
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1212
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1055
Epoch: 14 |   [Val] | Loss: 0.0819 | [CCC]:  0.4020 [' 0.4020'] | PCC: 0.4038 ['0.4038'] | RMSE: 0.4271 ['0.4271']
Epoch: 14 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_315_None_None].pth"!
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.9 | Training loss: 0.0995
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 92.1 | Training loss: 0.1005
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.9 | Training loss: 0.0968
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1111
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.0991
Epoch: 15 |   [Val] | Loss: 0.0917 | [CCC]:  0.3664 [' 0.3664'] | PCC: 0.3984 ['0.3984'] | RMSE: 0.5155 ['0.5155']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.4 | Training loss: 0.1022
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 92.3 | Training loss: 0.0995
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.3 | Training loss: 0.1071
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1283
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.1033
Epoch: 16 |   [Val] | Loss: 0.0821 | [CCC]:  0.3736 [' 0.3736'] | PCC: 0.3798 ['0.3798'] | RMSE: 0.4233 ['0.4233']
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.4 | Training loss: 0.0979
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 90.8 | Training loss: 0.0942
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.0 | Training loss: 0.0843
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0864
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.0920
Epoch: 17 |   [Val] | Loss: 0.0831 | [CCC]:  0.3841 [' 0.3841'] | PCC: 0.3882 ['0.3882'] | RMSE: 0.4811 ['0.4811']
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.3 | Training loss: 0.0854
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.7 | Training loss: 0.0976
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 83.3 | Training loss: 0.0907
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0508
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.0906
Epoch: 18 |   [Val] | Loss: 0.0813 | [CCC]:  0.3906 [' 0.3906'] | PCC: 0.4021 ['0.4021'] | RMSE: 0.4213 ['0.4213']
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.3 | Training loss: 0.0850
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.9 | Training loss: 0.0897
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.1 | Training loss: 0.0812
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0867
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.0853
Epoch: 19 |   [Val] | Loss: 0.0834 | [CCC]:  0.3944 [' 0.3944'] | PCC: 0.3986 ['0.3986'] | RMSE: 0.4682 ['0.4682']
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.5 | Training loss: 0.0763
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.3 | Training loss: 0.0877
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.4 | Training loss: 0.0873
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0852
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0838
Epoch: 20 |   [Val] | Loss: 0.0770 | [CCC]:  0.4140 [' 0.4140'] | PCC: 0.4145 ['0.4145'] | RMSE: 0.4318 ['0.4318']
Epoch: 20 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_315_None_None].pth"!
Epoch: 21 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.7 | Training loss: 0.0763
Epoch: 21 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.8 | Training loss: 0.0802
Epoch: 21 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.4 | Training loss: 0.0832
Epoch: 21 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0365
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0792
Epoch: 21 |   [Val] | Loss: 0.0838 | [CCC]:  0.3888 [' 0.3888'] | PCC: 0.3950 ['0.3950'] | RMSE: 0.4694 ['0.4694']
Epoch: 22 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.1 | Training loss: 0.0761
Epoch: 22 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.3 | Training loss: 0.0795
Epoch: 22 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.1 | Training loss: 0.0788
Epoch: 22 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0695
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0780
Epoch: 22 |   [Val] | Loss: 0.0789 | [CCC]:  0.3941 [' 0.3941'] | PCC: 0.3986 ['0.3986'] | RMSE: 0.4874 ['0.4874']
Epoch: 23 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.0 | Training loss: 0.0747
Epoch: 23 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.4 | Training loss: 0.0776
Epoch: 23 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.4 | Training loss: 0.0756
Epoch: 23 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0581
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0757
Epoch: 23 |   [Val] | Loss: 0.0815 | [CCC]:  0.4134 [' 0.4134'] | PCC: 0.4198 ['0.4198'] | RMSE: 0.4327 ['0.4327']
Epoch: 24 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.8 | Training loss: 0.0671
Epoch: 24 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.1 | Training loss: 0.0784
Epoch: 24 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.8 | Training loss: 0.0698
Epoch: 24 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0632
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0716
Epoch: 24 |   [Val] | Loss: 0.0889 | [CCC]:  0.3647 [' 0.3647'] | PCC: 0.4108 ['0.4108'] | RMSE: 0.4714 ['0.4714']
Epoch: 25 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.7 | Training loss: 0.0909
Epoch: 25 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.9 | Training loss: 0.0679
Epoch: 25 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.4 | Training loss: 0.0805
Epoch: 25 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0750
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0797
Epoch: 25 |   [Val] | Loss: 0.0807 | [CCC]:  0.3991 [' 0.3991'] | PCC: 0.4103 ['0.4103'] | RMSE: 0.4580 ['0.4580']
Epoch: 26 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.2 | Training loss: 0.0782
Epoch: 26 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.2 | Training loss: 0.0718
Epoch: 26 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.0 | Training loss: 0.0663
Epoch: 26 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1134
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0727
Epoch: 26 |   [Val] | Loss: 0.0690 | [CCC]:  0.4018 [' 0.4018'] | PCC: 0.4063 ['0.4063'] | RMSE: 0.4166 ['0.4166']
Epoch    26: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 27 | Batch:   1 | Lr: 0.00250 | Time used(s): 84.1 | Training loss: 0.0600
Epoch: 27 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.6 | Training loss: 0.0679
Epoch: 27 | Batch:   3 | Lr: 0.00250 | Time used(s): 81.9 | Training loss: 0.0637
Epoch: 27 | Batch:   4 | Lr: 0.00250 | Time used(s): 1.9 | Training loss: 0.0977
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0644
Epoch: 27 |   [Val] | Loss: 0.0708 | [CCC]:  0.3952 [' 0.3952'] | PCC: 0.3953 ['0.3953'] | RMSE: 0.4534 ['0.4534']
Epoch: 28 | Batch:   1 | Lr: 0.00250 | Time used(s): 82.1 | Training loss: 0.0656
Epoch: 28 | Batch:   2 | Lr: 0.00250 | Time used(s): 81.0 | Training loss: 0.0591
Epoch: 28 | Batch:   3 | Lr: 0.00250 | Time used(s): 82.0 | Training loss: 0.0646
Epoch: 28 | Batch:   4 | Lr: 0.00250 | Time used(s): 1.9 | Training loss: 0.0583
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0631
Epoch: 28 |   [Val] | Loss: 0.0782 | [CCC]:  0.3834 [' 0.3834'] | PCC: 0.3845 ['0.3845'] | RMSE: 0.4679 ['0.4679']
Epoch: 29 | Batch:   1 | Lr: 0.00250 | Time used(s): 82.5 | Training loss: 0.0624
Epoch: 29 | Batch:   2 | Lr: 0.00250 | Time used(s): 80.9 | Training loss: 0.0573
Epoch: 29 | Batch:   3 | Lr: 0.00250 | Time used(s): 82.8 | Training loss: 0.0628
Epoch: 29 | Batch:   4 | Lr: 0.00250 | Time used(s): 1.9 | Training loss: 0.0270
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0603
Epoch: 29 |   [Val] | Loss: 0.0724 | [CCC]:  0.3901 [' 0.3901'] | PCC: 0.3937 ['0.3937'] | RMSE: 0.4237 ['0.4237']
Epoch: 30 | Batch:   1 | Lr: 0.00250 | Time used(s): 82.3 | Training loss: 0.0515
Epoch: 30 | Batch:   2 | Lr: 0.00250 | Time used(s): 81.3 | Training loss: 0.0505
Epoch: 30 | Batch:   3 | Lr: 0.00250 | Time used(s): 81.5 | Training loss: 0.0612
Epoch: 30 | Batch:   4 | Lr: 0.00250 | Time used(s): 1.9 | Training loss: 0.0902
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0550
Epoch: 30 |   [Val] | Loss: 0.0742 | [CCC]:  0.3896 [' 0.3896'] | PCC: 0.3944 ['0.3944'] | RMSE: 0.4866 ['0.4866']
Epoch: 31 | Batch:   1 | Lr: 0.00250 | Time used(s): 82.6 | Training loss: 0.0634
Epoch: 31 | Batch:   2 | Lr: 0.00250 | Time used(s): 81.3 | Training loss: 0.0577
Epoch: 31 | Batch:   3 | Lr: 0.00250 | Time used(s): 81.1 | Training loss: 0.0423
Epoch: 31 | Batch:   4 | Lr: 0.00250 | Time used(s): 1.9 | Training loss: 0.0174
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0539
Epoch: 31 |   [Val] | Loss: 0.0861 | [CCC]:  0.3551 [' 0.3551'] | PCC: 0.3811 ['0.3811'] | RMSE: 0.4833 ['0.4833']
Epoch: 32 | Batch:   1 | Lr: 0.00250 | Time used(s): 83.4 | Training loss: 0.0654
Epoch: 32 | Batch:   2 | Lr: 0.00250 | Time used(s): 82.5 | Training loss: 0.0529
Epoch: 32 | Batch:   3 | Lr: 0.00250 | Time used(s): 83.4 | Training loss: 0.0591
Epoch: 32 | Batch:   4 | Lr: 0.00250 | Time used(s): 1.9 | Training loss: 0.0403
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0588
Epoch: 32 |   [Val] | Loss: 0.0917 | [CCC]:  0.3471 [' 0.3471'] | PCC: 0.3801 ['0.3801'] | RMSE: 0.5332 ['0.5332']
Epoch    32: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 33 | Batch:   1 | Lr: 0.00125 | Time used(s): 83.5 | Training loss: 0.0702
Epoch: 33 | Batch:   2 | Lr: 0.00125 | Time used(s): 82.6 | Training loss: 0.0805
Epoch: 33 | Batch:   3 | Lr: 0.00125 | Time used(s): 84.3 | Training loss: 0.0477
Epoch: 33 | Batch:   4 | Lr: 0.00125 | Time used(s): 1.9 | Training loss: 0.0640
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0661
Epoch: 33 |   [Val] | Loss: 0.0763 | [CCC]:  0.3849 [' 0.3849'] | PCC: 0.4028 ['0.4028'] | RMSE: 0.4426 ['0.4426']
Epoch: 34 | Batch:   1 | Lr: 0.00125 | Time used(s): 83.2 | Training loss: 0.0542
Epoch: 34 | Batch:   2 | Lr: 0.00125 | Time used(s): 83.2 | Training loss: 0.0584
Epoch: 34 | Batch:   3 | Lr: 0.00125 | Time used(s): 82.8 | Training loss: 0.0490
Epoch: 34 | Batch:   4 | Lr: 0.00125 | Time used(s): 1.9 | Training loss: 0.0585
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0539
Epoch: 34 |   [Val] | Loss: 0.0746 | [CCC]:  0.3821 [' 0.3821'] | PCC: 0.3858 ['0.3858'] | RMSE: 0.4682 ['0.4682']
Epoch: 35 | Batch:   1 | Lr: 0.00125 | Time used(s): 83.9 | Training loss: 0.0474
Epoch: 35 | Batch:   2 | Lr: 0.00125 | Time used(s): 82.0 | Training loss: 0.0492
Epoch: 35 | Batch:   3 | Lr: 0.00125 | Time used(s): 84.1 | Training loss: 0.0498
Epoch: 35 | Batch:   4 | Lr: 0.00125 | Time used(s): 1.9 | Training loss: 0.0658
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0490
Epoch: 35 |   [Val] | Loss: 0.0776 | [CCC]:  0.3937 [' 0.3937'] | PCC: 0.4009 ['0.4009'] | RMSE: 0.4537 ['0.4537']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 315 | Best [Val CCC]: 0.4140 [' 0.4140']| Loss: 0.0770 | PCC: 0.4145 ['0.4145'] | RMSE: 0.4318 ['0.4318']
On Test: CCC  0.5731 | PCC  0.5914 | RMSE  0.3924
****************************************************************************************************
Seed "315" over!
****************************************************************************************************
****************************************************************************************************
Using seed "316"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 316]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 83.3 | Training loss: 0.2508
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 82.9 | Training loss: 0.2465
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.1 | Training loss: 0.2399
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.2329
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2455
Epoch:  1 |   [Val] | Loss: 0.1773 | [CCC]:  0.1812 [' 0.1812'] | PCC: 0.2058 ['0.2058'] | RMSE: 0.4304 ['0.4304']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_316_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 82.9 | Training loss: 0.2166
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 82.0 | Training loss: 0.2114
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 83.3 | Training loss: 0.2220
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.1976
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2164
Epoch:  2 |   [Val] | Loss: 0.2217 | [CCC]:  0.0577 [' 0.0577'] | PCC: 0.1837 ['0.1837'] | RMSE: 0.8865 ['0.8865']
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 84.7 | Training loss: 0.2283
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 82.1 | Training loss: 0.1905
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 83.1 | Training loss: 0.2105
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.2252
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.2100
Epoch:  3 |   [Val] | Loss: 0.1842 | [CCC]:  0.1761 [' 0.1761'] | PCC: 0.2236 ['0.2236'] | RMSE: 0.4275 ['0.4275']
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 83.4 | Training loss: 0.1997
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 82.2 | Training loss: 0.1960
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.3 | Training loss: 0.1845
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.1865
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1933
Epoch:  4 |   [Val] | Loss: 0.1268 | [CCC]:  0.2912 [' 0.2912'] | PCC: 0.3003 ['0.3003'] | RMSE: 0.5590 ['0.5590']
Epoch:  4 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_316_None_None].pth"!
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 84.2 | Training loss: 0.1668
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 81.6 | Training loss: 0.1687
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 82.1 | Training loss: 0.1669
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.1766
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1676
Epoch:  5 |   [Val] | Loss: 0.1165 | [CCC]:  0.3066 [' 0.3066'] | PCC: 0.3356 ['0.3356'] | RMSE: 0.4812 ['0.4812']
Epoch:  5 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_316_None_None].pth"!
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 128.0 | Training loss: 0.1540
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 140.4 | Training loss: 0.1850
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 140.1 | Training loss: 0.1558
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1847
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1652
Epoch:  6 |   [Val] | Loss: 0.1390 | [CCC]:  0.2249 [' 0.2249'] | PCC: 0.3275 ['0.3275'] | RMSE: 0.6426 ['0.6426']
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 149.8 | Training loss: 0.1672
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 145.2 | Training loss: 0.1513
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 142.2 | Training loss: 0.1439
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1679
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1543
Epoch:  7 |   [Val] | Loss: 0.1026 | [CCC]:  0.3243 [' 0.3243'] | PCC: 0.3379 ['0.3379'] | RMSE: 0.4612 ['0.4612']
Epoch:  7 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_316_None_None].pth"!
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 147.1 | Training loss: 0.1319
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 141.6 | Training loss: 0.1419
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 139.7 | Training loss: 0.1429
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1153
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1385
Epoch:  8 |   [Val] | Loss: 0.0967 | [CCC]:  0.3436 [' 0.3436'] | PCC: 0.3490 ['0.3490'] | RMSE: 0.4592 ['0.4592']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_316_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 148.3 | Training loss: 0.1282
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 146.9 | Training loss: 0.1241
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 146.6 | Training loss: 0.1260
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1272
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1261
Epoch:  9 |   [Val] | Loss: 0.0854 | [CCC]:  0.3764 [' 0.3764'] | PCC: 0.3828 ['0.3828'] | RMSE: 0.4657 ['0.4657']
Epoch:  9 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_316_None_None].pth"!
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 144.0 | Training loss: 0.1101
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 137.7 | Training loss: 0.1239
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 145.7 | Training loss: 0.1147
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1252
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1164
Epoch: 10 |   [Val] | Loss: 0.0821 | [CCC]:  0.3872 [' 0.3872'] | PCC: 0.3989 ['0.3989'] | RMSE: 0.4078 ['0.4078']
Epoch: 10 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_316_None_None].pth"!
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 149.2 | Training loss: 0.1195
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 147.4 | Training loss: 0.1095
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 138.8 | Training loss: 0.1057
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.0980
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1113
Epoch: 11 |   [Val] | Loss: 0.0949 | [CCC]:  0.3526 [' 0.3526'] | PCC: 0.3724 ['0.3724'] | RMSE: 0.4849 ['0.4849']
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 150.2 | Training loss: 0.1174
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 158.0 | Training loss: 0.1106
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 166.0 | Training loss: 0.1079
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.5 | Training loss: 0.1160
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1120
Epoch: 12 |   [Val] | Loss: 0.0812 | [CCC]:  0.3924 [' 0.3924'] | PCC: 0.3986 ['0.3986'] | RMSE: 0.4682 ['0.4682']
Epoch: 12 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_316_None_None].pth"!
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 172.6 | Training loss: 0.1137
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 175.3 | Training loss: 0.1074
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 172.7 | Training loss: 0.1012
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.5 | Training loss: 0.1177
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1076
Epoch: 13 |   [Val] | Loss: 0.0873 | [CCC]:  0.3920 [' 0.3920'] | PCC: 0.4141 ['0.4141'] | RMSE: 0.5116 ['0.5116']
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 171.0 | Training loss: 0.1049
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 162.4 | Training loss: 0.1165
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 164.9 | Training loss: 0.1039
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 4.3 | Training loss: 0.1085
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1084
Epoch: 14 |   [Val] | Loss: 0.0758 | [CCC]:  0.3790 [' 0.3790'] | PCC: 0.4178 ['0.4178'] | RMSE: 0.3886 ['0.3886']
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 99.5 | Training loss: 0.1028
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 164.3 | Training loss: 0.1091
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 170.5 | Training loss: 0.0988
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.6 | Training loss: 0.1268
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1040
Epoch: 15 |   [Val] | Loss: 0.1050 | [CCC]:  0.3024 [' 0.3024'] | PCC: 0.3941 ['0.3941'] | RMSE: 0.5780 ['0.5780']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 170.1 | Training loss: 0.1175
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 167.0 | Training loss: 0.1048
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 166.2 | Training loss: 0.0991
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 6.0 | Training loss: 0.1172
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.1073
Epoch: 16 |   [Val] | Loss: 0.0800 | [CCC]:  0.3820 [' 0.3820'] | PCC: 0.4188 ['0.4188'] | RMSE: 0.4523 ['0.4523']
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 122.0 | Training loss: 0.1044
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 163.2 | Training loss: 0.0906
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 131.0 | Training loss: 0.0993
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.5 | Training loss: 0.1229
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.0985
Epoch: 17 |   [Val] | Loss: 0.0776 | [CCC]:  0.3884 [' 0.3884'] | PCC: 0.4059 ['0.4059'] | RMSE: 0.4037 ['0.4037']
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 168.2 | Training loss: 0.0945
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 166.6 | Training loss: 0.1000
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 168.0 | Training loss: 0.1010
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.6 | Training loss: 0.1039
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.0986
Epoch: 18 |   [Val] | Loss: 0.1021 | [CCC]:  0.3285 [' 0.3285'] | PCC: 0.3713 ['0.3713'] | RMSE: 0.4933 ['0.4933']
Epoch    18: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 19 | Batch:   1 | Lr: 0.00250 | Time used(s): 169.1 | Training loss: 0.1164
Epoch: 19 | Batch:   2 | Lr: 0.00250 | Time used(s): 163.6 | Training loss: 0.1049
Epoch: 19 | Batch:   3 | Lr: 0.00250 | Time used(s): 148.6 | Training loss: 0.0904
Epoch: 19 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.4 | Training loss: 0.0915
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.1037
Epoch: 19 |   [Val] | Loss: 0.1026 | [CCC]:  0.3074 [' 0.3074'] | PCC: 0.3731 ['0.3731'] | RMSE: 0.5273 ['0.5273']
Epoch: 20 | Batch:   1 | Lr: 0.00250 | Time used(s): 169.1 | Training loss: 0.1025
Epoch: 20 | Batch:   2 | Lr: 0.00250 | Time used(s): 167.8 | Training loss: 0.1025
Epoch: 20 | Batch:   3 | Lr: 0.00250 | Time used(s): 165.6 | Training loss: 0.0921
Epoch: 20 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.5 | Training loss: 0.0804
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0987
Epoch: 20 |   [Val] | Loss: 0.0781 | [CCC]:  0.3954 [' 0.3954'] | PCC: 0.4198 ['0.4198'] | RMSE: 0.4091 ['0.4091']
Epoch: 20 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_316_None_None].pth"!
Epoch: 21 | Batch:   1 | Lr: 0.00250 | Time used(s): 169.7 | Training loss: 0.0987
Epoch: 21 | Batch:   2 | Lr: 0.00250 | Time used(s): 162.1 | Training loss: 0.0915
Epoch: 21 | Batch:   3 | Lr: 0.00250 | Time used(s): 164.3 | Training loss: 0.0809
Epoch: 21 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.4 | Training loss: 0.1295
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0910
Epoch: 21 |   [Val] | Loss: 0.0799 | [CCC]:  0.3763 [' 0.3763'] | PCC: 0.4069 ['0.4069'] | RMSE: 0.4707 ['0.4707']
Epoch: 22 | Batch:   1 | Lr: 0.00250 | Time used(s): 171.4 | Training loss: 0.0842
Epoch: 22 | Batch:   2 | Lr: 0.00250 | Time used(s): 168.1 | Training loss: 0.0823
Epoch: 22 | Batch:   3 | Lr: 0.00250 | Time used(s): 170.2 | Training loss: 0.0890
Epoch: 22 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.6 | Training loss: 0.0709
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0849
Epoch: 22 |   [Val] | Loss: 0.0730 | [CCC]:  0.4033 [' 0.4033'] | PCC: 0.4066 ['0.4066'] | RMSE: 0.4208 ['0.4208']
Epoch: 22 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_316_None_None].pth"!
Epoch: 23 | Batch:   1 | Lr: 0.00250 | Time used(s): 169.5 | Training loss: 0.0779
Epoch: 23 | Batch:   2 | Lr: 0.00250 | Time used(s): 165.4 | Training loss: 0.0774
Epoch: 23 | Batch:   3 | Lr: 0.00250 | Time used(s): 91.1 | Training loss: 0.0854
Epoch: 23 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.0552
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0798
Epoch: 23 |   [Val] | Loss: 0.0762 | [CCC]:  0.4088 [' 0.4088'] | PCC: 0.4131 ['0.4131'] | RMSE: 0.4534 ['0.4534']
Epoch: 23 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_316_None_None].pth"!
Epoch: 24 | Batch:   1 | Lr: 0.00250 | Time used(s): 97.7 | Training loss: 0.0762
Epoch: 24 | Batch:   2 | Lr: 0.00250 | Time used(s): 90.3 | Training loss: 0.0833
Epoch: 24 | Batch:   3 | Lr: 0.00250 | Time used(s): 92.8 | Training loss: 0.0798
Epoch: 24 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.0795
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0797
Epoch: 24 |   [Val] | Loss: 0.0752 | [CCC]:  0.4225 [' 0.4225'] | PCC: 0.4309 ['0.4309'] | RMSE: 0.4187 ['0.4187']
Epoch: 24 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_316_None_None].pth"!
Epoch: 25 | Batch:   1 | Lr: 0.00250 | Time used(s): 95.8 | Training loss: 0.0781
Epoch: 25 | Batch:   2 | Lr: 0.00250 | Time used(s): 92.0 | Training loss: 0.0764
Epoch: 25 | Batch:   3 | Lr: 0.00250 | Time used(s): 92.2 | Training loss: 0.0704
Epoch: 25 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0730
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0749
Epoch: 25 |   [Val] | Loss: 0.0722 | [CCC]:  0.4335 [' 0.4335'] | PCC: 0.4351 ['0.4351'] | RMSE: 0.4305 ['0.4305']
Epoch: 25 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_316_None_None].pth"!
Epoch: 26 | Batch:   1 | Lr: 0.00250 | Time used(s): 96.1 | Training loss: 0.0745
Epoch: 26 | Batch:   2 | Lr: 0.00250 | Time used(s): 91.1 | Training loss: 0.0704
Epoch: 26 | Batch:   3 | Lr: 0.00250 | Time used(s): 93.0 | Training loss: 0.0699
Epoch: 26 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.0572
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0714
Epoch: 26 |   [Val] | Loss: 0.0713 | [CCC]:  0.4313 [' 0.4313'] | PCC: 0.4354 ['0.4354'] | RMSE: 0.4367 ['0.4367']
Epoch: 27 | Batch:   1 | Lr: 0.00250 | Time used(s): 97.4 | Training loss: 0.0619
Epoch: 27 | Batch:   2 | Lr: 0.00250 | Time used(s): 91.8 | Training loss: 0.0620
Epoch: 27 | Batch:   3 | Lr: 0.00250 | Time used(s): 91.6 | Training loss: 0.0731
Epoch: 27 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.1046
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0663
Epoch: 27 |   [Val] | Loss: 0.0703 | [CCC]:  0.4317 [' 0.4317'] | PCC: 0.4368 ['0.4368'] | RMSE: 0.4227 ['0.4227']
Epoch: 28 | Batch:   1 | Lr: 0.00250 | Time used(s): 95.7 | Training loss: 0.0559
Epoch: 28 | Batch:   2 | Lr: 0.00250 | Time used(s): 91.3 | Training loss: 0.0713
Epoch: 28 | Batch:   3 | Lr: 0.00250 | Time used(s): 93.5 | Training loss: 0.0631
Epoch: 28 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0809
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0637
Epoch: 28 |   [Val] | Loss: 0.0778 | [CCC]:  0.3950 [' 0.3950'] | PCC: 0.4185 ['0.4185'] | RMSE: 0.4622 ['0.4622']
Epoch: 29 | Batch:   1 | Lr: 0.00250 | Time used(s): 90.8 | Training loss: 0.0609
Epoch: 29 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.0 | Training loss: 0.0591
Epoch: 29 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.3 | Training loss: 0.0636
Epoch: 29 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0310
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0607
Epoch: 29 |   [Val] | Loss: 0.0741 | [CCC]:  0.4003 [' 0.4003'] | PCC: 0.4112 ['0.4112'] | RMSE: 0.4424 ['0.4424']
Epoch: 30 | Batch:   1 | Lr: 0.00250 | Time used(s): 87.0 | Training loss: 0.0561
Epoch: 30 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.6 | Training loss: 0.0587
Epoch: 30 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.4 | Training loss: 0.0597
Epoch: 30 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0525
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0581
Epoch: 30 |   [Val] | Loss: 0.0712 | [CCC]:  0.4008 [' 0.4008'] | PCC: 0.4065 ['0.4065'] | RMSE: 0.4539 ['0.4539']
Epoch: 31 | Batch:   1 | Lr: 0.00250 | Time used(s): 85.5 | Training loss: 0.0584
Epoch: 31 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.7 | Training loss: 0.0567
Epoch: 31 | Batch:   3 | Lr: 0.00250 | Time used(s): 87.2 | Training loss: 0.0575
Epoch: 31 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0782
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0579
Epoch: 31 |   [Val] | Loss: 0.0722 | [CCC]:  0.3949 [' 0.3949'] | PCC: 0.4068 ['0.4068'] | RMSE: 0.4275 ['0.4275']
Epoch    31: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 32 | Batch:   1 | Lr: 0.00125 | Time used(s): 86.8 | Training loss: 0.0553
Epoch: 32 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.0 | Training loss: 0.0543
Epoch: 32 | Batch:   3 | Lr: 0.00125 | Time used(s): 84.7 | Training loss: 0.0518
Epoch: 32 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0648
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0540
Epoch: 32 |   [Val] | Loss: 0.0709 | [CCC]:  0.4033 [' 0.4033'] | PCC: 0.4073 ['0.4073'] | RMSE: 0.4476 ['0.4476']
Epoch: 33 | Batch:   1 | Lr: 0.00125 | Time used(s): 85.6 | Training loss: 0.0498
Epoch: 33 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.4 | Training loss: 0.0514
Epoch: 33 | Batch:   3 | Lr: 0.00125 | Time used(s): 84.1 | Training loss: 0.0525
Epoch: 33 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0648
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0514
Epoch: 33 |   [Val] | Loss: 0.0717 | [CCC]:  0.3969 [' 0.3969'] | PCC: 0.4003 ['0.4003'] | RMSE: 0.4431 ['0.4431']
Epoch: 34 | Batch:   1 | Lr: 0.00125 | Time used(s): 85.0 | Training loss: 0.0602
Epoch: 34 | Batch:   2 | Lr: 0.00125 | Time used(s): 84.8 | Training loss: 0.0422
Epoch: 34 | Batch:   3 | Lr: 0.00125 | Time used(s): 86.8 | Training loss: 0.0428
Epoch: 34 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0763
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0489
Epoch: 34 |   [Val] | Loss: 0.0718 | [CCC]:  0.4049 [' 0.4049'] | PCC: 0.4098 ['0.4098'] | RMSE: 0.4427 ['0.4427']
Epoch: 35 | Batch:   1 | Lr: 0.00125 | Time used(s): 136.5 | Training loss: 0.0505
Epoch: 35 | Batch:   2 | Lr: 0.00125 | Time used(s): 133.2 | Training loss: 0.0513
Epoch: 35 | Batch:   3 | Lr: 0.00125 | Time used(s): 134.8 | Training loss: 0.0476
Epoch: 35 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0228
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0493
Epoch: 35 |   [Val] | Loss: 0.0689 | [CCC]:  0.4033 [' 0.4033'] | PCC: 0.4089 ['0.4089'] | RMSE: 0.4415 ['0.4415']
Epoch: 36 | Batch:   1 | Lr: 0.00125 | Time used(s): 134.7 | Training loss: 0.0440
Epoch: 36 | Batch:   2 | Lr: 0.00125 | Time used(s): 133.9 | Training loss: 0.0481
Epoch: 36 | Batch:   3 | Lr: 0.00125 | Time used(s): 131.9 | Training loss: 0.0448
Epoch: 36 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0509
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0457
Epoch: 36 |   [Val] | Loss: 0.0719 | [CCC]:  0.4035 [' 0.4035'] | PCC: 0.4039 ['0.4039'] | RMSE: 0.4365 ['0.4365']
Epoch: 37 | Batch:   1 | Lr: 0.00125 | Time used(s): 134.4 | Training loss: 0.0439
Epoch: 37 | Batch:   2 | Lr: 0.00125 | Time used(s): 132.0 | Training loss: 0.0384
Epoch: 37 | Batch:   3 | Lr: 0.00125 | Time used(s): 131.8 | Training loss: 0.0437
Epoch: 37 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.4 | Training loss: 0.0525
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0421
Epoch: 37 |   [Val] | Loss: 0.0702 | [CCC]:  0.3998 [' 0.3998'] | PCC: 0.4059 ['0.4059'] | RMSE: 0.4402 ['0.4402']
Epoch    37: reducing learning rate of group 0 to 6.2500e-04.
Epoch: 38 | Batch:   1 | Lr: 0.00063 | Time used(s): 133.7 | Training loss: 0.0446
Epoch: 38 | Batch:   2 | Lr: 0.00063 | Time used(s): 132.8 | Training loss: 0.0396
Epoch: 38 | Batch:   3 | Lr: 0.00063 | Time used(s): 132.2 | Training loss: 0.0320
Epoch: 38 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.2 | Training loss: 0.0820
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0394
Epoch: 38 |   [Val] | Loss: 0.0717 | [CCC]:  0.3843 [' 0.3843'] | PCC: 0.3936 ['0.3936'] | RMSE: 0.4410 ['0.4410']
Epoch: 39 | Batch:   1 | Lr: 0.00063 | Time used(s): 136.3 | Training loss: 0.0398
Epoch: 39 | Batch:   2 | Lr: 0.00063 | Time used(s): 133.5 | Training loss: 0.0412
Epoch: 39 | Batch:   3 | Lr: 0.00063 | Time used(s): 130.5 | Training loss: 0.0386
Epoch: 39 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.3 | Training loss: 0.0415
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0399
Epoch: 39 |   [Val] | Loss: 0.0713 | [CCC]:  0.3989 [' 0.3989'] | PCC: 0.4010 ['0.4010'] | RMSE: 0.4355 ['0.4355']
Epoch: 40 | Batch:   1 | Lr: 0.00063 | Time used(s): 134.9 | Training loss: 0.0351
Epoch: 40 | Batch:   2 | Lr: 0.00063 | Time used(s): 132.7 | Training loss: 0.0386
Epoch: 40 | Batch:   3 | Lr: 0.00063 | Time used(s): 101.1 | Training loss: 0.0347
Epoch: 40 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.3 | Training loss: 0.0516
--------------------------------------------------
Epoch: 40 | [Train] | Loss: 0.0364
Epoch: 40 |   [Val] | Loss: 0.0718 | [CCC]:  0.4011 [' 0.4011'] | PCC: 0.4045 ['0.4045'] | RMSE: 0.4361 ['0.4361']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 316 | Best [Val CCC]: 0.4335 [' 0.4335']| Loss: 0.0722 | PCC: 0.4351 ['0.4351'] | RMSE: 0.4305 ['0.4305']
On Test: CCC  0.5401 | PCC  0.5486 | RMSE  0.3907
****************************************************************************************************
Seed "316" over!
****************************************************************************************************
****************************************************************************************************
Using seed "317"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 317]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 133.5 | Training loss: 0.2502
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 129.9 | Training loss: 0.2475
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 131.0 | Training loss: 0.2345
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 3.5 | Training loss: 0.2111
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2436
Epoch:  1 |   [Val] | Loss: 0.2160 | [CCC]:  0.1110 [' 0.1110'] | PCC: 0.1802 ['0.1802'] | RMSE: 0.5684 ['0.5684']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_317_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 132.9 | Training loss: 0.2405
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 130.8 | Training loss: 0.2424
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 131.3 | Training loss: 0.2347
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.2126
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2388
Epoch:  2 |   [Val] | Loss: 0.2285 | [CCC]:  0.0467 [' 0.0467'] | PCC: 0.1903 ['0.1903'] | RMSE: 0.8486 ['0.8486']
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 133.3 | Training loss: 0.2297
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 131.2 | Training loss: 0.2100
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 125.1 | Training loss: 0.2057
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.2119
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.2151
Epoch:  3 |   [Val] | Loss: 0.1342 | [CCC]:  0.2641 [' 0.2641'] | PCC: 0.2709 ['0.2709'] | RMSE: 0.4582 ['0.4582']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_317_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 135.4 | Training loss: 0.1810
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 133.5 | Training loss: 0.1869
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 134.6 | Training loss: 0.1736
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1616
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1802
Epoch:  4 |   [Val] | Loss: 0.1439 | [CCC]:  0.2320 [' 0.2320'] | PCC: 0.2741 ['0.2741'] | RMSE: 0.6030 ['0.6030']
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 99.0 | Training loss: 0.1672
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 95.3 | Training loss: 0.1757
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.9 | Training loss: 0.1695
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1744
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1709
Epoch:  5 |   [Val] | Loss: 0.1244 | [CCC]:  0.2890 [' 0.2890'] | PCC: 0.3041 ['0.3041'] | RMSE: 0.5066 ['0.5066']
Epoch:  5 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_317_None_None].pth"!
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 130.0 | Training loss: 0.1598
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.2 | Training loss: 0.1539
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 102.1 | Training loss: 0.1491
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1304
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1539
Epoch:  6 |   [Val] | Loss: 0.1244 | [CCC]:  0.2900 [' 0.2900'] | PCC: 0.2989 ['0.2989'] | RMSE: 0.4896 ['0.4896']
Epoch:  6 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_317_None_None].pth"!
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 133.8 | Training loss: 0.1513
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 97.9 | Training loss: 0.1475
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.5 | Training loss: 0.1491
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1522
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1493
Epoch:  7 |   [Val] | Loss: 0.1047 | [CCC]:  0.2977 [' 0.2977'] | PCC: 0.3096 ['0.3096'] | RMSE: 0.5260 ['0.5260']
Epoch:  7 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_317_None_None].pth"!
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 134.1 | Training loss: 0.1319
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 124.9 | Training loss: 0.1433
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 100.0 | Training loss: 0.1423
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1552
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1394
Epoch:  8 |   [Val] | Loss: 0.0999 | [CCC]:  0.3248 [' 0.3248'] | PCC: 0.3284 ['0.3284'] | RMSE: 0.5129 ['0.5129']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_317_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 125.8 | Training loss: 0.1357
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 112.5 | Training loss: 0.1363
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.9 | Training loss: 0.1328
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1675
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1355
Epoch:  9 |   [Val] | Loss: 0.1067 | [CCC]:  0.2924 [' 0.2924'] | PCC: 0.3361 ['0.3361'] | RMSE: 0.4449 ['0.4449']
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 119.6 | Training loss: 0.1372
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 96.8 | Training loss: 0.1313
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 95.2 | Training loss: 0.1337
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1365
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1341
Epoch: 10 |   [Val] | Loss: 0.0919 | [CCC]:  0.3432 [' 0.3432'] | PCC: 0.3576 ['0.3576'] | RMSE: 0.4879 ['0.4879']
Epoch: 10 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_317_None_None].pth"!
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 129.1 | Training loss: 0.1288
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 130.6 | Training loss: 0.1330
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 109.6 | Training loss: 0.1099
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1542
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1244
Epoch: 11 |   [Val] | Loss: 0.0922 | [CCC]:  0.3439 [' 0.3439'] | PCC: 0.3717 ['0.3717'] | RMSE: 0.4065 ['0.4065']
Epoch: 11 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_317_None_None].pth"!
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 116.2 | Training loss: 0.1241
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 129.4 | Training loss: 0.1229
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 93.1 | Training loss: 0.1123
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1049
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1195
Epoch: 12 |   [Val] | Loss: 0.0808 | [CCC]:  0.3834 [' 0.3834'] | PCC: 0.3849 ['0.3849'] | RMSE: 0.4737 ['0.4737']
Epoch: 12 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_317_None_None].pth"!
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 106.9 | Training loss: 0.1148
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.3 | Training loss: 0.1171
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.4 | Training loss: 0.1088
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0996
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1134
Epoch: 13 |   [Val] | Loss: 0.0825 | [CCC]:  0.3807 [' 0.3807'] | PCC: 0.3857 ['0.3857'] | RMSE: 0.4240 ['0.4240']
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.0 | Training loss: 0.1139
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.2 | Training loss: 0.1101
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.2 | Training loss: 0.1043
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0915
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1091
Epoch: 14 |   [Val] | Loss: 0.0806 | [CCC]:  0.3909 [' 0.3909'] | PCC: 0.3958 ['0.3958'] | RMSE: 0.4340 ['0.4340']
Epoch: 14 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_317_None_None].pth"!
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.5 | Training loss: 0.1061
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.5 | Training loss: 0.1027
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.2 | Training loss: 0.0976
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0867
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1019
Epoch: 15 |   [Val] | Loss: 0.0954 | [CCC]:  0.3268 [' 0.3268'] | PCC: 0.3479 ['0.3479'] | RMSE: 0.4244 ['0.4244']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.2 | Training loss: 0.1154
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.6 | Training loss: 0.1084
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 121.9 | Training loss: 0.1025
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1315
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.1091
Epoch: 16 |   [Val] | Loss: 0.0794 | [CCC]:  0.3780 [' 0.3780'] | PCC: 0.3833 ['0.3833'] | RMSE: 0.4967 ['0.4967']
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 132.0 | Training loss: 0.0984
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 124.1 | Training loss: 0.1030
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 109.3 | Training loss: 0.0982
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1185
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.1002
Epoch: 17 |   [Val] | Loss: 0.0864 | [CCC]:  0.3435 [' 0.3435'] | PCC: 0.3884 ['0.3884'] | RMSE: 0.4188 ['0.4188']
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 132.1 | Training loss: 0.1102
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 123.7 | Training loss: 0.1047
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 112.3 | Training loss: 0.0955
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1360
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.1040
Epoch: 18 |   [Val] | Loss: 0.0843 | [CCC]:  0.3746 [' 0.3746'] | PCC: 0.3850 ['0.3850'] | RMSE: 0.5196 ['0.5196']
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 130.0 | Training loss: 0.0980
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 123.1 | Training loss: 0.1045
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 109.1 | Training loss: 0.0996
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1147
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.1009
Epoch: 19 |   [Val] | Loss: 0.0860 | [CCC]:  0.3514 [' 0.3514'] | PCC: 0.3575 ['0.3575'] | RMSE: 0.4274 ['0.4274']
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 129.4 | Training loss: 0.1026
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 123.4 | Training loss: 0.0931
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 102.4 | Training loss: 0.1008
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0732
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0984
Epoch: 20 |   [Val] | Loss: 0.0849 | [CCC]:  0.3796 [' 0.3796'] | PCC: 0.3870 ['0.3870'] | RMSE: 0.4484 ['0.4484']
Epoch    20: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 21 | Batch:   1 | Lr: 0.00250 | Time used(s): 131.3 | Training loss: 0.1013
Epoch: 21 | Batch:   2 | Lr: 0.00250 | Time used(s): 124.2 | Training loss: 0.0986
Epoch: 21 | Batch:   3 | Lr: 0.00250 | Time used(s): 109.3 | Training loss: 0.0951
Epoch: 21 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0911
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0982
Epoch: 21 |   [Val] | Loss: 0.0848 | [CCC]:  0.3491 [' 0.3491'] | PCC: 0.3716 ['0.3716'] | RMSE: 0.4835 ['0.4835']
Epoch: 22 | Batch:   1 | Lr: 0.00250 | Time used(s): 129.5 | Training loss: 0.0967
Epoch: 22 | Batch:   2 | Lr: 0.00250 | Time used(s): 123.5 | Training loss: 0.0944
Epoch: 22 | Batch:   3 | Lr: 0.00250 | Time used(s): 113.4 | Training loss: 0.0880
Epoch: 22 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.1311
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0937
Epoch: 22 |   [Val] | Loss: 0.0868 | [CCC]:  0.3687 [' 0.3687'] | PCC: 0.3722 ['0.3722'] | RMSE: 0.4509 ['0.4509']
Epoch: 23 | Batch:   1 | Lr: 0.00250 | Time used(s): 130.2 | Training loss: 0.0948
Epoch: 23 | Batch:   2 | Lr: 0.00250 | Time used(s): 123.5 | Training loss: 0.0837
Epoch: 23 | Batch:   3 | Lr: 0.00250 | Time used(s): 119.3 | Training loss: 0.0853
Epoch: 23 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0995
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0881
Epoch: 23 |   [Val] | Loss: 0.0772 | [CCC]:  0.3676 [' 0.3676'] | PCC: 0.3742 ['0.3742'] | RMSE: 0.4216 ['0.4216']
Epoch: 24 | Batch:   1 | Lr: 0.00250 | Time used(s): 128.5 | Training loss: 0.0826
Epoch: 24 | Batch:   2 | Lr: 0.00250 | Time used(s): 125.6 | Training loss: 0.0862
Epoch: 24 | Batch:   3 | Lr: 0.00250 | Time used(s): 135.0 | Training loss: 0.0834
Epoch: 24 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.4 | Training loss: 0.0805
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0840
Epoch: 24 |   [Val] | Loss: 0.0725 | [CCC]:  0.3864 [' 0.3864'] | PCC: 0.3959 ['0.3959'] | RMSE: 0.4704 ['0.4704']
Epoch: 25 | Batch:   1 | Lr: 0.00250 | Time used(s): 138.2 | Training loss: 0.0770
Epoch: 25 | Batch:   2 | Lr: 0.00250 | Time used(s): 137.7 | Training loss: 0.0851
Epoch: 25 | Batch:   3 | Lr: 0.00250 | Time used(s): 117.4 | Training loss: 0.0761
Epoch: 25 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0669
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0792
Epoch: 25 |   [Val] | Loss: 0.0764 | [CCC]:  0.4068 [' 0.4068'] | PCC: 0.4092 ['0.4092'] | RMSE: 0.4409 ['0.4409']
Epoch: 25 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_317_None_None].pth"!
Epoch: 26 | Batch:   1 | Lr: 0.00250 | Time used(s): 130.0 | Training loss: 0.0834
Epoch: 26 | Batch:   2 | Lr: 0.00250 | Time used(s): 123.9 | Training loss: 0.0723
Epoch: 26 | Batch:   3 | Lr: 0.00250 | Time used(s): 136.3 | Training loss: 0.0747
Epoch: 26 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.4 | Training loss: 0.0764
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0768
Epoch: 26 |   [Val] | Loss: 0.0762 | [CCC]:  0.4040 [' 0.4040'] | PCC: 0.4058 ['0.4058'] | RMSE: 0.4261 ['0.4261']
Epoch: 27 | Batch:   1 | Lr: 0.00250 | Time used(s): 138.5 | Training loss: 0.0791
Epoch: 27 | Batch:   2 | Lr: 0.00250 | Time used(s): 136.6 | Training loss: 0.0714
Epoch: 27 | Batch:   3 | Lr: 0.00250 | Time used(s): 134.5 | Training loss: 0.0710
Epoch: 27 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.4 | Training loss: 0.0775
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0739
Epoch: 27 |   [Val] | Loss: 0.0766 | [CCC]:  0.4207 [' 0.4207'] | PCC: 0.4210 ['0.4210'] | RMSE: 0.4482 ['0.4482']
Epoch: 27 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_317_None_None].pth"!
Epoch: 28 | Batch:   1 | Lr: 0.00250 | Time used(s): 140.4 | Training loss: 0.0722
Epoch: 28 | Batch:   2 | Lr: 0.00250 | Time used(s): 139.6 | Training loss: 0.0702
Epoch: 28 | Batch:   3 | Lr: 0.00250 | Time used(s): 116.7 | Training loss: 0.0683
Epoch: 28 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0885
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0705
Epoch: 28 |   [Val] | Loss: 0.0764 | [CCC]:  0.4005 [' 0.4005'] | PCC: 0.4195 ['0.4195'] | RMSE: 0.4243 ['0.4243']
Epoch: 29 | Batch:   1 | Lr: 0.00250 | Time used(s): 130.0 | Training loss: 0.0778
Epoch: 29 | Batch:   2 | Lr: 0.00250 | Time used(s): 124.1 | Training loss: 0.0694
Epoch: 29 | Batch:   3 | Lr: 0.00250 | Time used(s): 121.3 | Training loss: 0.0684
Epoch: 29 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0578
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0716
Epoch: 29 |   [Val] | Loss: 0.0762 | [CCC]:  0.4099 [' 0.4099'] | PCC: 0.4203 ['0.4203'] | RMSE: 0.4419 ['0.4419']
Epoch: 30 | Batch:   1 | Lr: 0.00250 | Time used(s): 128.7 | Training loss: 0.0727
Epoch: 30 | Batch:   2 | Lr: 0.00250 | Time used(s): 123.4 | Training loss: 0.0807
Epoch: 30 | Batch:   3 | Lr: 0.00250 | Time used(s): 115.9 | Training loss: 0.0644
Epoch: 30 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0491
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0722
Epoch: 30 |   [Val] | Loss: 0.0834 | [CCC]:  0.3578 [' 0.3578'] | PCC: 0.3825 ['0.3825'] | RMSE: 0.4943 ['0.4943']
Epoch: 31 | Batch:   1 | Lr: 0.00250 | Time used(s): 129.8 | Training loss: 0.0690
Epoch: 31 | Batch:   2 | Lr: 0.00250 | Time used(s): 125.4 | Training loss: 0.0655
Epoch: 31 | Batch:   3 | Lr: 0.00250 | Time used(s): 115.3 | Training loss: 0.0742
Epoch: 31 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.1024
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0701
Epoch: 31 |   [Val] | Loss: 0.0732 | [CCC]:  0.3995 [' 0.3995'] | PCC: 0.4038 ['0.4038'] | RMSE: 0.4286 ['0.4286']
Epoch: 32 | Batch:   1 | Lr: 0.00250 | Time used(s): 129.9 | Training loss: 0.0619
Epoch: 32 | Batch:   2 | Lr: 0.00250 | Time used(s): 122.1 | Training loss: 0.0703
Epoch: 32 | Batch:   3 | Lr: 0.00250 | Time used(s): 84.3 | Training loss: 0.0690
Epoch: 32 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0587
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0669
Epoch: 32 |   [Val] | Loss: 0.0726 | [CCC]:  0.4159 [' 0.4159'] | PCC: 0.4162 ['0.4162'] | RMSE: 0.4451 ['0.4451']
Epoch: 33 | Batch:   1 | Lr: 0.00250 | Time used(s): 85.4 | Training loss: 0.0694
Epoch: 33 | Batch:   2 | Lr: 0.00250 | Time used(s): 122.7 | Training loss: 0.0601
Epoch: 33 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.4 | Training loss: 0.0565
Epoch: 33 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0782
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0623
Epoch: 33 |   [Val] | Loss: 0.0672 | [CCC]:  0.4168 [' 0.4168'] | PCC: 0.4176 ['0.4176'] | RMSE: 0.4275 ['0.4275']
Epoch    33: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 34 | Batch:   1 | Lr: 0.00125 | Time used(s): 120.8 | Training loss: 0.0600
Epoch: 34 | Batch:   2 | Lr: 0.00125 | Time used(s): 103.8 | Training loss: 0.0606
Epoch: 34 | Batch:   3 | Lr: 0.00125 | Time used(s): 87.3 | Training loss: 0.0587
Epoch: 34 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0586
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0598
Epoch: 34 |   [Val] | Loss: 0.0723 | [CCC]:  0.4121 [' 0.4121'] | PCC: 0.4125 ['0.4125'] | RMSE: 0.4451 ['0.4451']
Epoch: 35 | Batch:   1 | Lr: 0.00125 | Time used(s): 120.6 | Training loss: 0.0524
Epoch: 35 | Batch:   2 | Lr: 0.00125 | Time used(s): 103.8 | Training loss: 0.0655
Epoch: 35 | Batch:   3 | Lr: 0.00125 | Time used(s): 87.9 | Training loss: 0.0502
Epoch: 35 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0564
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0560
Epoch: 35 |   [Val] | Loss: 0.0734 | [CCC]:  0.4056 [' 0.4056'] | PCC: 0.4074 ['0.4074'] | RMSE: 0.4404 ['0.4404']
Epoch: 36 | Batch:   1 | Lr: 0.00125 | Time used(s): 116.7 | Training loss: 0.0573
Epoch: 36 | Batch:   2 | Lr: 0.00125 | Time used(s): 123.0 | Training loss: 0.0489
Epoch: 36 | Batch:   3 | Lr: 0.00125 | Time used(s): 92.3 | Training loss: 0.0596
Epoch: 36 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0187
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0547
Epoch: 36 |   [Val] | Loss: 0.0755 | [CCC]:  0.4062 [' 0.4062'] | PCC: 0.4114 ['0.4114'] | RMSE: 0.4287 ['0.4287']
Epoch: 37 | Batch:   1 | Lr: 0.00125 | Time used(s): 121.9 | Training loss: 0.0573
Epoch: 37 | Batch:   2 | Lr: 0.00125 | Time used(s): 116.1 | Training loss: 0.0509
Epoch: 37 | Batch:   3 | Lr: 0.00125 | Time used(s): 93.5 | Training loss: 0.0478
Epoch: 37 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0410
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0518
Epoch: 37 |   [Val] | Loss: 0.0769 | [CCC]:  0.4043 [' 0.4043'] | PCC: 0.4057 ['0.4057'] | RMSE: 0.4599 ['0.4599']
Epoch: 38 | Batch:   1 | Lr: 0.00125 | Time used(s): 124.8 | Training loss: 0.0526
Epoch: 38 | Batch:   2 | Lr: 0.00125 | Time used(s): 116.7 | Training loss: 0.0474
Epoch: 38 | Batch:   3 | Lr: 0.00125 | Time used(s): 94.5 | Training loss: 0.0503
Epoch: 38 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0534
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0502
Epoch: 38 |   [Val] | Loss: 0.0713 | [CCC]:  0.4178 [' 0.4178'] | PCC: 0.4199 ['0.4199'] | RMSE: 0.4204 ['0.4204']
Epoch: 39 | Batch:   1 | Lr: 0.00125 | Time used(s): 122.3 | Training loss: 0.0477
Epoch: 39 | Batch:   2 | Lr: 0.00125 | Time used(s): 115.6 | Training loss: 0.0514
Epoch: 39 | Batch:   3 | Lr: 0.00125 | Time used(s): 98.1 | Training loss: 0.0486
Epoch: 39 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0021
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0485
Epoch: 39 |   [Val] | Loss: 0.0811 | [CCC]:  0.3996 [' 0.3996'] | PCC: 0.4094 ['0.4094'] | RMSE: 0.4360 ['0.4360']
Epoch    39: reducing learning rate of group 0 to 6.2500e-04.
Epoch: 40 | Batch:   1 | Lr: 0.00063 | Time used(s): 122.4 | Training loss: 0.0476
Epoch: 40 | Batch:   2 | Lr: 0.00063 | Time used(s): 119.3 | Training loss: 0.0555
Epoch: 40 | Batch:   3 | Lr: 0.00063 | Time used(s): 96.3 | Training loss: 0.0453
Epoch: 40 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.1 | Training loss: 0.0652
--------------------------------------------------
Epoch: 40 | [Train] | Loss: 0.0497
Epoch: 40 |   [Val] | Loss: 0.0732 | [CCC]:  0.4106 [' 0.4106'] | PCC: 0.4112 ['0.4112'] | RMSE: 0.4320 ['0.4320']
Epoch: 41 | Batch:   1 | Lr: 0.00063 | Time used(s): 125.0 | Training loss: 0.0457
Epoch: 41 | Batch:   2 | Lr: 0.00063 | Time used(s): 116.9 | Training loss: 0.0443
Epoch: 41 | Batch:   3 | Lr: 0.00063 | Time used(s): 95.6 | Training loss: 0.0385
Epoch: 41 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.1 | Training loss: 0.0612
--------------------------------------------------
Epoch: 41 | [Train] | Loss: 0.0431
Epoch: 41 |   [Val] | Loss: 0.0696 | [CCC]:  0.4110 [' 0.4110'] | PCC: 0.4138 ['0.4138'] | RMSE: 0.4235 ['0.4235']
Epoch: 42 | Batch:   1 | Lr: 0.00063 | Time used(s): 124.2 | Training loss: 0.0475
Epoch: 42 | Batch:   2 | Lr: 0.00063 | Time used(s): 116.7 | Training loss: 0.0345
Epoch: 42 | Batch:   3 | Lr: 0.00063 | Time used(s): 95.1 | Training loss: 0.0458
Epoch: 42 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.1 | Training loss: 0.0418
--------------------------------------------------
Epoch: 42 | [Train] | Loss: 0.0426
Epoch: 42 |   [Val] | Loss: 0.0721 | [CCC]:  0.4169 [' 0.4169'] | PCC: 0.4171 ['0.4171'] | RMSE: 0.4480 ['0.4480']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 317 | Best [Val CCC]: 0.4207 [' 0.4207']| Loss: 0.0766 | PCC: 0.4210 ['0.4210'] | RMSE: 0.4482 ['0.4482']
On Test: CCC  0.5879 | PCC  0.5884 | RMSE  0.3934
****************************************************************************************************
Seed "317" over!
****************************************************************************************************
****************************************************************************************************
Using seed "318"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 318]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 118.2 | Training loss: 0.2502
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 116.0 | Training loss: 0.2447
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.5 | Training loss: 0.2371
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.2392
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2439
Epoch:  1 |   [Val] | Loss: 0.1762 | [CCC]:  0.1662 [' 0.1662'] | PCC: 0.1896 ['0.1896'] | RMSE: 0.4355 ['0.4355']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_318_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 118.6 | Training loss: 0.2062
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 117.9 | Training loss: 0.2099
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 93.0 | Training loss: 0.2334
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.2274
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2167
Epoch:  2 |   [Val] | Loss: 0.1616 | [CCC]:  0.2014 [' 0.2014'] | PCC: 0.2181 ['0.2181'] | RMSE: 0.4735 ['0.4735']
Epoch:  2 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_318_None_None].pth"!
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 120.3 | Training loss: 0.1906
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 114.2 | Training loss: 0.2075
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.3 | Training loss: 0.1875
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1825
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.1950
Epoch:  3 |   [Val] | Loss: 0.1340 | [CCC]:  0.2598 [' 0.2598'] | PCC: 0.2966 ['0.2966'] | RMSE: 0.4606 ['0.4606']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_318_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 121.8 | Training loss: 0.1730
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 116.0 | Training loss: 0.1623
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 98.4 | Training loss: 0.1551
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1519
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1633
Epoch:  4 |   [Val] | Loss: 0.1816 | [CCC]:  0.1396 [' 0.1396'] | PCC: 0.2619 ['0.2619'] | RMSE: 0.6971 ['0.6971']
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 126.8 | Training loss: 0.1996
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 119.5 | Training loss: 0.1719
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 99.9 | Training loss: 0.1597
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1415
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1765
Epoch:  5 |   [Val] | Loss: 0.1158 | [CCC]:  0.2919 [' 0.2919'] | PCC: 0.2924 ['0.2924'] | RMSE: 0.4903 ['0.4903']
Epoch:  5 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_318_None_None].pth"!
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 124.6 | Training loss: 0.1568
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 118.2 | Training loss: 0.1614
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 98.0 | Training loss: 0.1470
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1502
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1550
Epoch:  6 |   [Val] | Loss: 0.1273 | [CCC]:  0.2726 [' 0.2726'] | PCC: 0.3080 ['0.3080'] | RMSE: 0.4814 ['0.4814']
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 123.6 | Training loss: 0.1508
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 114.1 | Training loss: 0.1337
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 99.8 | Training loss: 0.1479
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1107
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1436
Epoch:  7 |   [Val] | Loss: 0.1045 | [CCC]:  0.3288 [' 0.3288'] | PCC: 0.3387 ['0.3387'] | RMSE: 0.4384 ['0.4384']
Epoch:  7 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_318_None_None].pth"!
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 127.9 | Training loss: 0.1419
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 120.7 | Training loss: 0.1279
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 100.7 | Training loss: 0.1288
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1392
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1329
Epoch:  8 |   [Val] | Loss: 0.0974 | [CCC]:  0.3376 [' 0.3376'] | PCC: 0.3602 ['0.3602'] | RMSE: 0.4846 ['0.4846']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_318_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 107.6 | Training loss: 0.1368
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.1 | Training loss: 0.1224
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 107.6 | Training loss: 0.1168
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1309
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1254
Epoch:  9 |   [Val] | Loss: 0.0810 | [CCC]:  0.3670 [' 0.3670'] | PCC: 0.3699 ['0.3699'] | RMSE: 0.4813 ['0.4813']
Epoch:  9 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_318_None_None].pth"!
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 132.5 | Training loss: 0.1212
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 127.2 | Training loss: 0.1104
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 113.0 | Training loss: 0.1139
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0886
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1147
Epoch: 10 |   [Val] | Loss: 0.0929 | [CCC]:  0.3626 [' 0.3626'] | PCC: 0.3997 ['0.3997'] | RMSE: 0.4167 ['0.4167']
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 131.1 | Training loss: 0.1190
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 124.7 | Training loss: 0.1075
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 103.2 | Training loss: 0.1068
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0900
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1107
Epoch: 11 |   [Val] | Loss: 0.0753 | [CCC]:  0.4000 [' 0.4000'] | PCC: 0.4107 ['0.4107'] | RMSE: 0.4706 ['0.4706']
Epoch: 11 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_318_None_None].pth"!
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 129.7 | Training loss: 0.1144
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 123.2 | Training loss: 0.1021
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 113.9 | Training loss: 0.1096
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0674
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1080
Epoch: 12 |   [Val] | Loss: 0.0763 | [CCC]:  0.3796 [' 0.3796'] | PCC: 0.4058 ['0.4058'] | RMSE: 0.4632 ['0.4632']
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 130.8 | Training loss: 0.1129
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 124.1 | Training loss: 0.0939
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 116.7 | Training loss: 0.1159
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0548
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1067
Epoch: 13 |   [Val] | Loss: 0.0992 | [CCC]:  0.3189 [' 0.3189'] | PCC: 0.3934 ['0.3934'] | RMSE: 0.5418 ['0.5418']
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 131.4 | Training loss: 0.1201
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 122.7 | Training loss: 0.1246
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 117.8 | Training loss: 0.1210
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1156
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1218
Epoch: 14 |   [Val] | Loss: 0.0828 | [CCC]:  0.3709 [' 0.3709'] | PCC: 0.4001 ['0.4001'] | RMSE: 0.4489 ['0.4489']
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 128.6 | Training loss: 0.1092
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 123.3 | Training loss: 0.0999
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 113.8 | Training loss: 0.1108
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0788
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1062
Epoch: 15 |   [Val] | Loss: 0.0768 | [CCC]:  0.3830 [' 0.3830'] | PCC: 0.3844 ['0.3844'] | RMSE: 0.4652 ['0.4652']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 131.4 | Training loss: 0.0933
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 125.9 | Training loss: 0.0956
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 115.2 | Training loss: 0.0955
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0973
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.0949
Epoch: 16 |   [Val] | Loss: 0.0686 | [CCC]:  0.4182 [' 0.4182'] | PCC: 0.4201 ['0.4201'] | RMSE: 0.4481 ['0.4481']
Epoch: 16 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_318_None_None].pth"!
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 130.2 | Training loss: 0.0987
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 122.4 | Training loss: 0.0871
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 113.5 | Training loss: 0.0889
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0892
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.0915
Epoch: 17 |   [Val] | Loss: 0.0714 | [CCC]:  0.4225 [' 0.4225'] | PCC: 0.4258 ['0.4258'] | RMSE: 0.4232 ['0.4232']
Epoch: 17 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_318_None_None].pth"!
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 129.6 | Training loss: 0.0924
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 122.7 | Training loss: 0.0872
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 111.4 | Training loss: 0.0799
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0616
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.0861
Epoch: 18 |   [Val] | Loss: 0.0708 | [CCC]:  0.4055 [' 0.4055'] | PCC: 0.4277 ['0.4277'] | RMSE: 0.3925 ['0.3925']
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 129.3 | Training loss: 0.0937
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 122.4 | Training loss: 0.0781
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 113.1 | Training loss: 0.0831
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0853
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.0850
Epoch: 19 |   [Val] | Loss: 0.0740 | [CCC]:  0.4236 [' 0.4236'] | PCC: 0.4277 ['0.4277'] | RMSE: 0.4669 ['0.4669']
Epoch: 19 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_318_None_None].pth"!
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 129.8 | Training loss: 0.0742
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 123.5 | Training loss: 0.0776
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 107.0 | Training loss: 0.0790
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0705
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0768
Epoch: 20 |   [Val] | Loss: 0.0760 | [CCC]:  0.4106 [' 0.4106'] | PCC: 0.4166 ['0.4166'] | RMSE: 0.4601 ['0.4601']
Epoch: 21 | Batch:   1 | Lr: 0.00500 | Time used(s): 130.0 | Training loss: 0.0700
Epoch: 21 | Batch:   2 | Lr: 0.00500 | Time used(s): 124.3 | Training loss: 0.0783
Epoch: 21 | Batch:   3 | Lr: 0.00500 | Time used(s): 113.6 | Training loss: 0.0837
Epoch: 21 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0725
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0772
Epoch: 21 |   [Val] | Loss: 0.0670 | [CCC]:  0.4171 [' 0.4171'] | PCC: 0.4305 ['0.4305'] | RMSE: 0.4388 ['0.4388']
Epoch: 22 | Batch:   1 | Lr: 0.00500 | Time used(s): 129.8 | Training loss: 0.0801
Epoch: 22 | Batch:   2 | Lr: 0.00500 | Time used(s): 124.3 | Training loss: 0.0749
Epoch: 22 | Batch:   3 | Lr: 0.00500 | Time used(s): 115.0 | Training loss: 0.0738
Epoch: 22 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0691
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0762
Epoch: 22 |   [Val] | Loss: 0.0758 | [CCC]:  0.4074 [' 0.4074'] | PCC: 0.4260 ['0.4260'] | RMSE: 0.4183 ['0.4183']
Epoch: 23 | Batch:   1 | Lr: 0.00500 | Time used(s): 129.9 | Training loss: 0.0719
Epoch: 23 | Batch:   2 | Lr: 0.00500 | Time used(s): 121.9 | Training loss: 0.0656
Epoch: 23 | Batch:   3 | Lr: 0.00500 | Time used(s): 112.6 | Training loss: 0.0742
Epoch: 23 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0619
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0704
Epoch: 23 |   [Val] | Loss: 0.0757 | [CCC]:  0.4039 [' 0.4039'] | PCC: 0.4264 ['0.4264'] | RMSE: 0.4458 ['0.4458']
Epoch: 24 | Batch:   1 | Lr: 0.00500 | Time used(s): 130.1 | Training loss: 0.0747
Epoch: 24 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.9 | Training loss: 0.0627
Epoch: 24 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.9 | Training loss: 0.0701
Epoch: 24 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0359
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0686
Epoch: 24 |   [Val] | Loss: 0.0872 | [CCC]:  0.3635 [' 0.3635'] | PCC: 0.4206 ['0.4206'] | RMSE: 0.4667 ['0.4667']
Epoch: 25 | Batch:   1 | Lr: 0.00500 | Time used(s): 128.3 | Training loss: 0.0689
Epoch: 25 | Batch:   2 | Lr: 0.00500 | Time used(s): 126.8 | Training loss: 0.0660
Epoch: 25 | Batch:   3 | Lr: 0.00500 | Time used(s): 127.0 | Training loss: 0.0661
Epoch: 25 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0648
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0670
Epoch: 25 |   [Val] | Loss: 0.0954 | [CCC]:  0.3398 [' 0.3398'] | PCC: 0.3927 ['0.3927'] | RMSE: 0.4665 ['0.4665']
Epoch    25: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 26 | Batch:   1 | Lr: 0.00250 | Time used(s): 128.5 | Training loss: 0.0749
Epoch: 26 | Batch:   2 | Lr: 0.00250 | Time used(s): 127.1 | Training loss: 0.0695
Epoch: 26 | Batch:   3 | Lr: 0.00250 | Time used(s): 126.0 | Training loss: 0.0597
Epoch: 26 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0565
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0678
Epoch: 26 |   [Val] | Loss: 0.0661 | [CCC]:  0.3994 [' 0.3994'] | PCC: 0.4065 ['0.4065'] | RMSE: 0.4209 ['0.4209']
Epoch: 27 | Batch:   1 | Lr: 0.00250 | Time used(s): 128.3 | Training loss: 0.0600
Epoch: 27 | Batch:   2 | Lr: 0.00250 | Time used(s): 126.4 | Training loss: 0.0632
Epoch: 27 | Batch:   3 | Lr: 0.00250 | Time used(s): 127.0 | Training loss: 0.0646
Epoch: 27 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.4 | Training loss: 0.0365
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0622
Epoch: 27 |   [Val] | Loss: 0.0751 | [CCC]:  0.3906 [' 0.3906'] | PCC: 0.4100 ['0.4100'] | RMSE: 0.4506 ['0.4506']
Epoch: 28 | Batch:   1 | Lr: 0.00250 | Time used(s): 128.8 | Training loss: 0.0586
Epoch: 28 | Batch:   2 | Lr: 0.00250 | Time used(s): 127.7 | Training loss: 0.0577
Epoch: 28 | Batch:   3 | Lr: 0.00250 | Time used(s): 125.7 | Training loss: 0.0649
Epoch: 28 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0785
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0607
Epoch: 28 |   [Val] | Loss: 0.0691 | [CCC]:  0.3932 [' 0.3932'] | PCC: 0.3974 ['0.3974'] | RMSE: 0.4282 ['0.4282']
Epoch: 29 | Batch:   1 | Lr: 0.00250 | Time used(s): 129.7 | Training loss: 0.0465
Epoch: 29 | Batch:   2 | Lr: 0.00250 | Time used(s): 126.9 | Training loss: 0.0555
Epoch: 29 | Batch:   3 | Lr: 0.00250 | Time used(s): 124.5 | Training loss: 0.0621
Epoch: 29 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0668
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0549
Epoch: 29 |   [Val] | Loss: 0.0695 | [CCC]:  0.3934 [' 0.3934'] | PCC: 0.4044 ['0.4044'] | RMSE: 0.4179 ['0.4179']
Epoch: 30 | Batch:   1 | Lr: 0.00250 | Time used(s): 129.4 | Training loss: 0.0495
Epoch: 30 | Batch:   2 | Lr: 0.00250 | Time used(s): 128.2 | Training loss: 0.0519
Epoch: 30 | Batch:   3 | Lr: 0.00250 | Time used(s): 126.6 | Training loss: 0.0450
Epoch: 30 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.0707
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0491
Epoch: 30 |   [Val] | Loss: 0.0694 | [CCC]:  0.4145 [' 0.4145'] | PCC: 0.4222 ['0.4222'] | RMSE: 0.4421 ['0.4421']
Epoch: 31 | Batch:   1 | Lr: 0.00250 | Time used(s): 127.6 | Training loss: 0.0477
Epoch: 31 | Batch:   2 | Lr: 0.00250 | Time used(s): 126.0 | Training loss: 0.0500
Epoch: 31 | Batch:   3 | Lr: 0.00250 | Time used(s): 127.4 | Training loss: 0.0422
Epoch: 31 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0427
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0466
Epoch: 31 |   [Val] | Loss: 0.0708 | [CCC]:  0.4061 [' 0.4061'] | PCC: 0.4152 ['0.4152'] | RMSE: 0.4249 ['0.4249']
Epoch    31: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 32 | Batch:   1 | Lr: 0.00125 | Time used(s): 128.0 | Training loss: 0.0447
Epoch: 32 | Batch:   2 | Lr: 0.00125 | Time used(s): 127.1 | Training loss: 0.0399
Epoch: 32 | Batch:   3 | Lr: 0.00125 | Time used(s): 123.3 | Training loss: 0.0375
Epoch: 32 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0658
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0411
Epoch: 32 |   [Val] | Loss: 0.0704 | [CCC]:  0.4077 [' 0.4077'] | PCC: 0.4118 ['0.4118'] | RMSE: 0.4292 ['0.4292']
Epoch: 33 | Batch:   1 | Lr: 0.00125 | Time used(s): 128.0 | Training loss: 0.0387
Epoch: 33 | Batch:   2 | Lr: 0.00125 | Time used(s): 126.6 | Training loss: 0.0398
Epoch: 33 | Batch:   3 | Lr: 0.00125 | Time used(s): 128.1 | Training loss: 0.0341
Epoch: 33 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.3 | Training loss: 0.1202
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0389
Epoch: 33 |   [Val] | Loss: 0.0659 | [CCC]:  0.3997 [' 0.3997'] | PCC: 0.4020 ['0.4020'] | RMSE: 0.4312 ['0.4312']
Epoch: 34 | Batch:   1 | Lr: 0.00125 | Time used(s): 128.9 | Training loss: 0.0272
Epoch: 34 | Batch:   2 | Lr: 0.00125 | Time used(s): 127.0 | Training loss: 0.0406
Epoch: 34 | Batch:   3 | Lr: 0.00125 | Time used(s): 123.6 | Training loss: 0.0385
Epoch: 34 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0537
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0357
Epoch: 34 |   [Val] | Loss: 0.0685 | [CCC]:  0.4043 [' 0.4043'] | PCC: 0.4093 ['0.4093'] | RMSE: 0.4510 ['0.4510']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 318 | Best [Val CCC]: 0.4236 [' 0.4236']| Loss: 0.0740 | PCC: 0.4277 ['0.4277'] | RMSE: 0.4669 ['0.4669']
On Test: CCC  0.5595 | PCC  0.5640 | RMSE  0.4029
****************************************************************************************************
Seed "318" over!
****************************************************************************************************
****************************************************************************************************
Using seed "319"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 319]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 127.9 | Training loss: 0.2506
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 126.3 | Training loss: 0.2471
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 121.7 | Training loss: 0.2391
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2418
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2456
Epoch:  1 |   [Val] | Loss: 0.2223 | [CCC]:  0.0777 [' 0.0777'] | PCC: 0.1814 ['0.1814'] | RMSE: 0.5154 ['0.5154']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_319_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 127.7 | Training loss: 0.2340
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 125.8 | Training loss: 0.2087
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 126.1 | Training loss: 0.2161
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.2043
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2193
Epoch:  2 |   [Val] | Loss: 0.1586 | [CCC]:  0.2117 [' 0.2117'] | PCC: 0.2354 ['0.2354'] | RMSE: 0.4867 ['0.4867']
Epoch:  2 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_319_None_None].pth"!
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 127.9 | Training loss: 0.1971
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 128.1 | Training loss: 0.1956
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 128.8 | Training loss: 0.1840
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1722
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.1919
Epoch:  3 |   [Val] | Loss: 0.1437 | [CCC]:  0.2339 [' 0.2339'] | PCC: 0.2695 ['0.2695'] | RMSE: 0.5932 ['0.5932']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_319_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 130.9 | Training loss: 0.1707
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 130.8 | Training loss: 0.1739
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 132.6 | Training loss: 0.1620
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.5 | Training loss: 0.1820
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1691
Epoch:  4 |   [Val] | Loss: 0.1136 | [CCC]:  0.3283 [' 0.3283'] | PCC: 0.3318 ['0.3318'] | RMSE: 0.4429 ['0.4429']
Epoch:  4 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_319_None_None].pth"!
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 132.5 | Training loss: 0.1530
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 132.9 | Training loss: 0.1593
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 132.6 | Training loss: 0.1604
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.5 | Training loss: 0.1779
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1579
Epoch:  5 |   [Val] | Loss: 0.1094 | [CCC]:  0.3129 [' 0.3129'] | PCC: 0.3191 ['0.3191'] | RMSE: 0.5232 ['0.5232']
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 131.7 | Training loss: 0.1443
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 132.2 | Training loss: 0.1471
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 133.4 | Training loss: 0.1378
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.5 | Training loss: 0.1262
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1428
Epoch:  6 |   [Val] | Loss: 0.1191 | [CCC]:  0.2847 [' 0.2847'] | PCC: 0.3197 ['0.3197'] | RMSE: 0.5546 ['0.5546']
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 133.8 | Training loss: 0.1425
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 131.8 | Training loss: 0.1361
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 97.3 | Training loss: 0.1356
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1244
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1378
Epoch:  7 |   [Val] | Loss: 0.1035 | [CCC]:  0.3094 [' 0.3094'] | PCC: 0.3348 ['0.3348'] | RMSE: 0.4977 ['0.4977']
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.3 | Training loss: 0.1379
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 128.6 | Training loss: 0.1258
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.8 | Training loss: 0.1330
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1204
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1321
Epoch:  8 |   [Val] | Loss: 0.1052 | [CCC]:  0.2989 [' 0.2989'] | PCC: 0.3408 ['0.3408'] | RMSE: 0.4684 ['0.4684']
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 125.2 | Training loss: 0.1278
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 114.3 | Training loss: 0.1246
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.4 | Training loss: 0.1262
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1009
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1258
Epoch:  9 |   [Val] | Loss: 0.1154 | [CCC]:  0.2758 [' 0.2758'] | PCC: 0.3340 ['0.3340'] | RMSE: 0.6216 ['0.6216']
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 123.4 | Training loss: 0.1315
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 113.6 | Training loss: 0.1253
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.3 | Training loss: 0.1193
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0793
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1246
Epoch: 10 |   [Val] | Loss: 0.0796 | [CCC]:  0.3633 [' 0.3633'] | PCC: 0.3785 ['0.3785'] | RMSE: 0.4180 ['0.4180']
Epoch: 10 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_319_None_None].pth"!
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 124.2 | Training loss: 0.1141
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 119.1 | Training loss: 0.1144
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 95.3 | Training loss: 0.1065
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1011
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1115
Epoch: 11 |   [Val] | Loss: 0.0816 | [CCC]:  0.3735 [' 0.3735'] | PCC: 0.3942 ['0.3942'] | RMSE: 0.5010 ['0.5010']
Epoch: 11 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_319_None_None].pth"!
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 120.1 | Training loss: 0.1098
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 118.8 | Training loss: 0.1019
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.0 | Training loss: 0.1070
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0873
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1059
Epoch: 12 |   [Val] | Loss: 0.0725 | [CCC]:  0.3976 [' 0.3976'] | PCC: 0.4013 ['0.4013'] | RMSE: 0.4436 ['0.4436']
Epoch: 12 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_319_None_None].pth"!
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 123.4 | Training loss: 0.1032
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 114.3 | Training loss: 0.0949
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 93.2 | Training loss: 0.0973
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1293
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.0990
Epoch: 13 |   [Val] | Loss: 0.0766 | [CCC]:  0.3930 [' 0.3930'] | PCC: 0.4105 ['0.4105'] | RMSE: 0.4173 ['0.4173']
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 123.1 | Training loss: 0.0976
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 117.2 | Training loss: 0.1052
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 93.1 | Training loss: 0.0985
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1134
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1007
Epoch: 14 |   [Val] | Loss: 0.0850 | [CCC]:  0.3529 [' 0.3529'] | PCC: 0.3729 ['0.3729'] | RMSE: 0.5459 ['0.5459']
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 125.8 | Training loss: 0.0979
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 117.2 | Training loss: 0.0961
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 97.1 | Training loss: 0.0949
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1152
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.0966
Epoch: 15 |   [Val] | Loss: 0.1103 | [CCC]:  0.2642 [' 0.2642'] | PCC: 0.3365 ['0.3365'] | RMSE: 0.5274 ['0.5274']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 119.0 | Training loss: 0.1155
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 119.2 | Training loss: 0.0892
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.4 | Training loss: 0.1116
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0542
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.1046
Epoch: 16 |   [Val] | Loss: 0.1196 | [CCC]:  0.2554 [' 0.2554'] | PCC: 0.3319 ['0.3319'] | RMSE: 0.6020 ['0.6020']
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 123.7 | Training loss: 0.1139
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 116.0 | Training loss: 0.1097
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.1 | Training loss: 0.0994
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0841
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.1073
Epoch: 17 |   [Val] | Loss: 0.0770 | [CCC]:  0.3810 [' 0.3810'] | PCC: 0.3978 ['0.3978'] | RMSE: 0.4013 ['0.4013']
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 118.4 | Training loss: 0.0926
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 117.7 | Training loss: 0.0957
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.6 | Training loss: 0.0998
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0883
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.0959
Epoch: 18 |   [Val] | Loss: 0.0756 | [CCC]:  0.4153 [' 0.4153'] | PCC: 0.4165 ['0.4165'] | RMSE: 0.4581 ['0.4581']
Epoch: 18 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_319_None_None].pth"!
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 123.9 | Training loss: 0.0912
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 113.8 | Training loss: 0.0830
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.1 | Training loss: 0.0899
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0873
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.0880
Epoch: 19 |   [Val] | Loss: 0.0726 | [CCC]:  0.4074 [' 0.4074'] | PCC: 0.4082 ['0.4082'] | RMSE: 0.4340 ['0.4340']
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 120.6 | Training loss: 0.0884
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 109.4 | Training loss: 0.0868
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.7 | Training loss: 0.0812
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1047
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0858
Epoch: 20 |   [Val] | Loss: 0.1059 | [CCC]:  0.3110 [' 0.3110'] | PCC: 0.3974 ['0.3974'] | RMSE: 0.5712 ['0.5712']
Epoch: 21 | Batch:   1 | Lr: 0.00500 | Time used(s): 120.3 | Training loss: 0.1077
Epoch: 21 | Batch:   2 | Lr: 0.00500 | Time used(s): 118.0 | Training loss: 0.0952
Epoch: 21 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.8 | Training loss: 0.0919
Epoch: 21 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1021
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0983
Epoch: 21 |   [Val] | Loss: 0.0718 | [CCC]:  0.4021 [' 0.4021'] | PCC: 0.4124 ['0.4124'] | RMSE: 0.4047 ['0.4047']
Epoch: 22 | Batch:   1 | Lr: 0.00500 | Time used(s): 114.4 | Training loss: 0.0774
Epoch: 22 | Batch:   2 | Lr: 0.00500 | Time used(s): 95.0 | Training loss: 0.0810
Epoch: 22 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.7 | Training loss: 0.0979
Epoch: 22 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0654
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0851
Epoch: 22 |   [Val] | Loss: 0.0776 | [CCC]:  0.3892 [' 0.3892'] | PCC: 0.3985 ['0.3985'] | RMSE: 0.4631 ['0.4631']
Epoch: 23 | Batch:   1 | Lr: 0.00500 | Time used(s): 115.6 | Training loss: 0.0821
Epoch: 23 | Batch:   2 | Lr: 0.00500 | Time used(s): 100.1 | Training loss: 0.0875
Epoch: 23 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.8 | Training loss: 0.0778
Epoch: 23 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1020
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0828
Epoch: 23 |   [Val] | Loss: 0.0927 | [CCC]:  0.3333 [' 0.3333'] | PCC: 0.3890 ['0.3890'] | RMSE: 0.4989 ['0.4989']
Epoch: 24 | Batch:   1 | Lr: 0.00500 | Time used(s): 114.1 | Training loss: 0.0787
Epoch: 24 | Batch:   2 | Lr: 0.00500 | Time used(s): 99.4 | Training loss: 0.0697
Epoch: 24 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.8 | Training loss: 0.0794
Epoch: 24 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0756
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0759
Epoch: 24 |   [Val] | Loss: 0.0682 | [CCC]:  0.4269 [' 0.4269'] | PCC: 0.4271 ['0.4271'] | RMSE: 0.4440 ['0.4440']
Epoch: 24 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_319_None_None].pth"!
Epoch: 25 | Batch:   1 | Lr: 0.00500 | Time used(s): 110.9 | Training loss: 0.0727
Epoch: 25 | Batch:   2 | Lr: 0.00500 | Time used(s): 95.4 | Training loss: 0.0712
Epoch: 25 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.6 | Training loss: 0.0723
Epoch: 25 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0932
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0724
Epoch: 25 |   [Val] | Loss: 0.0764 | [CCC]:  0.3947 [' 0.3947'] | PCC: 0.4194 ['0.4194'] | RMSE: 0.4232 ['0.4232']
Epoch: 26 | Batch:   1 | Lr: 0.00500 | Time used(s): 109.6 | Training loss: 0.0700
Epoch: 26 | Batch:   2 | Lr: 0.00500 | Time used(s): 93.5 | Training loss: 0.0724
Epoch: 26 | Batch:   3 | Lr: 0.00500 | Time used(s): 82.9 | Training loss: 0.0735
Epoch: 26 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.0701
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0719
Epoch: 26 |   [Val] | Loss: 0.0754 | [CCC]:  0.3910 [' 0.3910'] | PCC: 0.4025 ['0.4025'] | RMSE: 0.4244 ['0.4244']
Epoch: 27 | Batch:   1 | Lr: 0.00500 | Time used(s): 100.6 | Training loss: 0.0571
Epoch: 27 | Batch:   2 | Lr: 0.00500 | Time used(s): 96.7 | Training loss: 0.0760
Epoch: 27 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.3 | Training loss: 0.0649
Epoch: 27 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1221
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0669
Epoch: 27 |   [Val] | Loss: 0.0741 | [CCC]:  0.3961 [' 0.3961'] | PCC: 0.4195 ['0.4195'] | RMSE: 0.4874 ['0.4874']
Epoch: 28 | Batch:   1 | Lr: 0.00500 | Time used(s): 106.4 | Training loss: 0.0721
Epoch: 28 | Batch:   2 | Lr: 0.00500 | Time used(s): 94.5 | Training loss: 0.0630
Epoch: 28 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.9 | Training loss: 0.0671
Epoch: 28 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1008
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0679
Epoch: 28 |   [Val] | Loss: 0.0694 | [CCC]:  0.4106 [' 0.4106'] | PCC: 0.4226 ['0.4226'] | RMSE: 0.4028 ['0.4028']
Epoch: 29 | Batch:   1 | Lr: 0.00500 | Time used(s): 115.3 | Training loss: 0.0640
Epoch: 29 | Batch:   2 | Lr: 0.00500 | Time used(s): 98.4 | Training loss: 0.0777
Epoch: 29 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.4 | Training loss: 0.0560
Epoch: 29 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0562
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0658
Epoch: 29 |   [Val] | Loss: 0.0683 | [CCC]:  0.4096 [' 0.4096'] | PCC: 0.4103 ['0.4103'] | RMSE: 0.4423 ['0.4423']
Epoch: 30 | Batch:   1 | Lr: 0.00500 | Time used(s): 104.2 | Training loss: 0.0653
Epoch: 30 | Batch:   2 | Lr: 0.00500 | Time used(s): 100.6 | Training loss: 0.0758
Epoch: 30 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.9 | Training loss: 0.0563
Epoch: 30 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0022
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0648
Epoch: 30 |   [Val] | Loss: 0.0794 | [CCC]:  0.4042 [' 0.4042'] | PCC: 0.4154 ['0.4154'] | RMSE: 0.4419 ['0.4419']
Epoch    30: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 31 | Batch:   1 | Lr: 0.00250 | Time used(s): 111.4 | Training loss: 0.0579
Epoch: 31 | Batch:   2 | Lr: 0.00250 | Time used(s): 95.3 | Training loss: 0.0610
Epoch: 31 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.1 | Training loss: 0.0613
Epoch: 31 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0646
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0601
Epoch: 31 |   [Val] | Loss: 0.0697 | [CCC]:  0.4102 [' 0.4102'] | PCC: 0.4167 ['0.4167'] | RMSE: 0.4132 ['0.4132']
Epoch: 32 | Batch:   1 | Lr: 0.00250 | Time used(s): 105.2 | Training loss: 0.0529
Epoch: 32 | Batch:   2 | Lr: 0.00250 | Time used(s): 96.4 | Training loss: 0.0511
Epoch: 32 | Batch:   3 | Lr: 0.00250 | Time used(s): 88.0 | Training loss: 0.0557
Epoch: 32 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0532
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0533
Epoch: 32 |   [Val] | Loss: 0.0705 | [CCC]:  0.4098 [' 0.4098'] | PCC: 0.4106 ['0.4106'] | RMSE: 0.4568 ['0.4568']
Epoch: 33 | Batch:   1 | Lr: 0.00250 | Time used(s): 110.1 | Training loss: 0.0450
Epoch: 33 | Batch:   2 | Lr: 0.00250 | Time used(s): 93.8 | Training loss: 0.0471
Epoch: 33 | Batch:   3 | Lr: 0.00250 | Time used(s): 87.1 | Training loss: 0.0530
Epoch: 33 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0842
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0489
Epoch: 33 |   [Val] | Loss: 0.0734 | [CCC]:  0.4115 [' 0.4115'] | PCC: 0.4127 ['0.4127'] | RMSE: 0.4289 ['0.4289']
Epoch: 34 | Batch:   1 | Lr: 0.00250 | Time used(s): 105.7 | Training loss: 0.0440
Epoch: 34 | Batch:   2 | Lr: 0.00250 | Time used(s): 94.0 | Training loss: 0.0453
Epoch: 34 | Batch:   3 | Lr: 0.00250 | Time used(s): 87.1 | Training loss: 0.0441
Epoch: 34 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0296
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0442
Epoch: 34 |   [Val] | Loss: 0.0736 | [CCC]:  0.4038 [' 0.4038'] | PCC: 0.4040 ['0.4040'] | RMSE: 0.4486 ['0.4486']
Epoch: 35 | Batch:   1 | Lr: 0.00250 | Time used(s): 106.8 | Training loss: 0.0481
Epoch: 35 | Batch:   2 | Lr: 0.00250 | Time used(s): 95.1 | Training loss: 0.0401
Epoch: 35 | Batch:   3 | Lr: 0.00250 | Time used(s): 87.8 | Training loss: 0.0366
Epoch: 35 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0785
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0422
Epoch: 35 |   [Val] | Loss: 0.0745 | [CCC]:  0.3945 [' 0.3945'] | PCC: 0.3989 ['0.3989'] | RMSE: 0.4446 ['0.4446']
Epoch: 36 | Batch:   1 | Lr: 0.00250 | Time used(s): 114.2 | Training loss: 0.0333
Epoch: 36 | Batch:   2 | Lr: 0.00250 | Time used(s): 96.4 | Training loss: 0.0395
Epoch: 36 | Batch:   3 | Lr: 0.00250 | Time used(s): 87.5 | Training loss: 0.0450
Epoch: 36 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0497
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0394
Epoch: 36 |   [Val] | Loss: 0.0763 | [CCC]:  0.3954 [' 0.3954'] | PCC: 0.3983 ['0.3983'] | RMSE: 0.4439 ['0.4439']
Epoch    36: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 37 | Batch:   1 | Lr: 0.00125 | Time used(s): 114.7 | Training loss: 0.0382
Epoch: 37 | Batch:   2 | Lr: 0.00125 | Time used(s): 96.8 | Training loss: 0.0388
Epoch: 37 | Batch:   3 | Lr: 0.00125 | Time used(s): 87.5 | Training loss: 0.0335
Epoch: 37 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0023
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0363
Epoch: 37 |   [Val] | Loss: 0.0774 | [CCC]:  0.3892 [' 0.3892'] | PCC: 0.4029 ['0.4029'] | RMSE: 0.4758 ['0.4758']
Epoch: 38 | Batch:   1 | Lr: 0.00125 | Time used(s): 104.8 | Training loss: 0.0428
Epoch: 38 | Batch:   2 | Lr: 0.00125 | Time used(s): 95.2 | Training loss: 0.0482
Epoch: 38 | Batch:   3 | Lr: 0.00125 | Time used(s): 86.0 | Training loss: 0.0287
Epoch: 38 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0313
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0398
Epoch: 38 |   [Val] | Loss: 0.0790 | [CCC]:  0.3819 [' 0.3819'] | PCC: 0.3958 ['0.3958'] | RMSE: 0.4481 ['0.4481']
Epoch: 39 | Batch:   1 | Lr: 0.00125 | Time used(s): 106.2 | Training loss: 0.0444
Epoch: 39 | Batch:   2 | Lr: 0.00125 | Time used(s): 95.0 | Training loss: 0.0402
Epoch: 39 | Batch:   3 | Lr: 0.00125 | Time used(s): 87.3 | Training loss: 0.0339
Epoch: 39 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0178
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0391
Epoch: 39 |   [Val] | Loss: 0.0762 | [CCC]:  0.3890 [' 0.3890'] | PCC: 0.4021 ['0.4021'] | RMSE: 0.4419 ['0.4419']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 319 | Best [Val CCC]: 0.4269 [' 0.4269']| Loss: 0.0682 | PCC: 0.4271 ['0.4271'] | RMSE: 0.4440 ['0.4440']
On Test: CCC  0.5326 | PCC  0.5517 | RMSE  0.4077
****************************************************************************************************
Seed "319" over!
****************************************************************************************************
****************************************************************************************************
Using seed "320"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 320]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 114.1 | Training loss: 0.2499
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 94.0 | Training loss: 0.2471
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.6 | Training loss: 0.2390
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.2119
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2448
Epoch:  1 |   [Val] | Loss: 0.1980 | [CCC]:  0.0867 [' 0.0867'] | PCC: 0.1988 ['0.1988'] | RMSE: 0.6189 ['0.6189']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 105.2 | Training loss: 0.2187
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 94.8 | Training loss: 0.2345
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.5 | Training loss: 0.2321
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.2032
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2281
Epoch:  2 |   [Val] | Loss: 0.1772 | [CCC]:  0.1509 [' 0.1509'] | PCC: 0.1881 ['0.1881'] | RMSE: 0.4622 ['0.4622']
Epoch:  2 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 121.4 | Training loss: 0.2060
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 104.5 | Training loss: 0.1977
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 93.6 | Training loss: 0.1897
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.2007
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.1978
Epoch:  3 |   [Val] | Loss: 0.2154 | [CCC]:  0.0643 [' 0.0643'] | PCC: 0.2390 ['0.2390'] | RMSE: 0.8874 ['0.8874']
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 119.4 | Training loss: 0.2309
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 101.8 | Training loss: 0.2237
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.9 | Training loss: 0.2089
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1869
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.2206
Epoch:  4 |   [Val] | Loss: 0.1337 | [CCC]:  0.2824 [' 0.2824'] | PCC: 0.2903 ['0.2903'] | RMSE: 0.4823 ['0.4823']
Epoch:  4 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 116.4 | Training loss: 0.1656
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 104.0 | Training loss: 0.1825
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 133.1 | Training loss: 0.1573
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1905
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1688
Epoch:  5 |   [Val] | Loss: 0.1314 | [CCC]:  0.2618 [' 0.2618'] | PCC: 0.2936 ['0.2936'] | RMSE: 0.4259 ['0.4259']
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 134.9 | Training loss: 0.1651
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 135.2 | Training loss: 0.1670
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 107.7 | Training loss: 0.1555
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1302
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1620
Epoch:  6 |   [Val] | Loss: 0.1413 | [CCC]:  0.2402 [' 0.2402'] | PCC: 0.3112 ['0.3112'] | RMSE: 0.6232 ['0.6232']
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 112.6 | Training loss: 0.1700
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 100.3 | Training loss: 0.1597
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.1 | Training loss: 0.1385
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1676
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1563
Epoch:  7 |   [Val] | Loss: 0.1035 | [CCC]:  0.3212 [' 0.3212'] | PCC: 0.3332 ['0.3332'] | RMSE: 0.4229 ['0.4229']
Epoch:  7 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.9 | Training loss: 0.1453
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 109.1 | Training loss: 0.1443
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.1 | Training loss: 0.1327
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1498
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1409
Epoch:  8 |   [Val] | Loss: 0.0914 | [CCC]:  0.3482 [' 0.3482'] | PCC: 0.3624 ['0.3624'] | RMSE: 0.5358 ['0.5358']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 120.8 | Training loss: 0.1311
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 108.1 | Training loss: 0.1412
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.2 | Training loss: 0.1267
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1540
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1334
Epoch:  9 |   [Val] | Loss: 0.0961 | [CCC]:  0.3334 [' 0.3334'] | PCC: 0.3552 ['0.3552'] | RMSE: 0.4648 ['0.4648']
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 119.5 | Training loss: 0.1388
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 113.8 | Training loss: 0.1217
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.1 | Training loss: 0.1322
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1199
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1307
Epoch: 10 |   [Val] | Loss: 0.1070 | [CCC]:  0.3108 [' 0.3108'] | PCC: 0.3395 ['0.3395'] | RMSE: 0.4808 ['0.4808']
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 121.8 | Training loss: 0.1255
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 126.2 | Training loss: 0.1152
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 120.8 | Training loss: 0.1253
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1215
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1220
Epoch: 11 |   [Val] | Loss: 0.0886 | [CCC]:  0.3506 [' 0.3506'] | PCC: 0.3675 ['0.3675'] | RMSE: 0.4081 ['0.4081']
Epoch: 11 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 120.3 | Training loss: 0.1152
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 103.5 | Training loss: 0.1136
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.4 | Training loss: 0.1135
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1128
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1141
Epoch: 12 |   [Val] | Loss: 0.0931 | [CCC]:  0.3567 [' 0.3567'] | PCC: 0.3891 ['0.3891'] | RMSE: 0.5376 ['0.5376']
Epoch: 12 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 116.4 | Training loss: 0.1324
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 108.4 | Training loss: 0.1020
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.7 | Training loss: 0.1178
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1015
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1171
Epoch: 13 |   [Val] | Loss: 0.1141 | [CCC]:  0.2902 [' 0.2902'] | PCC: 0.3743 ['0.3743'] | RMSE: 0.5599 ['0.5599']
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 116.6 | Training loss: 0.1367
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 109.0 | Training loss: 0.1172
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.8 | Training loss: 0.1296
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1324
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1279
Epoch: 14 |   [Val] | Loss: 0.0947 | [CCC]:  0.3252 [' 0.3252'] | PCC: 0.3592 ['0.3592'] | RMSE: 0.4591 ['0.4591']
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 116.4 | Training loss: 0.1241
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 105.6 | Training loss: 0.1322
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.2 | Training loss: 0.1300
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0967
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1283
Epoch: 15 |   [Val] | Loss: 0.1132 | [CCC]:  0.2636 [' 0.2636'] | PCC: 0.3526 ['0.3526'] | RMSE: 0.4418 ['0.4418']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 116.6 | Training loss: 0.1301
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 105.9 | Training loss: 0.1310
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.8 | Training loss: 0.1115
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0905
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.1237
Epoch: 16 |   [Val] | Loss: 0.0832 | [CCC]:  0.3663 [' 0.3663'] | PCC: 0.3743 ['0.3743'] | RMSE: 0.4770 ['0.4770']
Epoch: 16 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 114.9 | Training loss: 0.1136
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 106.0 | Training loss: 0.1121
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.3 | Training loss: 0.1014
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0997
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.1089
Epoch: 17 |   [Val] | Loss: 0.0801 | [CCC]:  0.3963 [' 0.3963'] | PCC: 0.4022 ['0.4022'] | RMSE: 0.4234 ['0.4234']
Epoch: 17 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 127.7 | Training loss: 0.1076
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 116.2 | Training loss: 0.1017
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 96.5 | Training loss: 0.0968
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0877
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.1018
Epoch: 18 |   [Val] | Loss: 0.0862 | [CCC]:  0.3850 [' 0.3850'] | PCC: 0.4066 ['0.4066'] | RMSE: 0.4520 ['0.4520']
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 115.6 | Training loss: 0.1076
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 112.0 | Training loss: 0.0928
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 93.9 | Training loss: 0.0961
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0493
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.0980
Epoch: 19 |   [Val] | Loss: 0.0814 | [CCC]:  0.4036 [' 0.4036'] | PCC: 0.4085 ['0.4085'] | RMSE: 0.4444 ['0.4444']
Epoch: 19 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 123.1 | Training loss: 0.0990
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 114.3 | Training loss: 0.0900
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.7 | Training loss: 0.0960
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0796
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0948
Epoch: 20 |   [Val] | Loss: 0.0860 | [CCC]:  0.3970 [' 0.3970'] | PCC: 0.4038 ['0.4038'] | RMSE: 0.4469 ['0.4469']
Epoch: 21 | Batch:   1 | Lr: 0.00500 | Time used(s): 117.7 | Training loss: 0.0875
Epoch: 21 | Batch:   2 | Lr: 0.00500 | Time used(s): 103.0 | Training loss: 0.0878
Epoch: 21 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.3 | Training loss: 0.0905
Epoch: 21 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0899
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0886
Epoch: 21 |   [Val] | Loss: 0.0785 | [CCC]:  0.3897 [' 0.3897'] | PCC: 0.3986 ['0.3986'] | RMSE: 0.4115 ['0.4115']
Epoch: 22 | Batch:   1 | Lr: 0.00500 | Time used(s): 117.7 | Training loss: 0.0964
Epoch: 22 | Batch:   2 | Lr: 0.00500 | Time used(s): 109.4 | Training loss: 0.0822
Epoch: 22 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.6 | Training loss: 0.0860
Epoch: 22 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0546
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0877
Epoch: 22 |   [Val] | Loss: 0.0749 | [CCC]:  0.4022 [' 0.4022'] | PCC: 0.4036 ['0.4036'] | RMSE: 0.4293 ['0.4293']
Epoch: 23 | Batch:   1 | Lr: 0.00500 | Time used(s): 128.9 | Training loss: 0.0769
Epoch: 23 | Batch:   2 | Lr: 0.00500 | Time used(s): 138.6 | Training loss: 0.0895
Epoch: 23 | Batch:   3 | Lr: 0.00500 | Time used(s): 132.8 | Training loss: 0.0844
Epoch: 23 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0985
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0838
Epoch: 23 |   [Val] | Loss: 0.0730 | [CCC]:  0.4151 [' 0.4151'] | PCC: 0.4162 ['0.4162'] | RMSE: 0.4522 ['0.4522']
Epoch: 23 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch: 24 | Batch:   1 | Lr: 0.00500 | Time used(s): 118.9 | Training loss: 0.0893
Epoch: 24 | Batch:   2 | Lr: 0.00500 | Time used(s): 130.7 | Training loss: 0.0793
Epoch: 24 | Batch:   3 | Lr: 0.00500 | Time used(s): 98.9 | Training loss: 0.0716
Epoch: 24 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0934
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0803
Epoch: 24 |   [Val] | Loss: 0.0730 | [CCC]:  0.4102 [' 0.4102'] | PCC: 0.4133 ['0.4133'] | RMSE: 0.4226 ['0.4226']
Epoch: 25 | Batch:   1 | Lr: 0.00500 | Time used(s): 127.2 | Training loss: 0.0719
Epoch: 25 | Batch:   2 | Lr: 0.00500 | Time used(s): 123.1 | Training loss: 0.0820
Epoch: 25 | Batch:   3 | Lr: 0.00500 | Time used(s): 136.6 | Training loss: 0.0706
Epoch: 25 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.0923
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0751
Epoch: 25 |   [Val] | Loss: 0.0725 | [CCC]:  0.4003 [' 0.4003'] | PCC: 0.4053 ['0.4053'] | RMSE: 0.4384 ['0.4384']
Epoch: 26 | Batch:   1 | Lr: 0.00500 | Time used(s): 158.5 | Training loss: 0.0727
Epoch: 26 | Batch:   2 | Lr: 0.00500 | Time used(s): 154.9 | Training loss: 0.0776
Epoch: 26 | Batch:   3 | Lr: 0.00500 | Time used(s): 149.1 | Training loss: 0.0739
Epoch: 26 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.0663
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0746
Epoch: 26 |   [Val] | Loss: 0.0765 | [CCC]:  0.4122 [' 0.4122'] | PCC: 0.4297 ['0.4297'] | RMSE: 0.4544 ['0.4544']
Epoch: 27 | Batch:   1 | Lr: 0.00500 | Time used(s): 141.0 | Training loss: 0.0708
Epoch: 27 | Batch:   2 | Lr: 0.00500 | Time used(s): 121.1 | Training loss: 0.0783
Epoch: 27 | Batch:   3 | Lr: 0.00500 | Time used(s): 112.6 | Training loss: 0.0839
Epoch: 27 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0361
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0770
Epoch: 27 |   [Val] | Loss: 0.0717 | [CCC]:  0.4087 [' 0.4087'] | PCC: 0.4236 ['0.4236'] | RMSE: 0.4427 ['0.4427']
Epoch: 28 | Batch:   1 | Lr: 0.00500 | Time used(s): 132.4 | Training loss: 0.0777
Epoch: 28 | Batch:   2 | Lr: 0.00500 | Time used(s): 129.9 | Training loss: 0.0775
Epoch: 28 | Batch:   3 | Lr: 0.00500 | Time used(s): 125.7 | Training loss: 0.0899
Epoch: 28 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0681
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0815
Epoch: 28 |   [Val] | Loss: 0.0845 | [CCC]:  0.3515 [' 0.3515'] | PCC: 0.4200 ['0.4200'] | RMSE: 0.5000 ['0.5000']
Epoch: 29 | Batch:   1 | Lr: 0.00500 | Time used(s): 127.7 | Training loss: 0.0847
Epoch: 29 | Batch:   2 | Lr: 0.00500 | Time used(s): 115.1 | Training loss: 0.0780
Epoch: 29 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.9 | Training loss: 0.0792
Epoch: 29 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1024
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0810
Epoch: 29 |   [Val] | Loss: 0.0704 | [CCC]:  0.4188 [' 0.4188'] | PCC: 0.4189 ['0.4189'] | RMSE: 0.4413 ['0.4413']
Epoch: 29 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch: 30 | Batch:   1 | Lr: 0.00500 | Time used(s): 84.1 | Training loss: 0.0628
Epoch: 30 | Batch:   2 | Lr: 0.00500 | Time used(s): 104.0 | Training loss: 0.0634
Epoch: 30 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.8 | Training loss: 0.0758
Epoch: 30 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0817
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0676
Epoch: 30 |   [Val] | Loss: 0.0704 | [CCC]:  0.4244 [' 0.4244'] | PCC: 0.4259 ['0.4259'] | RMSE: 0.4382 ['0.4382']
Epoch: 30 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch: 31 | Batch:   1 | Lr: 0.00500 | Time used(s): 115.9 | Training loss: 0.0605
Epoch: 31 | Batch:   2 | Lr: 0.00500 | Time used(s): 107.6 | Training loss: 0.0683
Epoch: 31 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.2 | Training loss: 0.0705
Epoch: 31 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0257
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0658
Epoch: 31 |   [Val] | Loss: 0.0666 | [CCC]:  0.4286 [' 0.4286'] | PCC: 0.4299 ['0.4299'] | RMSE: 0.4306 ['0.4306']
Epoch: 31 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch: 32 | Batch:   1 | Lr: 0.00500 | Time used(s): 133.3 | Training loss: 0.0613
Epoch: 32 | Batch:   2 | Lr: 0.00500 | Time used(s): 132.0 | Training loss: 0.0647
Epoch: 32 | Batch:   3 | Lr: 0.00500 | Time used(s): 140.3 | Training loss: 0.0616
Epoch: 32 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0133
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0617
Epoch: 32 |   [Val] | Loss: 0.0722 | [CCC]:  0.3831 [' 0.3831'] | PCC: 0.4180 ['0.4180'] | RMSE: 0.4020 ['0.4020']
Epoch: 33 | Batch:   1 | Lr: 0.00500 | Time used(s): 143.9 | Training loss: 0.0634
Epoch: 33 | Batch:   2 | Lr: 0.00500 | Time used(s): 94.3 | Training loss: 0.0637
Epoch: 33 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.8 | Training loss: 0.0683
Epoch: 33 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0426
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0648
Epoch: 33 |   [Val] | Loss: 0.0755 | [CCC]:  0.4153 [' 0.4153'] | PCC: 0.4253 ['0.4253'] | RMSE: 0.4526 ['0.4526']
Epoch: 34 | Batch:   1 | Lr: 0.00500 | Time used(s): 154.3 | Training loss: 0.0651
Epoch: 34 | Batch:   2 | Lr: 0.00500 | Time used(s): 138.2 | Training loss: 0.0693
Epoch: 34 | Batch:   3 | Lr: 0.00500 | Time used(s): 126.2 | Training loss: 0.0566
Epoch: 34 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.0550
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0635
Epoch: 34 |   [Val] | Loss: 0.0841 | [CCC]:  0.3736 [' 0.3736'] | PCC: 0.4040 ['0.4040'] | RMSE: 0.4527 ['0.4527']
Epoch: 35 | Batch:   1 | Lr: 0.00500 | Time used(s): 175.1 | Training loss: 0.0666
Epoch: 35 | Batch:   2 | Lr: 0.00500 | Time used(s): 152.2 | Training loss: 0.0606
Epoch: 35 | Batch:   3 | Lr: 0.00500 | Time used(s): 173.8 | Training loss: 0.0543
Epoch: 35 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0652
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0605
Epoch: 35 |   [Val] | Loss: 0.0715 | [CCC]:  0.3839 [' 0.3839'] | PCC: 0.3901 ['0.3901'] | RMSE: 0.4178 ['0.4178']
Epoch: 36 | Batch:   1 | Lr: 0.00500 | Time used(s): 142.8 | Training loss: 0.0728
Epoch: 36 | Batch:   2 | Lr: 0.00500 | Time used(s): 134.9 | Training loss: 0.0702
Epoch: 36 | Batch:   3 | Lr: 0.00500 | Time used(s): 137.4 | Training loss: 0.0607
Epoch: 36 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.0414
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0675
Epoch: 36 |   [Val] | Loss: 0.0808 | [CCC]:  0.3817 [' 0.3817'] | PCC: 0.3820 ['0.3820'] | RMSE: 0.4632 ['0.4632']
Epoch: 37 | Batch:   1 | Lr: 0.00500 | Time used(s): 126.2 | Training loss: 0.0843
Epoch: 37 | Batch:   2 | Lr: 0.00500 | Time used(s): 136.1 | Training loss: 0.0660
Epoch: 37 | Batch:   3 | Lr: 0.00500 | Time used(s): 130.6 | Training loss: 0.0687
Epoch: 37 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0645
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0729
Epoch: 37 |   [Val] | Loss: 0.0622 | [CCC]:  0.4208 [' 0.4208'] | PCC: 0.4283 ['0.4283'] | RMSE: 0.4043 ['0.4043']
Epoch    37: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 38 | Batch:   1 | Lr: 0.00250 | Time used(s): 126.0 | Training loss: 0.0522
Epoch: 38 | Batch:   2 | Lr: 0.00250 | Time used(s): 136.0 | Training loss: 0.0644
Epoch: 38 | Batch:   3 | Lr: 0.00250 | Time used(s): 129.4 | Training loss: 0.0551
Epoch: 38 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0686
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0574
Epoch: 38 |   [Val] | Loss: 0.0648 | [CCC]:  0.4377 [' 0.4377'] | PCC: 0.4391 ['0.4391'] | RMSE: 0.4384 ['0.4384']
Epoch: 38 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_320_None_None].pth"!
Epoch: 39 | Batch:   1 | Lr: 0.00250 | Time used(s): 153.1 | Training loss: 0.0573
Epoch: 39 | Batch:   2 | Lr: 0.00250 | Time used(s): 135.0 | Training loss: 0.0509
Epoch: 39 | Batch:   3 | Lr: 0.00250 | Time used(s): 142.2 | Training loss: 0.0458
Epoch: 39 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0476
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0513
Epoch: 39 |   [Val] | Loss: 0.0725 | [CCC]:  0.4058 [' 0.4058'] | PCC: 0.4111 ['0.4111'] | RMSE: 0.4131 ['0.4131']
Epoch: 40 | Batch:   1 | Lr: 0.00250 | Time used(s): 144.4 | Training loss: 0.0564
Epoch: 40 | Batch:   2 | Lr: 0.00250 | Time used(s): 118.1 | Training loss: 0.0504
Epoch: 40 | Batch:   3 | Lr: 0.00250 | Time used(s): 132.3 | Training loss: 0.0454
Epoch: 40 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0395
--------------------------------------------------
Epoch: 40 | [Train] | Loss: 0.0506
Epoch: 40 |   [Val] | Loss: 0.0733 | [CCC]:  0.4059 [' 0.4059'] | PCC: 0.4106 ['0.4106'] | RMSE: 0.4462 ['0.4462']
Epoch: 41 | Batch:   1 | Lr: 0.00250 | Time used(s): 139.6 | Training loss: 0.0439
Epoch: 41 | Batch:   2 | Lr: 0.00250 | Time used(s): 138.2 | Training loss: 0.0514
Epoch: 41 | Batch:   3 | Lr: 0.00250 | Time used(s): 132.6 | Training loss: 0.0423
Epoch: 41 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0907
--------------------------------------------------
Epoch: 41 | [Train] | Loss: 0.0466
Epoch: 41 |   [Val] | Loss: 0.0719 | [CCC]:  0.3951 [' 0.3951'] | PCC: 0.4118 ['0.4118'] | RMSE: 0.4269 ['0.4269']
Epoch: 42 | Batch:   1 | Lr: 0.00250 | Time used(s): 135.0 | Training loss: 0.0465
Epoch: 42 | Batch:   2 | Lr: 0.00250 | Time used(s): 131.4 | Training loss: 0.0460
Epoch: 42 | Batch:   3 | Lr: 0.00250 | Time used(s): 133.5 | Training loss: 0.0471
Epoch: 42 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0635
--------------------------------------------------
Epoch: 42 | [Train] | Loss: 0.0468
Epoch: 42 |   [Val] | Loss: 0.0745 | [CCC]:  0.4099 [' 0.4099'] | PCC: 0.4187 ['0.4187'] | RMSE: 0.4435 ['0.4435']
Epoch: 43 | Batch:   1 | Lr: 0.00250 | Time used(s): 134.1 | Training loss: 0.0410
Epoch: 43 | Batch:   2 | Lr: 0.00250 | Time used(s): 135.3 | Training loss: 0.0426
Epoch: 43 | Batch:   3 | Lr: 0.00250 | Time used(s): 132.3 | Training loss: 0.0460
Epoch: 43 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0168
--------------------------------------------------
Epoch: 43 | [Train] | Loss: 0.0428
Epoch: 43 |   [Val] | Loss: 0.0725 | [CCC]:  0.4125 [' 0.4125'] | PCC: 0.4171 ['0.4171'] | RMSE: 0.4186 ['0.4186']
Epoch: 44 | Batch:   1 | Lr: 0.00250 | Time used(s): 133.9 | Training loss: 0.0403
Epoch: 44 | Batch:   2 | Lr: 0.00250 | Time used(s): 138.1 | Training loss: 0.0423
Epoch: 44 | Batch:   3 | Lr: 0.00250 | Time used(s): 142.4 | Training loss: 0.0350
Epoch: 44 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0179
--------------------------------------------------
Epoch: 44 | [Train] | Loss: 0.0389
Epoch: 44 |   [Val] | Loss: 0.0744 | [CCC]:  0.4126 [' 0.4126'] | PCC: 0.4245 ['0.4245'] | RMSE: 0.4320 ['0.4320']
Epoch    44: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 45 | Batch:   1 | Lr: 0.00125 | Time used(s): 141.8 | Training loss: 0.0366
Epoch: 45 | Batch:   2 | Lr: 0.00125 | Time used(s): 139.7 | Training loss: 0.0430
Epoch: 45 | Batch:   3 | Lr: 0.00125 | Time used(s): 138.8 | Training loss: 0.0253
Epoch: 45 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.3 | Training loss: 0.0320
--------------------------------------------------
Epoch: 45 | [Train] | Loss: 0.0349
Epoch: 45 |   [Val] | Loss: 0.0723 | [CCC]:  0.4164 [' 0.4164'] | PCC: 0.4190 ['0.4190'] | RMSE: 0.4235 ['0.4235']
Epoch: 46 | Batch:   1 | Lr: 0.00125 | Time used(s): 147.1 | Training loss: 0.0352
Epoch: 46 | Batch:   2 | Lr: 0.00125 | Time used(s): 128.5 | Training loss: 0.0255
Epoch: 46 | Batch:   3 | Lr: 0.00125 | Time used(s): 120.3 | Training loss: 0.0350
Epoch: 46 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0341
--------------------------------------------------
Epoch: 46 | [Train] | Loss: 0.0319
Epoch: 46 |   [Val] | Loss: 0.0738 | [CCC]:  0.4079 [' 0.4079'] | PCC: 0.4145 ['0.4145'] | RMSE: 0.4378 ['0.4378']
Epoch: 47 | Batch:   1 | Lr: 0.00125 | Time used(s): 143.6 | Training loss: 0.0314
Epoch: 47 | Batch:   2 | Lr: 0.00125 | Time used(s): 148.4 | Training loss: 0.0306
Epoch: 47 | Batch:   3 | Lr: 0.00125 | Time used(s): 129.7 | Training loss: 0.0305
Epoch: 47 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.3 | Training loss: 0.0206
--------------------------------------------------
Epoch: 47 | [Train] | Loss: 0.0306
Epoch: 47 |   [Val] | Loss: 0.0743 | [CCC]:  0.4083 [' 0.4083'] | PCC: 0.4145 ['0.4145'] | RMSE: 0.4484 ['0.4484']
Epoch: 48 | Batch:   1 | Lr: 0.00125 | Time used(s): 144.4 | Training loss: 0.0372
Epoch: 48 | Batch:   2 | Lr: 0.00125 | Time used(s): 124.8 | Training loss: 0.0265
Epoch: 48 | Batch:   3 | Lr: 0.00125 | Time used(s): 131.6 | Training loss: 0.0248
Epoch: 48 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0106
--------------------------------------------------
Epoch: 48 | [Train] | Loss: 0.0292
Epoch: 48 |   [Val] | Loss: 0.0721 | [CCC]:  0.4054 [' 0.4054'] | PCC: 0.4092 ['0.4092'] | RMSE: 0.4186 ['0.4186']
Epoch: 49 | Batch:   1 | Lr: 0.00125 | Time used(s): 140.7 | Training loss: 0.0265
Epoch: 49 | Batch:   2 | Lr: 0.00125 | Time used(s): 187.0 | Training loss: 0.0318
Epoch: 49 | Batch:   3 | Lr: 0.00125 | Time used(s): 133.1 | Training loss: 0.0251
Epoch: 49 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.3 | Training loss: 0.0460
--------------------------------------------------
Epoch: 49 | [Train] | Loss: 0.0281
Epoch: 49 |   [Val] | Loss: 0.0760 | [CCC]:  0.3985 [' 0.3985'] | PCC: 0.4160 ['0.4160'] | RMSE: 0.4276 ['0.4276']
Epoch: 50 | Batch:   1 | Lr: 0.00125 | Time used(s): 178.4 | Training loss: 0.0320
Epoch: 50 | Batch:   2 | Lr: 0.00125 | Time used(s): 139.2 | Training loss: 0.0271
Epoch: 50 | Batch:   3 | Lr: 0.00125 | Time used(s): 139.1 | Training loss: 0.0159
Epoch: 50 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.3 | Training loss: 0.0153
--------------------------------------------------
Epoch: 50 | [Train] | Loss: 0.0249
Epoch: 50 |   [Val] | Loss: 0.0734 | [CCC]:  0.4069 [' 0.4069'] | PCC: 0.4130 ['0.4130'] | RMSE: 0.4290 ['0.4290']
Epoch    50: reducing learning rate of group 0 to 6.2500e-04.
Epoch: 51 | Batch:   1 | Lr: 0.00063 | Time used(s): 151.5 | Training loss: 0.0190
Epoch: 51 | Batch:   2 | Lr: 0.00063 | Time used(s): 146.6 | Training loss: 0.0279
Epoch: 51 | Batch:   3 | Lr: 0.00063 | Time used(s): 135.2 | Training loss: 0.0201
Epoch: 51 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.3 | Training loss: 0.0204
--------------------------------------------------
Epoch: 51 | [Train] | Loss: 0.0223
Epoch: 51 |   [Val] | Loss: 0.0741 | [CCC]:  0.4005 [' 0.4005'] | PCC: 0.4107 ['0.4107'] | RMSE: 0.4305 ['0.4305']
Epoch: 52 | Batch:   1 | Lr: 0.00063 | Time used(s): 174.6 | Training loss: 0.0192
Epoch: 52 | Batch:   2 | Lr: 0.00063 | Time used(s): 141.7 | Training loss: 0.0242
Epoch: 52 | Batch:   3 | Lr: 0.00063 | Time used(s): 123.9 | Training loss: 0.0183
Epoch: 52 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.2 | Training loss: 0.0600
--------------------------------------------------
Epoch: 52 | [Train] | Loss: 0.0212
Epoch: 52 |   [Val] | Loss: 0.0735 | [CCC]:  0.4019 [' 0.4019'] | PCC: 0.4071 ['0.4071'] | RMSE: 0.4260 ['0.4260']
Epoch: 53 | Batch:   1 | Lr: 0.00063 | Time used(s): 139.7 | Training loss: 0.0200
Epoch: 53 | Batch:   2 | Lr: 0.00063 | Time used(s): 118.8 | Training loss: 0.0225
Epoch: 53 | Batch:   3 | Lr: 0.00063 | Time used(s): 120.0 | Training loss: 0.0184
Epoch: 53 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.0 | Training loss: 0.0265
--------------------------------------------------
Epoch: 53 | [Train] | Loss: 0.0204
Epoch: 53 |   [Val] | Loss: 0.0747 | [CCC]:  0.3969 [' 0.3969'] | PCC: 0.4075 ['0.4075'] | RMSE: 0.4325 ['0.4325']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 320 | Best [Val CCC]: 0.4377 [' 0.4377']| Loss: 0.0648 | PCC: 0.4391 ['0.4391'] | RMSE: 0.4384 ['0.4384']
On Test: CCC  0.5658 | PCC  0.5817 | RMSE  0.3810
****************************************************************************************************
Seed "320" over!
****************************************************************************************************
****************************************************************************************************
Using seed "321"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 321]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 150.4 | Training loss: 0.2500
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 132.7 | Training loss: 0.2464
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 129.2 | Training loss: 0.2324
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2207
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2426
Epoch:  1 |   [Val] | Loss: 0.2346 | [CCC]:  0.0591 [' 0.0591'] | PCC: 0.2093 ['0.2093'] | RMSE: 0.6537 ['0.6537']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_321_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 138.6 | Training loss: 0.2449
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 119.4 | Training loss: 0.2472
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 120.4 | Training loss: 0.2472
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2461
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2464
Epoch:  2 |   [Val] | Loss: 0.2289 | [CCC]:  0.0561 [' 0.0561'] | PCC: 0.2022 ['0.2022'] | RMSE: 1.0994 ['1.0994']
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 133.2 | Training loss: 0.2414
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 126.8 | Training loss: 0.2341
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.4 | Training loss: 0.2087
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.2240
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.2280
Epoch:  3 |   [Val] | Loss: 0.1622 | [CCC]:  0.1974 [' 0.1974'] | PCC: 0.2087 ['0.2087'] | RMSE: 0.5420 ['0.5420']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_321_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.7 | Training loss: 0.2019
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 104.7 | Training loss: 0.2001
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 100.6 | Training loss: 0.1766
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1816
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1927
Epoch:  4 |   [Val] | Loss: 0.1717 | [CCC]:  0.1599 [' 0.1599'] | PCC: 0.2532 ['0.2532'] | RMSE: 0.5866 ['0.5866']
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 112.1 | Training loss: 0.1929
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 104.3 | Training loss: 0.1980
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 107.2 | Training loss: 0.2097
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1774
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1998
Epoch:  5 |   [Val] | Loss: 0.1379 | [CCC]:  0.2320 [' 0.2320'] | PCC: 0.2967 ['0.2967'] | RMSE: 0.5791 ['0.5791']
Epoch:  5 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_321_None_None].pth"!
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 98.7 | Training loss: 0.1741
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 99.9 | Training loss: 0.1648
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 98.3 | Training loss: 0.1618
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1347
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1664
Epoch:  6 |   [Val] | Loss: 0.1079 | [CCC]:  0.3056 [' 0.3056'] | PCC: 0.3075 ['0.3075'] | RMSE: 0.5027 ['0.5027']
Epoch:  6 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_321_None_None].pth"!
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 108.1 | Training loss: 0.1541
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 91.6 | Training loss: 0.1579
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.3 | Training loss: 0.1483
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1436
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1533
Epoch:  7 |   [Val] | Loss: 0.1202 | [CCC]:  0.2851 [' 0.2851'] | PCC: 0.3140 ['0.3140'] | RMSE: 0.4523 ['0.4523']
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 107.0 | Training loss: 0.1567
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.8 | Training loss: 0.1376
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.2 | Training loss: 0.1497
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1191
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1475
Epoch:  8 |   [Val] | Loss: 0.0957 | [CCC]:  0.3301 [' 0.3301'] | PCC: 0.3349 ['0.3349'] | RMSE: 0.4887 ['0.4887']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_321_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 115.7 | Training loss: 0.1360
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 100.1 | Training loss: 0.1288
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 96.3 | Training loss: 0.1389
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1399
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1346
Epoch:  9 |   [Val] | Loss: 0.0977 | [CCC]:  0.3460 [' 0.3460'] | PCC: 0.3471 ['0.3471'] | RMSE: 0.4813 ['0.4813']
Epoch:  9 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_321_None_None].pth"!
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 98.9 | Training loss: 0.1347
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 97.3 | Training loss: 0.1284
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 120.4 | Training loss: 0.1378
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1652
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1341
Epoch: 10 |   [Val] | Loss: 0.1235 | [CCC]:  0.2471 [' 0.2471'] | PCC: 0.3283 ['0.3283'] | RMSE: 0.4565 ['0.4565']
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 126.7 | Training loss: 0.1511
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 111.8 | Training loss: 0.1403
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 101.0 | Training loss: 0.1406
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0982
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1433
Epoch: 11 |   [Val] | Loss: 0.1039 | [CCC]:  0.3206 [' 0.3206'] | PCC: 0.3664 ['0.3664'] | RMSE: 0.5377 ['0.5377']
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 133.5 | Training loss: 0.1369
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 124.6 | Training loss: 0.1254
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 112.5 | Training loss: 0.1295
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1445
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1308
Epoch: 12 |   [Val] | Loss: 0.0820 | [CCC]:  0.3805 [' 0.3805'] | PCC: 0.3948 ['0.3948'] | RMSE: 0.4373 ['0.4373']
Epoch: 12 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_321_None_None].pth"!
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 130.6 | Training loss: 0.1243
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 105.9 | Training loss: 0.1229
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.9 | Training loss: 0.1114
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1260
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1197
Epoch: 13 |   [Val] | Loss: 0.0938 | [CCC]:  0.3284 [' 0.3284'] | PCC: 0.3729 ['0.3729'] | RMSE: 0.5058 ['0.5058']
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 121.7 | Training loss: 0.1284
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 129.9 | Training loss: 0.1195
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 111.7 | Training loss: 0.1127
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1369
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1205
Epoch: 14 |   [Val] | Loss: 0.0808 | [CCC]:  0.3787 [' 0.3787'] | PCC: 0.3880 ['0.3880'] | RMSE: 0.4295 ['0.4295']
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 101.2 | Training loss: 0.1140
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 101.6 | Training loss: 0.1127
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 98.2 | Training loss: 0.1179
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0951
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1146
Epoch: 15 |   [Val] | Loss: 0.1033 | [CCC]:  0.3377 [' 0.3377'] | PCC: 0.3955 ['0.3955'] | RMSE: 0.4956 ['0.4956']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 124.4 | Training loss: 0.1285
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 105.5 | Training loss: 0.1142
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 96.7 | Training loss: 0.1085
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1292
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.1173
Epoch: 16 |   [Val] | Loss: 0.0777 | [CCC]:  0.3986 [' 0.3986'] | PCC: 0.4168 ['0.4168'] | RMSE: 0.4347 ['0.4347']
Epoch: 16 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_321_None_None].pth"!
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 127.3 | Training loss: 0.1133
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 100.8 | Training loss: 0.1024
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 121.2 | Training loss: 0.1123
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1069
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.1093
Epoch: 17 |   [Val] | Loss: 0.0858 | [CCC]:  0.3580 [' 0.3580'] | PCC: 0.4156 ['0.4156'] | RMSE: 0.4536 ['0.4536']
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 127.3 | Training loss: 0.1145
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 109.1 | Training loss: 0.1230
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 101.4 | Training loss: 0.0963
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0954
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.1110
Epoch: 18 |   [Val] | Loss: 0.0999 | [CCC]:  0.3582 [' 0.3582'] | PCC: 0.4034 ['0.4034'] | RMSE: 0.4971 ['0.4971']
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 145.1 | Training loss: 0.1061
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 121.2 | Training loss: 0.0964
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 102.3 | Training loss: 0.1110
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0792
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.1041
Epoch: 19 |   [Val] | Loss: 0.0811 | [CCC]:  0.3861 [' 0.3861'] | PCC: 0.4165 ['0.4165'] | RMSE: 0.3858 ['0.3858']
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 130.9 | Training loss: 0.1020
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 122.2 | Training loss: 0.1003
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 107.2 | Training loss: 0.0894
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0940
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0972
Epoch: 20 |   [Val] | Loss: 0.0897 | [CCC]:  0.3578 [' 0.3578'] | PCC: 0.3966 ['0.3966'] | RMSE: 0.5384 ['0.5384']
Epoch: 21 | Batch:   1 | Lr: 0.00500 | Time used(s): 117.7 | Training loss: 0.0977
Epoch: 21 | Batch:   2 | Lr: 0.00500 | Time used(s): 122.7 | Training loss: 0.0946
Epoch: 21 | Batch:   3 | Lr: 0.00500 | Time used(s): 101.2 | Training loss: 0.0928
Epoch: 21 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1203
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0954
Epoch: 21 |   [Val] | Loss: 0.0953 | [CCC]:  0.3283 [' 0.3283'] | PCC: 0.4045 ['0.4045'] | RMSE: 0.4410 ['0.4410']
Epoch: 22 | Batch:   1 | Lr: 0.00500 | Time used(s): 100.4 | Training loss: 0.1136
Epoch: 22 | Batch:   2 | Lr: 0.00500 | Time used(s): 101.7 | Training loss: 0.1087
Epoch: 22 | Batch:   3 | Lr: 0.00500 | Time used(s): 99.2 | Training loss: 0.0826
Epoch: 22 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0746
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.1012
Epoch: 22 |   [Val] | Loss: 0.0785 | [CCC]:  0.4133 [' 0.4133'] | PCC: 0.4291 ['0.4291'] | RMSE: 0.4959 ['0.4959']
Epoch: 22 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_321_None_None].pth"!
Epoch: 23 | Batch:   1 | Lr: 0.00500 | Time used(s): 113.7 | Training loss: 0.0896
Epoch: 23 | Batch:   2 | Lr: 0.00500 | Time used(s): 100.6 | Training loss: 0.1039
Epoch: 23 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.9 | Training loss: 0.0910
Epoch: 23 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0667
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0943
Epoch: 23 |   [Val] | Loss: 0.0768 | [CCC]:  0.4105 [' 0.4105'] | PCC: 0.4330 ['0.4330'] | RMSE: 0.3943 ['0.3943']
Epoch: 24 | Batch:   1 | Lr: 0.00500 | Time used(s): 112.0 | Training loss: 0.0858
Epoch: 24 | Batch:   2 | Lr: 0.00500 | Time used(s): 100.8 | Training loss: 0.0955
Epoch: 24 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.7 | Training loss: 0.0920
Epoch: 24 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0638
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0907
Epoch: 24 |   [Val] | Loss: 0.0761 | [CCC]:  0.4402 [' 0.4402'] | PCC: 0.4494 ['0.4494'] | RMSE: 0.4597 ['0.4597']
Epoch: 24 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_321_None_None].pth"!
Epoch: 25 | Batch:   1 | Lr: 0.00500 | Time used(s): 111.3 | Training loss: 0.0847
Epoch: 25 | Batch:   2 | Lr: 0.00500 | Time used(s): 99.5 | Training loss: 0.0817
Epoch: 25 | Batch:   3 | Lr: 0.00500 | Time used(s): 97.7 | Training loss: 0.0815
Epoch: 25 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0930
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0828
Epoch: 25 |   [Val] | Loss: 0.0686 | [CCC]:  0.4279 [' 0.4279'] | PCC: 0.4473 ['0.4473'] | RMSE: 0.3928 ['0.3928']
Epoch: 26 | Batch:   1 | Lr: 0.00500 | Time used(s): 109.3 | Training loss: 0.0844
Epoch: 26 | Batch:   2 | Lr: 0.00500 | Time used(s): 108.8 | Training loss: 0.0865
Epoch: 26 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.8 | Training loss: 0.0792
Epoch: 26 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.0464
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0828
Epoch: 26 |   [Val] | Loss: 0.0772 | [CCC]:  0.4035 [' 0.4035'] | PCC: 0.4295 ['0.4295'] | RMSE: 0.4589 ['0.4589']
Epoch: 27 | Batch:   1 | Lr: 0.00500 | Time used(s): 101.0 | Training loss: 0.0825
Epoch: 27 | Batch:   2 | Lr: 0.00500 | Time used(s): 112.3 | Training loss: 0.0662
Epoch: 27 | Batch:   3 | Lr: 0.00500 | Time used(s): 115.7 | Training loss: 0.0759
Epoch: 27 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0591
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0746
Epoch: 27 |   [Val] | Loss: 0.0767 | [CCC]:  0.4529 [' 0.4529'] | PCC: 0.4535 ['0.4535'] | RMSE: 0.4338 ['0.4338']
Epoch: 27 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_321_None_None].pth"!
Epoch: 28 | Batch:   1 | Lr: 0.00500 | Time used(s): 122.6 | Training loss: 0.0748
Epoch: 28 | Batch:   2 | Lr: 0.00500 | Time used(s): 116.8 | Training loss: 0.0748
Epoch: 28 | Batch:   3 | Lr: 0.00500 | Time used(s): 112.2 | Training loss: 0.0714
Epoch: 28 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0435
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0732
Epoch: 28 |   [Val] | Loss: 0.0723 | [CCC]:  0.4231 [' 0.4231'] | PCC: 0.4370 ['0.4370'] | RMSE: 0.4152 ['0.4152']
Epoch: 29 | Batch:   1 | Lr: 0.00500 | Time used(s): 96.1 | Training loss: 0.0703
Epoch: 29 | Batch:   2 | Lr: 0.00500 | Time used(s): 106.3 | Training loss: 0.0714
Epoch: 29 | Batch:   3 | Lr: 0.00500 | Time used(s): 95.2 | Training loss: 0.0625
Epoch: 29 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0484
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0677
Epoch: 29 |   [Val] | Loss: 0.0773 | [CCC]:  0.4345 [' 0.4345'] | PCC: 0.4399 ['0.4399'] | RMSE: 0.4383 ['0.4383']
Epoch: 30 | Batch:   1 | Lr: 0.00500 | Time used(s): 106.7 | Training loss: 0.0656
Epoch: 30 | Batch:   2 | Lr: 0.00500 | Time used(s): 110.3 | Training loss: 0.0662
Epoch: 30 | Batch:   3 | Lr: 0.00500 | Time used(s): 97.0 | Training loss: 0.0645
Epoch: 30 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0697
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0655
Epoch: 30 |   [Val] | Loss: 0.0745 | [CCC]:  0.4199 [' 0.4199'] | PCC: 0.4576 ['0.4576'] | RMSE: 0.4391 ['0.4391']
Epoch: 31 | Batch:   1 | Lr: 0.00500 | Time used(s): 102.4 | Training loss: 0.0675
Epoch: 31 | Batch:   2 | Lr: 0.00500 | Time used(s): 100.5 | Training loss: 0.0639
Epoch: 31 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.5 | Training loss: 0.0665
Epoch: 31 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.0740
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0661
Epoch: 31 |   [Val] | Loss: 0.0755 | [CCC]:  0.4253 [' 0.4253'] | PCC: 0.4541 ['0.4541'] | RMSE: 0.4154 ['0.4154']
Epoch: 32 | Batch:   1 | Lr: 0.00500 | Time used(s): 110.0 | Training loss: 0.0612
Epoch: 32 | Batch:   2 | Lr: 0.00500 | Time used(s): 95.8 | Training loss: 0.0619
Epoch: 32 | Batch:   3 | Lr: 0.00500 | Time used(s): 111.5 | Training loss: 0.0625
Epoch: 32 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0729
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0621
Epoch: 32 |   [Val] | Loss: 0.0835 | [CCC]:  0.3480 [' 0.3480'] | PCC: 0.3509 ['0.3509'] | RMSE: 0.4790 ['0.4790']
Epoch: 33 | Batch:   1 | Lr: 0.00500 | Time used(s): 122.4 | Training loss: 0.0601
Epoch: 33 | Batch:   2 | Lr: 0.00500 | Time used(s): 110.4 | Training loss: 0.0583
Epoch: 33 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.1 | Training loss: 0.0553
Epoch: 33 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0987
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0586
Epoch: 33 |   [Val] | Loss: 0.0715 | [CCC]:  0.4373 [' 0.4373'] | PCC: 0.4514 ['0.4514'] | RMSE: 0.4145 ['0.4145']
Epoch    33: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 34 | Batch:   1 | Lr: 0.00250 | Time used(s): 118.0 | Training loss: 0.0522
Epoch: 34 | Batch:   2 | Lr: 0.00250 | Time used(s): 104.0 | Training loss: 0.0480
Epoch: 34 | Batch:   3 | Lr: 0.00250 | Time used(s): 95.1 | Training loss: 0.0478
Epoch: 34 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0950
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0501
Epoch: 34 |   [Val] | Loss: 0.0680 | [CCC]:  0.4425 [' 0.4425'] | PCC: 0.4528 ['0.4528'] | RMSE: 0.4406 ['0.4406']
Epoch: 35 | Batch:   1 | Lr: 0.00250 | Time used(s): 122.7 | Training loss: 0.0569
Epoch: 35 | Batch:   2 | Lr: 0.00250 | Time used(s): 164.1 | Training loss: 0.0486
Epoch: 35 | Batch:   3 | Lr: 0.00250 | Time used(s): 118.6 | Training loss: 0.0450
Epoch: 35 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.0153
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0496
Epoch: 35 |   [Val] | Loss: 0.0671 | [CCC]:  0.4451 [' 0.4451'] | PCC: 0.4468 ['0.4468'] | RMSE: 0.4141 ['0.4141']
Epoch: 36 | Batch:   1 | Lr: 0.00250 | Time used(s): 168.2 | Training loss: 0.0498
Epoch: 36 | Batch:   2 | Lr: 0.00250 | Time used(s): 138.3 | Training loss: 0.0491
Epoch: 36 | Batch:   3 | Lr: 0.00250 | Time used(s): 137.4 | Training loss: 0.0418
Epoch: 36 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.0402
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0468
Epoch: 36 |   [Val] | Loss: 0.0756 | [CCC]:  0.4164 [' 0.4164'] | PCC: 0.4292 ['0.4292'] | RMSE: 0.4118 ['0.4118']
Epoch: 37 | Batch:   1 | Lr: 0.00250 | Time used(s): 148.7 | Training loss: 0.0444
Epoch: 37 | Batch:   2 | Lr: 0.00250 | Time used(s): 167.9 | Training loss: 0.0446
Epoch: 37 | Batch:   3 | Lr: 0.00250 | Time used(s): 131.6 | Training loss: 0.0421
Epoch: 37 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.0254
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0434
Epoch: 37 |   [Val] | Loss: 0.0692 | [CCC]:  0.4241 [' 0.4241'] | PCC: 0.4266 ['0.4266'] | RMSE: 0.4402 ['0.4402']
Epoch: 38 | Batch:   1 | Lr: 0.00250 | Time used(s): 169.7 | Training loss: 0.0422
Epoch: 38 | Batch:   2 | Lr: 0.00250 | Time used(s): 136.5 | Training loss: 0.0386
Epoch: 38 | Batch:   3 | Lr: 0.00250 | Time used(s): 124.8 | Training loss: 0.0345
Epoch: 38 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.4 | Training loss: 0.0569
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0387
Epoch: 38 |   [Val] | Loss: 0.0722 | [CCC]:  0.4131 [' 0.4131'] | PCC: 0.4290 ['0.4290'] | RMSE: 0.4114 ['0.4114']
Epoch: 39 | Batch:   1 | Lr: 0.00250 | Time used(s): 180.5 | Training loss: 0.0362
Epoch: 39 | Batch:   2 | Lr: 0.00250 | Time used(s): 149.1 | Training loss: 0.0376
Epoch: 39 | Batch:   3 | Lr: 0.00250 | Time used(s): 166.3 | Training loss: 0.0517
Epoch: 39 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.4 | Training loss: 0.0337
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0417
Epoch: 39 |   [Val] | Loss: 0.0750 | [CCC]:  0.4290 [' 0.4290'] | PCC: 0.4300 ['0.4300'] | RMSE: 0.4517 ['0.4517']
Epoch    39: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 40 | Batch:   1 | Lr: 0.00125 | Time used(s): 157.6 | Training loss: 0.0325
Epoch: 40 | Batch:   2 | Lr: 0.00125 | Time used(s): 97.6 | Training loss: 0.0434
Epoch: 40 | Batch:   3 | Lr: 0.00125 | Time used(s): 145.5 | Training loss: 0.0339
Epoch: 40 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.4 | Training loss: 0.0387
--------------------------------------------------
Epoch: 40 | [Train] | Loss: 0.0366
Epoch: 40 |   [Val] | Loss: 0.0730 | [CCC]:  0.4188 [' 0.4188'] | PCC: 0.4220 ['0.4220'] | RMSE: 0.4166 ['0.4166']
Epoch: 41 | Batch:   1 | Lr: 0.00125 | Time used(s): 196.2 | Training loss: 0.0332
Epoch: 41 | Batch:   2 | Lr: 0.00125 | Time used(s): 155.7 | Training loss: 0.0344
Epoch: 41 | Batch:   3 | Lr: 0.00125 | Time used(s): 143.5 | Training loss: 0.0359
Epoch: 41 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.4 | Training loss: 0.0091
--------------------------------------------------
Epoch: 41 | [Train] | Loss: 0.0341
Epoch: 41 |   [Val] | Loss: 0.0727 | [CCC]:  0.4191 [' 0.4191'] | PCC: 0.4241 ['0.4241'] | RMSE: 0.4106 ['0.4106']
Epoch: 42 | Batch:   1 | Lr: 0.00125 | Time used(s): 193.2 | Training loss: 0.0274
Epoch: 42 | Batch:   2 | Lr: 0.00125 | Time used(s): 141.9 | Training loss: 0.0359
Epoch: 42 | Batch:   3 | Lr: 0.00125 | Time used(s): 110.2 | Training loss: 0.0306
Epoch: 42 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0033
--------------------------------------------------
Epoch: 42 | [Train] | Loss: 0.0309
Epoch: 42 |   [Val] | Loss: 0.0717 | [CCC]:  0.4282 [' 0.4282'] | PCC: 0.4299 ['0.4299'] | RMSE: 0.4250 ['0.4250']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 321 | Best [Val CCC]: 0.4529 [' 0.4529']| Loss: 0.0767 | PCC: 0.4535 ['0.4535'] | RMSE: 0.4338 ['0.4338']
On Test: CCC  0.5521 | PCC  0.5582 | RMSE  0.3928
****************************************************************************************************
Seed "321" over!
****************************************************************************************************
****************************************************************************************************
Using seed "322"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 322]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 109.8 | Training loss: 0.2501
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 112.4 | Training loss: 0.2432
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 120.4 | Training loss: 0.2236
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2409
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2390
Epoch:  1 |   [Val] | Loss: 0.1814 | [CCC]:  0.1712 [' 0.1712'] | PCC: 0.2025 ['0.2025'] | RMSE: 0.4265 ['0.4265']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 106.8 | Training loss: 0.2199
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 106.2 | Training loss: 0.2444
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 110.9 | Training loss: 0.2490
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.2488
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2379
Epoch:  2 |   [Val] | Loss: 0.2494 | [CCC]:  0.0012 [' 0.0012'] | PCC: 0.0491 ['0.0491'] | RMSE: 1.1100 ['1.1100']
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 111.6 | Training loss: 0.2484
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 123.6 | Training loss: 0.2478
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 116.0 | Training loss: 0.2461
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2427
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.2474
Epoch:  3 |   [Val] | Loss: 0.2220 | [CCC]:  0.0458 [' 0.0458'] | PCC: 0.1796 ['0.1796'] | RMSE: 0.6221 ['0.6221']
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 117.8 | Training loss: 0.2402
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 111.4 | Training loss: 0.2209
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 115.9 | Training loss: 0.2102
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.2382
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.2240
Epoch:  4 |   [Val] | Loss: 0.2318 | [CCC]:  0.0615 [' 0.0615'] | PCC: 0.2255 ['0.2255'] | RMSE: 0.5145 ['0.5145']
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 113.7 | Training loss: 0.2412
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 115.5 | Training loss: 0.2350
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 109.7 | Training loss: 0.2106
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2261
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.2289
Epoch:  5 |   [Val] | Loss: 0.2150 | [CCC]:  0.0811 [' 0.0811'] | PCC: 0.2230 ['0.2230'] | RMSE: 0.7584 ['0.7584']
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 123.5 | Training loss: 0.2279
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 120.1 | Training loss: 0.1981
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 114.5 | Training loss: 0.2190
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.2052
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.2148
Epoch:  6 |   [Val] | Loss: 0.1797 | [CCC]:  0.1444 [' 0.1444'] | PCC: 0.2132 ['0.2132'] | RMSE: 0.7210 ['0.7210']
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 107.2 | Training loss: 0.2036
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 124.1 | Training loss: 0.1729
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 112.4 | Training loss: 0.1922
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.2052
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1898
Epoch:  7 |   [Val] | Loss: 0.1905 | [CCC]:  0.1291 [' 0.1291'] | PCC: 0.2611 ['0.2611'] | RMSE: 0.5741 ['0.5741']
Epoch     7: reducing learning rate of group 0 to 2.5000e-03.
Epoch:  8 | Batch:   1 | Lr: 0.00250 | Time used(s): 123.3 | Training loss: 0.2081
Epoch:  8 | Batch:   2 | Lr: 0.00250 | Time used(s): 121.0 | Training loss: 0.1973
Epoch:  8 | Batch:   3 | Lr: 0.00250 | Time used(s): 111.0 | Training loss: 0.1656
Epoch:  8 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.1974
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1904
Epoch:  8 |   [Val] | Loss: 0.1538 | [CCC]:  0.2077 [' 0.2077'] | PCC: 0.2799 ['0.2799'] | RMSE: 0.5921 ['0.5921']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00250 | Time used(s): 121.3 | Training loss: 0.1842
Epoch:  9 | Batch:   2 | Lr: 0.00250 | Time used(s): 114.1 | Training loss: 0.1689
Epoch:  9 | Batch:   3 | Lr: 0.00250 | Time used(s): 110.8 | Training loss: 0.1575
Epoch:  9 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.1608
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1701
Epoch:  9 |   [Val] | Loss: 0.1369 | [CCC]:  0.2214 [' 0.2214'] | PCC: 0.2641 ['0.2641'] | RMSE: 0.5456 ['0.5456']
Epoch:  9 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 10 | Batch:   1 | Lr: 0.00250 | Time used(s): 125.2 | Training loss: 0.1605
Epoch: 10 | Batch:   2 | Lr: 0.00250 | Time used(s): 118.3 | Training loss: 0.1628
Epoch: 10 | Batch:   3 | Lr: 0.00250 | Time used(s): 112.2 | Training loss: 0.1622
Epoch: 10 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.1497
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1617
Epoch: 10 |   [Val] | Loss: 0.1211 | [CCC]:  0.2673 [' 0.2673'] | PCC: 0.2689 ['0.2689'] | RMSE: 0.5163 ['0.5163']
Epoch: 10 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 11 | Batch:   1 | Lr: 0.00250 | Time used(s): 119.7 | Training loss: 0.1534
Epoch: 11 | Batch:   2 | Lr: 0.00250 | Time used(s): 113.9 | Training loss: 0.1479
Epoch: 11 | Batch:   3 | Lr: 0.00250 | Time used(s): 118.1 | Training loss: 0.1501
Epoch: 11 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.1451
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1504
Epoch: 11 |   [Val] | Loss: 0.1257 | [CCC]:  0.2635 [' 0.2635'] | PCC: 0.2758 ['0.2758'] | RMSE: 0.5163 ['0.5163']
Epoch: 12 | Batch:   1 | Lr: 0.00250 | Time used(s): 117.9 | Training loss: 0.1601
Epoch: 12 | Batch:   2 | Lr: 0.00250 | Time used(s): 108.7 | Training loss: 0.1463
Epoch: 12 | Batch:   3 | Lr: 0.00250 | Time used(s): 112.8 | Training loss: 0.1521
Epoch: 12 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.1559
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1529
Epoch: 12 |   [Val] | Loss: 0.1100 | [CCC]:  0.2846 [' 0.2846'] | PCC: 0.2970 ['0.2970'] | RMSE: 0.4402 ['0.4402']
Epoch: 12 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 13 | Batch:   1 | Lr: 0.00250 | Time used(s): 119.8 | Training loss: 0.1421
Epoch: 13 | Batch:   2 | Lr: 0.00250 | Time used(s): 98.3 | Training loss: 0.1522
Epoch: 13 | Batch:   3 | Lr: 0.00250 | Time used(s): 146.3 | Training loss: 0.1522
Epoch: 13 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.5 | Training loss: 0.1338
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1486
Epoch: 13 |   [Val] | Loss: 0.1291 | [CCC]:  0.2462 [' 0.2462'] | PCC: 0.2869 ['0.2869'] | RMSE: 0.5477 ['0.5477']
Epoch: 14 | Batch:   1 | Lr: 0.00250 | Time used(s): 160.9 | Training loss: 0.1584
Epoch: 14 | Batch:   2 | Lr: 0.00250 | Time used(s): 110.1 | Training loss: 0.1640
Epoch: 14 | Batch:   3 | Lr: 0.00250 | Time used(s): 123.9 | Training loss: 0.1531
Epoch: 14 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.1257
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1580
Epoch: 14 |   [Val] | Loss: 0.1116 | [CCC]:  0.2902 [' 0.2902'] | PCC: 0.2906 ['0.2906'] | RMSE: 0.4983 ['0.4983']
Epoch: 14 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 15 | Batch:   1 | Lr: 0.00250 | Time used(s): 147.2 | Training loss: 0.1421
Epoch: 15 | Batch:   2 | Lr: 0.00250 | Time used(s): 150.4 | Training loss: 0.1449
Epoch: 15 | Batch:   3 | Lr: 0.00250 | Time used(s): 133.8 | Training loss: 0.1423
Epoch: 15 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.4 | Training loss: 0.1280
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1428
Epoch: 15 |   [Val] | Loss: 0.1272 | [CCC]:  0.2521 [' 0.2521'] | PCC: 0.3033 ['0.3033'] | RMSE: 0.5758 ['0.5758']
Epoch: 16 | Batch:   1 | Lr: 0.00250 | Time used(s): 139.9 | Training loss: 0.1465
Epoch: 16 | Batch:   2 | Lr: 0.00250 | Time used(s): 151.2 | Training loss: 0.1404
Epoch: 16 | Batch:   3 | Lr: 0.00250 | Time used(s): 147.5 | Training loss: 0.1384
Epoch: 16 | Batch:   4 | Lr: 0.00250 | Time used(s): 3.2 | Training loss: 0.1272
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.1415
Epoch: 16 |   [Val] | Loss: 0.1111 | [CCC]:  0.2975 [' 0.2975'] | PCC: 0.3206 ['0.3206'] | RMSE: 0.5016 ['0.5016']
Epoch: 16 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 17 | Batch:   1 | Lr: 0.00250 | Time used(s): 151.3 | Training loss: 0.1362
Epoch: 17 | Batch:   2 | Lr: 0.00250 | Time used(s): 146.6 | Training loss: 0.1330
Epoch: 17 | Batch:   3 | Lr: 0.00250 | Time used(s): 152.1 | Training loss: 0.1331
Epoch: 17 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.1394
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.1342
Epoch: 17 |   [Val] | Loss: 0.1157 | [CCC]:  0.2735 [' 0.2735'] | PCC: 0.3168 ['0.3168'] | RMSE: 0.5280 ['0.5280']
Epoch: 18 | Batch:   1 | Lr: 0.00250 | Time used(s): 145.8 | Training loss: 0.1443
Epoch: 18 | Batch:   2 | Lr: 0.00250 | Time used(s): 151.4 | Training loss: 0.1303
Epoch: 18 | Batch:   3 | Lr: 0.00250 | Time used(s): 120.3 | Training loss: 0.1273
Epoch: 18 | Batch:   4 | Lr: 0.00250 | Time used(s): 4.2 | Training loss: 0.1274
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.1339
Epoch: 18 |   [Val] | Loss: 0.0974 | [CCC]:  0.3457 [' 0.3457'] | PCC: 0.3464 ['0.3464'] | RMSE: 0.4616 ['0.4616']
Epoch: 18 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 19 | Batch:   1 | Lr: 0.00250 | Time used(s): 167.0 | Training loss: 0.1277
Epoch: 19 | Batch:   2 | Lr: 0.00250 | Time used(s): 151.3 | Training loss: 0.1286
Epoch: 19 | Batch:   3 | Lr: 0.00250 | Time used(s): 159.5 | Training loss: 0.1276
Epoch: 19 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.9 | Training loss: 0.1352
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.1281
Epoch: 19 |   [Val] | Loss: 0.0951 | [CCC]:  0.3494 [' 0.3494'] | PCC: 0.3568 ['0.3568'] | RMSE: 0.4447 ['0.4447']
Epoch: 19 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 20 | Batch:   1 | Lr: 0.00250 | Time used(s): 170.5 | Training loss: 0.1290
Epoch: 20 | Batch:   2 | Lr: 0.00250 | Time used(s): 156.0 | Training loss: 0.1281
Epoch: 20 | Batch:   3 | Lr: 0.00250 | Time used(s): 162.4 | Training loss: 0.1174
Epoch: 20 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.4 | Training loss: 0.1359
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.1250
Epoch: 20 |   [Val] | Loss: 0.1015 | [CCC]:  0.3421 [' 0.3421'] | PCC: 0.3612 ['0.3612'] | RMSE: 0.4536 ['0.4536']
Epoch: 21 | Batch:   1 | Lr: 0.00250 | Time used(s): 151.6 | Training loss: 0.1273
Epoch: 21 | Batch:   2 | Lr: 0.00250 | Time used(s): 151.9 | Training loss: 0.1272
Epoch: 21 | Batch:   3 | Lr: 0.00250 | Time used(s): 158.5 | Training loss: 0.1202
Epoch: 21 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.4 | Training loss: 0.1191
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.1248
Epoch: 21 |   [Val] | Loss: 0.1021 | [CCC]:  0.3139 [' 0.3139'] | PCC: 0.3596 ['0.3596'] | RMSE: 0.4964 ['0.4964']
Epoch: 22 | Batch:   1 | Lr: 0.00250 | Time used(s): 163.2 | Training loss: 0.1248
Epoch: 22 | Batch:   2 | Lr: 0.00250 | Time used(s): 157.8 | Training loss: 0.1166
Epoch: 22 | Batch:   3 | Lr: 0.00250 | Time used(s): 124.7 | Training loss: 0.1234
Epoch: 22 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.4 | Training loss: 0.1351
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.1218
Epoch: 22 |   [Val] | Loss: 0.0923 | [CCC]:  0.3638 [' 0.3638'] | PCC: 0.3663 ['0.3663'] | RMSE: 0.4446 ['0.4446']
Epoch: 22 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 23 | Batch:   1 | Lr: 0.00250 | Time used(s): 150.5 | Training loss: 0.1101
Epoch: 23 | Batch:   2 | Lr: 0.00250 | Time used(s): 154.6 | Training loss: 0.1154
Epoch: 23 | Batch:   3 | Lr: 0.00250 | Time used(s): 134.4 | Training loss: 0.1211
Epoch: 23 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.1248
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.1157
Epoch: 23 |   [Val] | Loss: 0.0922 | [CCC]:  0.3698 [' 0.3698'] | PCC: 0.3744 ['0.3744'] | RMSE: 0.4446 ['0.4446']
Epoch: 23 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 24 | Batch:   1 | Lr: 0.00250 | Time used(s): 152.6 | Training loss: 0.1101
Epoch: 24 | Batch:   2 | Lr: 0.00250 | Time used(s): 138.7 | Training loss: 0.1168
Epoch: 24 | Batch:   3 | Lr: 0.00250 | Time used(s): 152.5 | Training loss: 0.1132
Epoch: 24 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.0700
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.1126
Epoch: 24 |   [Val] | Loss: 0.0772 | [CCC]:  0.3647 [' 0.3647'] | PCC: 0.3792 ['0.3792'] | RMSE: 0.4479 ['0.4479']
Epoch: 25 | Batch:   1 | Lr: 0.00250 | Time used(s): 153.4 | Training loss: 0.1113
Epoch: 25 | Batch:   2 | Lr: 0.00250 | Time used(s): 131.5 | Training loss: 0.1080
Epoch: 25 | Batch:   3 | Lr: 0.00250 | Time used(s): 157.5 | Training loss: 0.1181
Epoch: 25 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.4 | Training loss: 0.1123
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.1124
Epoch: 25 |   [Val] | Loss: 0.0792 | [CCC]:  0.3566 [' 0.3566'] | PCC: 0.3771 ['0.3771'] | RMSE: 0.4552 ['0.4552']
Epoch: 26 | Batch:   1 | Lr: 0.00250 | Time used(s): 139.0 | Training loss: 0.1013
Epoch: 26 | Batch:   2 | Lr: 0.00250 | Time used(s): 140.2 | Training loss: 0.1066
Epoch: 26 | Batch:   3 | Lr: 0.00250 | Time used(s): 118.6 | Training loss: 0.1120
Epoch: 26 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.1021
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.1066
Epoch: 26 |   [Val] | Loss: 0.0883 | [CCC]:  0.3799 [' 0.3799'] | PCC: 0.3849 ['0.3849'] | RMSE: 0.4329 ['0.4329']
Epoch: 26 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 27 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.0 | Training loss: 0.1082
Epoch: 27 | Batch:   2 | Lr: 0.00250 | Time used(s): 93.9 | Training loss: 0.1098
Epoch: 27 | Batch:   3 | Lr: 0.00250 | Time used(s): 103.6 | Training loss: 0.1025
Epoch: 27 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.1308
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.1072
Epoch: 27 |   [Val] | Loss: 0.0910 | [CCC]:  0.3534 [' 0.3534'] | PCC: 0.3861 ['0.3861'] | RMSE: 0.4919 ['0.4919']
Epoch: 28 | Batch:   1 | Lr: 0.00250 | Time used(s): 165.1 | Training loss: 0.1216
Epoch: 28 | Batch:   2 | Lr: 0.00250 | Time used(s): 140.5 | Training loss: 0.1139
Epoch: 28 | Batch:   3 | Lr: 0.00250 | Time used(s): 126.6 | Training loss: 0.0999
Epoch: 28 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.1114
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.1118
Epoch: 28 |   [Val] | Loss: 0.0875 | [CCC]:  0.3356 [' 0.3356'] | PCC: 0.3739 ['0.3739'] | RMSE: 0.4156 ['0.4156']
Epoch: 29 | Batch:   1 | Lr: 0.00250 | Time used(s): 139.9 | Training loss: 0.1132
Epoch: 29 | Batch:   2 | Lr: 0.00250 | Time used(s): 144.0 | Training loss: 0.1120
Epoch: 29 | Batch:   3 | Lr: 0.00250 | Time used(s): 136.4 | Training loss: 0.1141
Epoch: 29 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.0958
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.1128
Epoch: 29 |   [Val] | Loss: 0.0831 | [CCC]:  0.3696 [' 0.3696'] | PCC: 0.3930 ['0.3930'] | RMSE: 0.5138 ['0.5138']
Epoch: 30 | Batch:   1 | Lr: 0.00250 | Time used(s): 147.3 | Training loss: 0.1078
Epoch: 30 | Batch:   2 | Lr: 0.00250 | Time used(s): 152.4 | Training loss: 0.1194
Epoch: 30 | Batch:   3 | Lr: 0.00250 | Time used(s): 134.9 | Training loss: 0.0915
Epoch: 30 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.1648
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.1072
Epoch: 30 |   [Val] | Loss: 0.0826 | [CCC]:  0.3535 [' 0.3535'] | PCC: 0.3767 ['0.3767'] | RMSE: 0.4185 ['0.4185']
Epoch: 31 | Batch:   1 | Lr: 0.00250 | Time used(s): 152.5 | Training loss: 0.1087
Epoch: 31 | Batch:   2 | Lr: 0.00250 | Time used(s): 141.8 | Training loss: 0.1052
Epoch: 31 | Batch:   3 | Lr: 0.00250 | Time used(s): 139.0 | Training loss: 0.1023
Epoch: 31 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.1102
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.1055
Epoch: 31 |   [Val] | Loss: 0.0752 | [CCC]:  0.3988 [' 0.3988'] | PCC: 0.4128 ['0.4128'] | RMSE: 0.4874 ['0.4874']
Epoch: 31 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 32 | Batch:   1 | Lr: 0.00250 | Time used(s): 103.3 | Training loss: 0.0945
Epoch: 32 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.6 | Training loss: 0.0976
Epoch: 32 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.9 | Training loss: 0.1076
Epoch: 32 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0858
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0997
Epoch: 32 |   [Val] | Loss: 0.0745 | [CCC]:  0.4121 [' 0.4121'] | PCC: 0.4184 ['0.4184'] | RMSE: 0.4094 ['0.4094']
Epoch: 32 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 33 | Batch:   1 | Lr: 0.00250 | Time used(s): 118.4 | Training loss: 0.0959
Epoch: 33 | Batch:   2 | Lr: 0.00250 | Time used(s): 110.6 | Training loss: 0.1020
Epoch: 33 | Batch:   3 | Lr: 0.00250 | Time used(s): 106.9 | Training loss: 0.1013
Epoch: 33 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0738
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0993
Epoch: 33 |   [Val] | Loss: 0.0787 | [CCC]:  0.4074 [' 0.4074'] | PCC: 0.4152 ['0.4152'] | RMSE: 0.4253 ['0.4253']
Epoch: 34 | Batch:   1 | Lr: 0.00250 | Time used(s): 103.0 | Training loss: 0.0959
Epoch: 34 | Batch:   2 | Lr: 0.00250 | Time used(s): 144.5 | Training loss: 0.1017
Epoch: 34 | Batch:   3 | Lr: 0.00250 | Time used(s): 98.3 | Training loss: 0.0886
Epoch: 34 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0869
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0953
Epoch: 34 |   [Val] | Loss: 0.0709 | [CCC]:  0.4133 [' 0.4133'] | PCC: 0.4165 ['0.4165'] | RMSE: 0.4427 ['0.4427']
Epoch: 34 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 35 | Batch:   1 | Lr: 0.00250 | Time used(s): 111.0 | Training loss: 0.0805
Epoch: 35 | Batch:   2 | Lr: 0.00250 | Time used(s): 103.4 | Training loss: 0.0933
Epoch: 35 | Batch:   3 | Lr: 0.00250 | Time used(s): 95.3 | Training loss: 0.0978
Epoch: 35 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.1136
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0909
Epoch: 35 |   [Val] | Loss: 0.0785 | [CCC]:  0.3734 [' 0.3734'] | PCC: 0.4126 ['0.4126'] | RMSE: 0.4670 ['0.4670']
Epoch: 36 | Batch:   1 | Lr: 0.00250 | Time used(s): 113.1 | Training loss: 0.0919
Epoch: 36 | Batch:   2 | Lr: 0.00250 | Time used(s): 107.8 | Training loss: 0.0826
Epoch: 36 | Batch:   3 | Lr: 0.00250 | Time used(s): 96.1 | Training loss: 0.0905
Epoch: 36 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0847
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0883
Epoch: 36 |   [Val] | Loss: 0.0761 | [CCC]:  0.4117 [' 0.4117'] | PCC: 0.4170 ['0.4170'] | RMSE: 0.4718 ['0.4718']
Epoch: 37 | Batch:   1 | Lr: 0.00250 | Time used(s): 114.2 | Training loss: 0.0884
Epoch: 37 | Batch:   2 | Lr: 0.00250 | Time used(s): 102.4 | Training loss: 0.0900
Epoch: 37 | Batch:   3 | Lr: 0.00250 | Time used(s): 99.2 | Training loss: 0.0836
Epoch: 37 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0752
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0872
Epoch: 37 |   [Val] | Loss: 0.0736 | [CCC]:  0.4017 [' 0.4017'] | PCC: 0.4132 ['0.4132'] | RMSE: 0.4032 ['0.4032']
Epoch: 38 | Batch:   1 | Lr: 0.00250 | Time used(s): 115.2 | Training loss: 0.0846
Epoch: 38 | Batch:   2 | Lr: 0.00250 | Time used(s): 103.3 | Training loss: 0.0892
Epoch: 38 | Batch:   3 | Lr: 0.00250 | Time used(s): 96.4 | Training loss: 0.0824
Epoch: 38 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0851
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0854
Epoch: 38 |   [Val] | Loss: 0.0752 | [CCC]:  0.4214 [' 0.4214'] | PCC: 0.4236 ['0.4236'] | RMSE: 0.4435 ['0.4435']
Epoch: 38 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 39 | Batch:   1 | Lr: 0.00250 | Time used(s): 117.0 | Training loss: 0.0844
Epoch: 39 | Batch:   2 | Lr: 0.00250 | Time used(s): 110.2 | Training loss: 0.0839
Epoch: 39 | Batch:   3 | Lr: 0.00250 | Time used(s): 98.5 | Training loss: 0.0834
Epoch: 39 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0624
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0835
Epoch: 39 |   [Val] | Loss: 0.0711 | [CCC]:  0.4142 [' 0.4142'] | PCC: 0.4151 ['0.4151'] | RMSE: 0.4378 ['0.4378']
Epoch: 40 | Batch:   1 | Lr: 0.00250 | Time used(s): 108.4 | Training loss: 0.0757
Epoch: 40 | Batch:   2 | Lr: 0.00250 | Time used(s): 115.9 | Training loss: 0.0853
Epoch: 40 | Batch:   3 | Lr: 0.00250 | Time used(s): 93.3 | Training loss: 0.0769
Epoch: 40 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.1009
--------------------------------------------------
Epoch: 40 | [Train] | Loss: 0.0797
Epoch: 40 |   [Val] | Loss: 0.0759 | [CCC]:  0.4093 [' 0.4093'] | PCC: 0.4104 ['0.4104'] | RMSE: 0.4439 ['0.4439']
Epoch: 41 | Batch:   1 | Lr: 0.00250 | Time used(s): 104.9 | Training loss: 0.0763
Epoch: 41 | Batch:   2 | Lr: 0.00250 | Time used(s): 112.1 | Training loss: 0.0875
Epoch: 41 | Batch:   3 | Lr: 0.00250 | Time used(s): 113.6 | Training loss: 0.0824
Epoch: 41 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0658
--------------------------------------------------
Epoch: 41 | [Train] | Loss: 0.0818
Epoch: 41 |   [Val] | Loss: 0.0705 | [CCC]:  0.3902 [' 0.3902'] | PCC: 0.4054 ['0.4054'] | RMSE: 0.4350 ['0.4350']
Epoch: 42 | Batch:   1 | Lr: 0.00250 | Time used(s): 139.1 | Training loss: 0.0842
Epoch: 42 | Batch:   2 | Lr: 0.00250 | Time used(s): 124.9 | Training loss: 0.0798
Epoch: 42 | Batch:   3 | Lr: 0.00250 | Time used(s): 115.6 | Training loss: 0.0761
Epoch: 42 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.0645
--------------------------------------------------
Epoch: 42 | [Train] | Loss: 0.0798
Epoch: 42 |   [Val] | Loss: 0.0707 | [CCC]:  0.4005 [' 0.4005'] | PCC: 0.4157 ['0.4157'] | RMSE: 0.4535 ['0.4535']
Epoch: 43 | Batch:   1 | Lr: 0.00250 | Time used(s): 129.2 | Training loss: 0.0726
Epoch: 43 | Batch:   2 | Lr: 0.00250 | Time used(s): 128.7 | Training loss: 0.0806
Epoch: 43 | Batch:   3 | Lr: 0.00250 | Time used(s): 111.6 | Training loss: 0.0727
Epoch: 43 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0598
--------------------------------------------------
Epoch: 43 | [Train] | Loss: 0.0751
Epoch: 43 |   [Val] | Loss: 0.0761 | [CCC]:  0.3867 [' 0.3867'] | PCC: 0.4109 ['0.4109'] | RMSE: 0.4783 ['0.4783']
Epoch: 44 | Batch:   1 | Lr: 0.00250 | Time used(s): 127.3 | Training loss: 0.0716
Epoch: 44 | Batch:   2 | Lr: 0.00250 | Time used(s): 129.5 | Training loss: 0.0711
Epoch: 44 | Batch:   3 | Lr: 0.00250 | Time used(s): 122.1 | Training loss: 0.0745
Epoch: 44 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0964
--------------------------------------------------
Epoch: 44 | [Train] | Loss: 0.0728
Epoch: 44 |   [Val] | Loss: 0.0823 | [CCC]:  0.3822 [' 0.3822'] | PCC: 0.4211 ['0.4211'] | RMSE: 0.4808 ['0.4808']
Epoch    44: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 45 | Batch:   1 | Lr: 0.00125 | Time used(s): 138.1 | Training loss: 0.0780
Epoch: 45 | Batch:   2 | Lr: 0.00125 | Time used(s): 124.4 | Training loss: 0.0723
Epoch: 45 | Batch:   3 | Lr: 0.00125 | Time used(s): 118.6 | Training loss: 0.0651
Epoch: 45 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0577
--------------------------------------------------
Epoch: 45 | [Train] | Loss: 0.0715
Epoch: 45 |   [Val] | Loss: 0.0732 | [CCC]:  0.4241 [' 0.4241'] | PCC: 0.4282 ['0.4282'] | RMSE: 0.4112 ['0.4112']
Epoch: 45 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 46 | Batch:   1 | Lr: 0.00125 | Time used(s): 136.6 | Training loss: 0.0705
Epoch: 46 | Batch:   2 | Lr: 0.00125 | Time used(s): 117.6 | Training loss: 0.0645
Epoch: 46 | Batch:   3 | Lr: 0.00125 | Time used(s): 116.5 | Training loss: 0.0711
Epoch: 46 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0356
--------------------------------------------------
Epoch: 46 | [Train] | Loss: 0.0682
Epoch: 46 |   [Val] | Loss: 0.0724 | [CCC]:  0.4275 [' 0.4275'] | PCC: 0.4281 ['0.4281'] | RMSE: 0.4359 ['0.4359']
Epoch: 46 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 47 | Batch:   1 | Lr: 0.00125 | Time used(s): 132.3 | Training loss: 0.0691
Epoch: 47 | Batch:   2 | Lr: 0.00125 | Time used(s): 124.3 | Training loss: 0.0621
Epoch: 47 | Batch:   3 | Lr: 0.00125 | Time used(s): 116.2 | Training loss: 0.0672
Epoch: 47 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0523
--------------------------------------------------
Epoch: 47 | [Train] | Loss: 0.0659
Epoch: 47 |   [Val] | Loss: 0.0711 | [CCC]:  0.4230 [' 0.4230'] | PCC: 0.4257 ['0.4257'] | RMSE: 0.4376 ['0.4376']
Epoch: 48 | Batch:   1 | Lr: 0.00125 | Time used(s): 130.1 | Training loss: 0.0603
Epoch: 48 | Batch:   2 | Lr: 0.00125 | Time used(s): 120.8 | Training loss: 0.0653
Epoch: 48 | Batch:   3 | Lr: 0.00125 | Time used(s): 90.4 | Training loss: 0.0603
Epoch: 48 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0815
--------------------------------------------------
Epoch: 48 | [Train] | Loss: 0.0623
Epoch: 48 |   [Val] | Loss: 0.0717 | [CCC]:  0.4148 [' 0.4148'] | PCC: 0.4236 ['0.4236'] | RMSE: 0.4273 ['0.4273']
Epoch: 49 | Batch:   1 | Lr: 0.00125 | Time used(s): 114.3 | Training loss: 0.0642
Epoch: 49 | Batch:   2 | Lr: 0.00125 | Time used(s): 105.6 | Training loss: 0.0560
Epoch: 49 | Batch:   3 | Lr: 0.00125 | Time used(s): 96.6 | Training loss: 0.0645
Epoch: 49 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0667
--------------------------------------------------
Epoch: 49 | [Train] | Loss: 0.0616
Epoch: 49 |   [Val] | Loss: 0.0761 | [CCC]:  0.4086 [' 0.4086'] | PCC: 0.4294 ['0.4294'] | RMSE: 0.4489 ['0.4489']
Epoch: 50 | Batch:   1 | Lr: 0.00125 | Time used(s): 117.6 | Training loss: 0.0669
Epoch: 50 | Batch:   2 | Lr: 0.00125 | Time used(s): 91.4 | Training loss: 0.0601
Epoch: 50 | Batch:   3 | Lr: 0.00125 | Time used(s): 109.7 | Training loss: 0.0621
Epoch: 50 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0703
--------------------------------------------------
Epoch: 50 | [Train] | Loss: 0.0631
Epoch: 50 |   [Val] | Loss: 0.0726 | [CCC]:  0.4351 [' 0.4351'] | PCC: 0.4353 ['0.4353'] | RMSE: 0.4278 ['0.4278']
Epoch: 50 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 51 | Batch:   1 | Lr: 0.00125 | Time used(s): 104.7 | Training loss: 0.0577
Epoch: 51 | Batch:   2 | Lr: 0.00125 | Time used(s): 108.7 | Training loss: 0.0582
Epoch: 51 | Batch:   3 | Lr: 0.00125 | Time used(s): 104.0 | Training loss: 0.0613
Epoch: 51 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0728
--------------------------------------------------
Epoch: 51 | [Train] | Loss: 0.0593
Epoch: 51 |   [Val] | Loss: 0.0715 | [CCC]:  0.4331 [' 0.4331'] | PCC: 0.4364 ['0.4364'] | RMSE: 0.4337 ['0.4337']
Epoch: 52 | Batch:   1 | Lr: 0.00125 | Time used(s): 108.5 | Training loss: 0.0540
Epoch: 52 | Batch:   2 | Lr: 0.00125 | Time used(s): 95.4 | Training loss: 0.0581
Epoch: 52 | Batch:   3 | Lr: 0.00125 | Time used(s): 96.4 | Training loss: 0.0582
Epoch: 52 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0567
--------------------------------------------------
Epoch: 52 | [Train] | Loss: 0.0568
Epoch: 52 |   [Val] | Loss: 0.0727 | [CCC]:  0.4280 [' 0.4280'] | PCC: 0.4297 ['0.4297'] | RMSE: 0.4212 ['0.4212']
Epoch: 53 | Batch:   1 | Lr: 0.00125 | Time used(s): 120.0 | Training loss: 0.0600
Epoch: 53 | Batch:   2 | Lr: 0.00125 | Time used(s): 103.3 | Training loss: 0.0620
Epoch: 53 | Batch:   3 | Lr: 0.00125 | Time used(s): 107.8 | Training loss: 0.0608
Epoch: 53 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0782
--------------------------------------------------
Epoch: 53 | [Train] | Loss: 0.0612
Epoch: 53 |   [Val] | Loss: 0.0749 | [CCC]:  0.4255 [' 0.4255'] | PCC: 0.4262 ['0.4262'] | RMSE: 0.4357 ['0.4357']
Epoch: 54 | Batch:   1 | Lr: 0.00125 | Time used(s): 104.5 | Training loss: 0.0559
Epoch: 54 | Batch:   2 | Lr: 0.00125 | Time used(s): 99.6 | Training loss: 0.0665
Epoch: 54 | Batch:   3 | Lr: 0.00125 | Time used(s): 88.8 | Training loss: 0.0561
Epoch: 54 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0695
--------------------------------------------------
Epoch: 54 | [Train] | Loss: 0.0596
Epoch: 54 |   [Val] | Loss: 0.0743 | [CCC]:  0.4128 [' 0.4128'] | PCC: 0.4265 ['0.4265'] | RMSE: 0.4586 ['0.4586']
Epoch: 55 | Batch:   1 | Lr: 0.00125 | Time used(s): 104.9 | Training loss: 0.0543
Epoch: 55 | Batch:   2 | Lr: 0.00125 | Time used(s): 96.5 | Training loss: 0.0518
Epoch: 55 | Batch:   3 | Lr: 0.00125 | Time used(s): 89.9 | Training loss: 0.0600
Epoch: 55 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0482
--------------------------------------------------
Epoch: 55 | [Train] | Loss: 0.0553
Epoch: 55 |   [Val] | Loss: 0.0751 | [CCC]:  0.4244 [' 0.4244'] | PCC: 0.4392 ['0.4392'] | RMSE: 0.4478 ['0.4478']
Epoch: 56 | Batch:   1 | Lr: 0.00125 | Time used(s): 112.6 | Training loss: 0.0547
Epoch: 56 | Batch:   2 | Lr: 0.00125 | Time used(s): 91.2 | Training loss: 0.0639
Epoch: 56 | Batch:   3 | Lr: 0.00125 | Time used(s): 85.5 | Training loss: 0.0572
Epoch: 56 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0550
--------------------------------------------------
Epoch: 56 | [Train] | Loss: 0.0585
Epoch: 56 |   [Val] | Loss: 0.0667 | [CCC]:  0.4383 [' 0.4383'] | PCC: 0.4409 ['0.4409'] | RMSE: 0.4141 ['0.4141']
Epoch: 56 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_322_None_None].pth"!
Epoch: 57 | Batch:   1 | Lr: 0.00125 | Time used(s): 118.7 | Training loss: 0.0622
Epoch: 57 | Batch:   2 | Lr: 0.00125 | Time used(s): 96.1 | Training loss: 0.0474
Epoch: 57 | Batch:   3 | Lr: 0.00125 | Time used(s): 85.1 | Training loss: 0.0485
Epoch: 57 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0385
--------------------------------------------------
Epoch: 57 | [Train] | Loss: 0.0525
Epoch: 57 |   [Val] | Loss: 0.0705 | [CCC]:  0.4287 [' 0.4287'] | PCC: 0.4290 ['0.4290'] | RMSE: 0.4366 ['0.4366']
Epoch: 58 | Batch:   1 | Lr: 0.00125 | Time used(s): 107.3 | Training loss: 0.0468
Epoch: 58 | Batch:   2 | Lr: 0.00125 | Time used(s): 89.4 | Training loss: 0.0489
Epoch: 58 | Batch:   3 | Lr: 0.00125 | Time used(s): 90.4 | Training loss: 0.0494
Epoch: 58 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0448
--------------------------------------------------
Epoch: 58 | [Train] | Loss: 0.0483
Epoch: 58 |   [Val] | Loss: 0.0738 | [CCC]:  0.4337 [' 0.4337'] | PCC: 0.4340 ['0.4340'] | RMSE: 0.4346 ['0.4346']
Epoch: 59 | Batch:   1 | Lr: 0.00125 | Time used(s): 100.6 | Training loss: 0.0393
Epoch: 59 | Batch:   2 | Lr: 0.00125 | Time used(s): 90.8 | Training loss: 0.0441
Epoch: 59 | Batch:   3 | Lr: 0.00125 | Time used(s): 104.8 | Training loss: 0.0516
Epoch: 59 | Batch:   4 | Lr: 0.00125 | Time used(s): 1.9 | Training loss: 0.0712
--------------------------------------------------
Epoch: 59 | [Train] | Loss: 0.0454
Epoch: 59 |   [Val] | Loss: 0.0746 | [CCC]:  0.4097 [' 0.4097'] | PCC: 0.4210 ['0.4210'] | RMSE: 0.4614 ['0.4614']
Epoch: 60 | Batch:   1 | Lr: 0.00125 | Time used(s): 107.3 | Training loss: 0.0477
Epoch: 60 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.5 | Training loss: 0.0432
Epoch: 60 | Batch:   3 | Lr: 0.00125 | Time used(s): 82.2 | Training loss: 0.0514
Epoch: 60 | Batch:   4 | Lr: 0.00125 | Time used(s): 1.9 | Training loss: 0.0429
--------------------------------------------------
Epoch: 60 | [Train] | Loss: 0.0474
Epoch: 60 |   [Val] | Loss: 0.0817 | [CCC]:  0.4125 [' 0.4125'] | PCC: 0.4221 ['0.4221'] | RMSE: 0.4828 ['0.4828']
Epoch: 61 | Batch:   1 | Lr: 0.00125 | Time used(s): 83.8 | Training loss: 0.0487
Epoch: 61 | Batch:   2 | Lr: 0.00125 | Time used(s): 82.7 | Training loss: 0.0541
Epoch: 61 | Batch:   3 | Lr: 0.00125 | Time used(s): 83.4 | Training loss: 0.0367
Epoch: 61 | Batch:   4 | Lr: 0.00125 | Time used(s): 1.9 | Training loss: 0.0322
--------------------------------------------------
Epoch: 61 | [Train] | Loss: 0.0463
Epoch: 61 |   [Val] | Loss: 0.0689 | [CCC]:  0.4198 [' 0.4198'] | PCC: 0.4332 ['0.4332'] | RMSE: 0.4213 ['0.4213']
Epoch: 62 | Batch:   1 | Lr: 0.00125 | Time used(s): 83.0 | Training loss: 0.0531
Epoch: 62 | Batch:   2 | Lr: 0.00125 | Time used(s): 83.5 | Training loss: 0.0504
Epoch: 62 | Batch:   3 | Lr: 0.00125 | Time used(s): 82.4 | Training loss: 0.0326
Epoch: 62 | Batch:   4 | Lr: 0.00125 | Time used(s): 1.9 | Training loss: 0.0220
--------------------------------------------------
Epoch: 62 | [Train] | Loss: 0.0450
Epoch: 62 |   [Val] | Loss: 0.0772 | [CCC]:  0.4248 [' 0.4248'] | PCC: 0.4297 ['0.4297'] | RMSE: 0.4644 ['0.4644']
Epoch    62: reducing learning rate of group 0 to 6.2500e-04.
Epoch: 63 | Batch:   1 | Lr: 0.00063 | Time used(s): 84.3 | Training loss: 0.0386
Epoch: 63 | Batch:   2 | Lr: 0.00063 | Time used(s): 82.6 | Training loss: 0.0431
Epoch: 63 | Batch:   3 | Lr: 0.00063 | Time used(s): 82.7 | Training loss: 0.0467
Epoch: 63 | Batch:   4 | Lr: 0.00063 | Time used(s): 1.9 | Training loss: 0.0362
--------------------------------------------------
Epoch: 63 | [Train] | Loss: 0.0427
Epoch: 63 |   [Val] | Loss: 0.0724 | [CCC]:  0.4272 [' 0.4272'] | PCC: 0.4298 ['0.4298'] | RMSE: 0.4226 ['0.4226']
Epoch: 64 | Batch:   1 | Lr: 0.00063 | Time used(s): 99.4 | Training loss: 0.0464
Epoch: 64 | Batch:   2 | Lr: 0.00063 | Time used(s): 88.7 | Training loss: 0.0311
Epoch: 64 | Batch:   3 | Lr: 0.00063 | Time used(s): 86.6 | Training loss: 0.0352
Epoch: 64 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.1 | Training loss: 0.0440
--------------------------------------------------
Epoch: 64 | [Train] | Loss: 0.0376
Epoch: 64 |   [Val] | Loss: 0.0733 | [CCC]:  0.4267 [' 0.4267'] | PCC: 0.4305 ['0.4305'] | RMSE: 0.4351 ['0.4351']
Epoch: 65 | Batch:   1 | Lr: 0.00063 | Time used(s): 104.4 | Training loss: 0.0414
Epoch: 65 | Batch:   2 | Lr: 0.00063 | Time used(s): 88.2 | Training loss: 0.0338
Epoch: 65 | Batch:   3 | Lr: 0.00063 | Time used(s): 87.5 | Training loss: 0.0317
Epoch: 65 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.0 | Training loss: 0.0329
--------------------------------------------------
Epoch: 65 | [Train] | Loss: 0.0356
Epoch: 65 |   [Val] | Loss: 0.0740 | [CCC]:  0.4300 [' 0.4300'] | PCC: 0.4303 ['0.4303'] | RMSE: 0.4427 ['0.4427']
Epoch: 66 | Batch:   1 | Lr: 0.00063 | Time used(s): 123.5 | Training loss: 0.0462
Epoch: 66 | Batch:   2 | Lr: 0.00063 | Time used(s): 104.7 | Training loss: 0.0286
Epoch: 66 | Batch:   3 | Lr: 0.00063 | Time used(s): 86.3 | Training loss: 0.0276
Epoch: 66 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.0 | Training loss: 0.0295
--------------------------------------------------
Epoch: 66 | [Train] | Loss: 0.0341
Epoch: 66 |   [Val] | Loss: 0.0734 | [CCC]:  0.4253 [' 0.4253'] | PCC: 0.4265 ['0.4265'] | RMSE: 0.4374 ['0.4374']
Epoch: 67 | Batch:   1 | Lr: 0.00063 | Time used(s): 124.5 | Training loss: 0.0341
Epoch: 67 | Batch:   2 | Lr: 0.00063 | Time used(s): 105.1 | Training loss: 0.0263
Epoch: 67 | Batch:   3 | Lr: 0.00063 | Time used(s): 87.6 | Training loss: 0.0344
Epoch: 67 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.0 | Training loss: 0.0601
--------------------------------------------------
Epoch: 67 | [Train] | Loss: 0.0321
Epoch: 67 |   [Val] | Loss: 0.0751 | [CCC]:  0.4263 [' 0.4263'] | PCC: 0.4266 ['0.4266'] | RMSE: 0.4317 ['0.4317']
Epoch: 68 | Batch:   1 | Lr: 0.00063 | Time used(s): 90.1 | Training loss: 0.0362
Epoch: 68 | Batch:   2 | Lr: 0.00063 | Time used(s): 85.1 | Training loss: 0.0267
Epoch: 68 | Batch:   3 | Lr: 0.00063 | Time used(s): 86.6 | Training loss: 0.0276
Epoch: 68 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.1 | Training loss: 0.0206
--------------------------------------------------
Epoch: 68 | [Train] | Loss: 0.0300
Epoch: 68 |   [Val] | Loss: 0.0748 | [CCC]:  0.4117 [' 0.4117'] | PCC: 0.4187 ['0.4187'] | RMSE: 0.4512 ['0.4512']
Epoch    68: reducing learning rate of group 0 to 3.1250e-04.
Epoch: 69 | Batch:   1 | Lr: 0.00031 | Time used(s): 88.1 | Training loss: 0.0356
Epoch: 69 | Batch:   2 | Lr: 0.00031 | Time used(s): 85.9 | Training loss: 0.0293
Epoch: 69 | Batch:   3 | Lr: 0.00031 | Time used(s): 85.9 | Training loss: 0.0295
Epoch: 69 | Batch:   4 | Lr: 0.00031 | Time used(s): 2.0 | Training loss: 0.0015
--------------------------------------------------
Epoch: 69 | [Train] | Loss: 0.0310
Epoch: 69 |   [Val] | Loss: 0.0747 | [CCC]:  0.4183 [' 0.4183'] | PCC: 0.4203 ['0.4203'] | RMSE: 0.4400 ['0.4400']
Epoch: 70 | Batch:   1 | Lr: 0.00031 | Time used(s): 89.0 | Training loss: 0.0295
Epoch: 70 | Batch:   2 | Lr: 0.00031 | Time used(s): 86.1 | Training loss: 0.0301
Epoch: 70 | Batch:   3 | Lr: 0.00031 | Time used(s): 84.3 | Training loss: 0.0260
Epoch: 70 | Batch:   4 | Lr: 0.00031 | Time used(s): 2.0 | Training loss: 0.0674
--------------------------------------------------
Epoch: 70 | [Train] | Loss: 0.0291
Epoch: 70 |   [Val] | Loss: 0.0744 | [CCC]:  0.4235 [' 0.4235'] | PCC: 0.4263 ['0.4263'] | RMSE: 0.4444 ['0.4444']
Epoch: 71 | Batch:   1 | Lr: 0.00031 | Time used(s): 88.7 | Training loss: 0.0313
Epoch: 71 | Batch:   2 | Lr: 0.00031 | Time used(s): 85.2 | Training loss: 0.0248
Epoch: 71 | Batch:   3 | Lr: 0.00031 | Time used(s): 87.1 | Training loss: 0.0236
Epoch: 71 | Batch:   4 | Lr: 0.00031 | Time used(s): 2.0 | Training loss: 0.0713
--------------------------------------------------
Epoch: 71 | [Train] | Loss: 0.0273
Epoch: 71 |   [Val] | Loss: 0.0731 | [CCC]:  0.4246 [' 0.4246'] | PCC: 0.4256 ['0.4256'] | RMSE: 0.4267 ['0.4267']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 322 | Best [Val CCC]: 0.4383 [' 0.4383']| Loss: 0.0667 | PCC: 0.4409 ['0.4409'] | RMSE: 0.4141 ['0.4141']
On Test: CCC  0.5569 | PCC  0.5645 | RMSE  0.3836
****************************************************************************************************
Seed "322" over!
****************************************************************************************************
****************************************************************************************************
Using seed "323"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 323]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.9 | Training loss: 0.2497
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.2 | Training loss: 0.2441
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.5 | Training loss: 0.2291
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.2437
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2410
Epoch:  1 |   [Val] | Loss: 0.2005 | [CCC]:  0.0994 [' 0.0994'] | PCC: 0.1763 ['0.1763'] | RMSE: 0.5407 ['0.5407']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_323_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.9 | Training loss: 0.2143
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.0 | Training loss: 0.2441
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.2 | Training loss: 0.2466
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.2464
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2352
Epoch:  2 |   [Val] | Loss: 0.2310 | [CCC]:  0.0426 [' 0.0426'] | PCC: 0.1257 ['0.1257'] | RMSE: 0.6866 ['0.6866']
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 107.7 | Training loss: 0.2383
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 91.4 | Training loss: 0.2211
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.6 | Training loss: 0.2069
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2187
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.2220
Epoch:  3 |   [Val] | Loss: 0.1714 | [CCC]:  0.1794 [' 0.1794'] | PCC: 0.1911 ['0.1911'] | RMSE: 0.5644 ['0.5644']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_323_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.2 | Training loss: 0.1969
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.1 | Training loss: 0.1844
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.2 | Training loss: 0.2202
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1751
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.2001
Epoch:  4 |   [Val] | Loss: 0.1333 | [CCC]:  0.2518 [' 0.2518'] | PCC: 0.2656 ['0.2656'] | RMSE: 0.5143 ['0.5143']
Epoch:  4 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_323_None_None].pth"!
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.5 | Training loss: 0.1595
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 90.7 | Training loss: 0.2151
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.9 | Training loss: 0.2109
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1668
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1947
Epoch:  5 |   [Val] | Loss: 0.1492 | [CCC]:  0.2099 [' 0.2099'] | PCC: 0.2916 ['0.2916'] | RMSE: 0.5665 ['0.5665']
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 94.0 | Training loss: 0.1766
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.9 | Training loss: 0.1672
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.2 | Training loss: 0.1732
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1420
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1718
Epoch:  6 |   [Val] | Loss: 0.1184 | [CCC]:  0.2776 [' 0.2776'] | PCC: 0.2986 ['0.2986'] | RMSE: 0.5012 ['0.5012']
Epoch:  6 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_323_None_None].pth"!
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.3 | Training loss: 0.1657
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.7 | Training loss: 0.1651
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.0 | Training loss: 0.1418
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1744
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1578
Epoch:  7 |   [Val] | Loss: 0.1223 | [CCC]:  0.2816 [' 0.2816'] | PCC: 0.3005 ['0.3005'] | RMSE: 0.4604 ['0.4604']
Epoch:  7 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_323_None_None].pth"!
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.8 | Training loss: 0.1577
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 90.0 | Training loss: 0.1341
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.3 | Training loss: 0.1554
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1282
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1487
Epoch:  8 |   [Val] | Loss: 0.1086 | [CCC]:  0.3028 [' 0.3028'] | PCC: 0.3157 ['0.3157'] | RMSE: 0.4586 ['0.4586']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_323_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.7 | Training loss: 0.1346
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.4 | Training loss: 0.1292
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.4 | Training loss: 0.1359
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1666
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1337
Epoch:  9 |   [Val] | Loss: 0.0863 | [CCC]:  0.3551 [' 0.3551'] | PCC: 0.3583 ['0.3583'] | RMSE: 0.4617 ['0.4617']
Epoch:  9 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_323_None_None].pth"!
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.9 | Training loss: 0.1251
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.9 | Training loss: 0.1283
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.8 | Training loss: 0.1256
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1359
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1265
Epoch: 10 |   [Val] | Loss: 0.1163 | [CCC]:  0.2590 [' 0.2590'] | PCC: 0.3115 ['0.3115'] | RMSE: 0.5523 ['0.5523']
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.0 | Training loss: 0.1406
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.2 | Training loss: 0.1271
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.5 | Training loss: 0.1367
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1289
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1347
Epoch: 11 |   [Val] | Loss: 0.0899 | [CCC]:  0.3168 [' 0.3168'] | PCC: 0.3361 ['0.3361'] | RMSE: 0.4522 ['0.4522']
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.4 | Training loss: 0.1119
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.6 | Training loss: 0.1226
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.7 | Training loss: 0.1247
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1408
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1201
Epoch: 12 |   [Val] | Loss: 0.0851 | [CCC]:  0.3442 [' 0.3442'] | PCC: 0.3476 ['0.3476'] | RMSE: 0.4559 ['0.4559']
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.8 | Training loss: 0.1129
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.7 | Training loss: 0.1141
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.8 | Training loss: 0.1118
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1188
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1130
Epoch: 13 |   [Val] | Loss: 0.0868 | [CCC]:  0.3525 [' 0.3525'] | PCC: 0.3826 ['0.3826'] | RMSE: 0.4795 ['0.4795']
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.0 | Training loss: 0.1122
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.8 | Training loss: 0.1099
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.5 | Training loss: 0.1092
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1336
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1108
Epoch: 14 |   [Val] | Loss: 0.0708 | [CCC]:  0.4156 [' 0.4156'] | PCC: 0.4172 ['0.4172'] | RMSE: 0.4281 ['0.4281']
Epoch: 14 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_323_None_None].pth"!
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.3 | Training loss: 0.1029
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.9 | Training loss: 0.1099
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.6 | Training loss: 0.1063
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0762
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1059
Epoch: 15 |   [Val] | Loss: 0.0710 | [CCC]:  0.4071 [' 0.4071'] | PCC: 0.4185 ['0.4185'] | RMSE: 0.4410 ['0.4410']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.6 | Training loss: 0.1130
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.5 | Training loss: 0.0897
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.1009
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0884
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.1010
Epoch: 16 |   [Val] | Loss: 0.0771 | [CCC]:  0.3640 [' 0.3640'] | PCC: 0.3892 ['0.3892'] | RMSE: 0.4358 ['0.4358']
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.1 | Training loss: 0.1023
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.4 | Training loss: 0.0983
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.0 | Training loss: 0.1034
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1128
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.1015
Epoch: 17 |   [Val] | Loss: 0.0850 | [CCC]:  0.3589 [' 0.3589'] | PCC: 0.3900 ['0.3900'] | RMSE: 0.4824 ['0.4824']
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.5 | Training loss: 0.1078
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.2 | Training loss: 0.0933
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.6 | Training loss: 0.1107
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1291
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.1043
Epoch: 18 |   [Val] | Loss: 0.0723 | [CCC]:  0.3941 [' 0.3941'] | PCC: 0.4052 ['0.4052'] | RMSE: 0.4266 ['0.4266']
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.7 | Training loss: 0.0978
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.8 | Training loss: 0.1003
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.2 | Training loss: 0.0928
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0814
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.0967
Epoch: 19 |   [Val] | Loss: 0.0708 | [CCC]:  0.3749 [' 0.3749'] | PCC: 0.3879 ['0.3879'] | RMSE: 0.4389 ['0.4389']
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.8 | Training loss: 0.0923
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.8 | Training loss: 0.0908
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.4 | Training loss: 0.0907
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1086
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0915
Epoch: 20 |   [Val] | Loss: 0.0830 | [CCC]:  0.3458 [' 0.3458'] | PCC: 0.3602 ['0.3602'] | RMSE: 0.5165 ['0.5165']
Epoch    20: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 21 | Batch:   1 | Lr: 0.00250 | Time used(s): 85.7 | Training loss: 0.0927
Epoch: 21 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.1 | Training loss: 0.0888
Epoch: 21 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.3 | Training loss: 0.0910
Epoch: 21 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.1044
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0911
Epoch: 21 |   [Val] | Loss: 0.0747 | [CCC]:  0.3756 [' 0.3756'] | PCC: 0.3816 ['0.3816'] | RMSE: 0.4249 ['0.4249']
Epoch: 22 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.2 | Training loss: 0.0843
Epoch: 22 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.6 | Training loss: 0.0856
Epoch: 22 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.2 | Training loss: 0.0896
Epoch: 22 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.1072
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0868
Epoch: 22 |   [Val] | Loss: 0.0767 | [CCC]:  0.4098 [' 0.4098'] | PCC: 0.4128 ['0.4128'] | RMSE: 0.4543 ['0.4543']
Epoch: 23 | Batch:   1 | Lr: 0.00250 | Time used(s): 85.8 | Training loss: 0.0808
Epoch: 23 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.6 | Training loss: 0.0826
Epoch: 23 | Batch:   3 | Lr: 0.00250 | Time used(s): 84.7 | Training loss: 0.0871
Epoch: 23 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0925
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0837
Epoch: 23 |   [Val] | Loss: 0.0731 | [CCC]:  0.4181 [' 0.4181'] | PCC: 0.4190 ['0.4190'] | RMSE: 0.4281 ['0.4281']
Epoch: 23 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_323_None_None].pth"!
Epoch: 24 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.8 | Training loss: 0.0776
Epoch: 24 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.6 | Training loss: 0.0813
Epoch: 24 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.2 | Training loss: 0.0791
Epoch: 24 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.1079
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0798
Epoch: 24 |   [Val] | Loss: 0.0693 | [CCC]:  0.4008 [' 0.4008'] | PCC: 0.4068 ['0.4068'] | RMSE: 0.4300 ['0.4300']
Epoch: 25 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.5 | Training loss: 0.0746
Epoch: 25 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.0 | Training loss: 0.0751
Epoch: 25 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.0 | Training loss: 0.0780
Epoch: 25 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0946
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0762
Epoch: 25 |   [Val] | Loss: 0.0741 | [CCC]:  0.4077 [' 0.4077'] | PCC: 0.4079 ['0.4079'] | RMSE: 0.4364 ['0.4364']
Epoch: 26 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.1 | Training loss: 0.0794
Epoch: 26 | Batch:   2 | Lr: 0.00250 | Time used(s): 83.8 | Training loss: 0.0658
Epoch: 26 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.0 | Training loss: 0.0806
Epoch: 26 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0469
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0748
Epoch: 26 |   [Val] | Loss: 0.0775 | [CCC]:  0.4128 [' 0.4128'] | PCC: 0.4132 ['0.4132'] | RMSE: 0.4372 ['0.4372']
Epoch: 27 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.1 | Training loss: 0.0713
Epoch: 27 | Batch:   2 | Lr: 0.00250 | Time used(s): 86.0 | Training loss: 0.0785
Epoch: 27 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.0 | Training loss: 0.0694
Epoch: 27 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0859
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0733
Epoch: 27 |   [Val] | Loss: 0.0729 | [CCC]:  0.4264 [' 0.4264'] | PCC: 0.4430 ['0.4430'] | RMSE: 0.4088 ['0.4088']
Epoch: 27 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_323_None_None].pth"!
Epoch: 28 | Batch:   1 | Lr: 0.00250 | Time used(s): 85.9 | Training loss: 0.0723
Epoch: 28 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.7 | Training loss: 0.0733
Epoch: 28 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.4 | Training loss: 0.0653
Epoch: 28 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0664
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0702
Epoch: 28 |   [Val] | Loss: 0.0732 | [CCC]:  0.4418 [' 0.4418'] | PCC: 0.4445 ['0.4445'] | RMSE: 0.4241 ['0.4241']
Epoch: 28 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_323_None_None].pth"!
Epoch: 29 | Batch:   1 | Lr: 0.00250 | Time used(s): 87.3 | Training loss: 0.0710
Epoch: 29 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.3 | Training loss: 0.0646
Epoch: 29 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.1 | Training loss: 0.0676
Epoch: 29 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0473
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0674
Epoch: 29 |   [Val] | Loss: 0.0762 | [CCC]:  0.3968 [' 0.3968'] | PCC: 0.4189 ['0.4189'] | RMSE: 0.4632 ['0.4632']
Epoch: 30 | Batch:   1 | Lr: 0.00250 | Time used(s): 88.0 | Training loss: 0.0862
Epoch: 30 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.0 | Training loss: 0.0689
Epoch: 30 | Batch:   3 | Lr: 0.00250 | Time used(s): 84.9 | Training loss: 0.0685
Epoch: 30 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.1076
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0750
Epoch: 30 |   [Val] | Loss: 0.0722 | [CCC]:  0.4194 [' 0.4194'] | PCC: 0.4404 ['0.4404'] | RMSE: 0.4268 ['0.4268']
Epoch: 31 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.5 | Training loss: 0.0702
Epoch: 31 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.2 | Training loss: 0.0636
Epoch: 31 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.1 | Training loss: 0.0686
Epoch: 31 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0511
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0672
Epoch: 31 |   [Val] | Loss: 0.0666 | [CCC]:  0.4160 [' 0.4160'] | PCC: 0.4202 ['0.4202'] | RMSE: 0.4211 ['0.4211']
Epoch: 32 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.5 | Training loss: 0.0632
Epoch: 32 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.0 | Training loss: 0.0693
Epoch: 32 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.5 | Training loss: 0.0626
Epoch: 32 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0848
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0654
Epoch: 32 |   [Val] | Loss: 0.0700 | [CCC]:  0.4162 [' 0.4162'] | PCC: 0.4188 ['0.4188'] | RMSE: 0.4563 ['0.4563']
Epoch: 33 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.5 | Training loss: 0.0595
Epoch: 33 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.5 | Training loss: 0.0535
Epoch: 33 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.2 | Training loss: 0.0718
Epoch: 33 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0526
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0615
Epoch: 33 |   [Val] | Loss: 0.0715 | [CCC]:  0.4236 [' 0.4236'] | PCC: 0.4289 ['0.4289'] | RMSE: 0.4181 ['0.4181']
Epoch: 34 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.2 | Training loss: 0.0571
Epoch: 34 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.0 | Training loss: 0.0619
Epoch: 34 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.6 | Training loss: 0.0652
Epoch: 34 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0476
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0612
Epoch: 34 |   [Val] | Loss: 0.0708 | [CCC]:  0.4138 [' 0.4138'] | PCC: 0.4167 ['0.4167'] | RMSE: 0.4324 ['0.4324']
Epoch    34: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 35 | Batch:   1 | Lr: 0.00125 | Time used(s): 86.9 | Training loss: 0.0638
Epoch: 35 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.9 | Training loss: 0.0521
Epoch: 35 | Batch:   3 | Lr: 0.00125 | Time used(s): 86.0 | Training loss: 0.0506
Epoch: 35 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0595
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0556
Epoch: 35 |   [Val] | Loss: 0.0718 | [CCC]:  0.4146 [' 0.4146'] | PCC: 0.4164 ['0.4164'] | RMSE: 0.4295 ['0.4295']
Epoch: 36 | Batch:   1 | Lr: 0.00125 | Time used(s): 87.2 | Training loss: 0.0466
Epoch: 36 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.8 | Training loss: 0.0607
Epoch: 36 | Batch:   3 | Lr: 0.00125 | Time used(s): 86.7 | Training loss: 0.0491
Epoch: 36 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0503
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0521
Epoch: 36 |   [Val] | Loss: 0.0808 | [CCC]:  0.4099 [' 0.4099'] | PCC: 0.4181 ['0.4181'] | RMSE: 0.4658 ['0.4658']
Epoch: 37 | Batch:   1 | Lr: 0.00125 | Time used(s): 87.3 | Training loss: 0.0572
Epoch: 37 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.7 | Training loss: 0.0622
Epoch: 37 | Batch:   3 | Lr: 0.00125 | Time used(s): 85.7 | Training loss: 0.0541
Epoch: 37 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0422
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0575
Epoch: 37 |   [Val] | Loss: 0.0721 | [CCC]:  0.4096 [' 0.4096'] | PCC: 0.4272 ['0.4272'] | RMSE: 0.4288 ['0.4288']
Epoch: 38 | Batch:   1 | Lr: 0.00125 | Time used(s): 87.2 | Training loss: 0.0536
Epoch: 38 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.5 | Training loss: 0.0601
Epoch: 38 | Batch:   3 | Lr: 0.00125 | Time used(s): 85.7 | Training loss: 0.0519
Epoch: 38 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0671
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0554
Epoch: 38 |   [Val] | Loss: 0.0744 | [CCC]:  0.3945 [' 0.3945'] | PCC: 0.4005 ['0.4005'] | RMSE: 0.4314 ['0.4314']
Epoch: 39 | Batch:   1 | Lr: 0.00125 | Time used(s): 85.3 | Training loss: 0.0405
Epoch: 39 | Batch:   2 | Lr: 0.00125 | Time used(s): 84.4 | Training loss: 0.0536
Epoch: 39 | Batch:   3 | Lr: 0.00125 | Time used(s): 85.2 | Training loss: 0.0560
Epoch: 39 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0741
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0504
Epoch: 39 |   [Val] | Loss: 0.0786 | [CCC]:  0.3914 [' 0.3914'] | PCC: 0.3967 ['0.3967'] | RMSE: 0.4387 ['0.4387']
Epoch: 40 | Batch:   1 | Lr: 0.00125 | Time used(s): 86.5 | Training loss: 0.0499
Epoch: 40 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.8 | Training loss: 0.0478
Epoch: 40 | Batch:   3 | Lr: 0.00125 | Time used(s): 85.0 | Training loss: 0.0531
Epoch: 40 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0259
--------------------------------------------------
Epoch: 40 | [Train] | Loss: 0.0499
Epoch: 40 |   [Val] | Loss: 0.0731 | [CCC]:  0.3971 [' 0.3971'] | PCC: 0.4010 ['0.4010'] | RMSE: 0.4296 ['0.4296']
Epoch    40: reducing learning rate of group 0 to 6.2500e-04.
Epoch: 41 | Batch:   1 | Lr: 0.00063 | Time used(s): 87.4 | Training loss: 0.0499
Epoch: 41 | Batch:   2 | Lr: 0.00063 | Time used(s): 84.8 | Training loss: 0.0484
Epoch: 41 | Batch:   3 | Lr: 0.00063 | Time used(s): 85.8 | Training loss: 0.0408
Epoch: 41 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.0 | Training loss: 0.0328
--------------------------------------------------
Epoch: 41 | [Train] | Loss: 0.0461
Epoch: 41 |   [Val] | Loss: 0.0740 | [CCC]:  0.4035 [' 0.4035'] | PCC: 0.4044 ['0.4044'] | RMSE: 0.4375 ['0.4375']
Epoch: 42 | Batch:   1 | Lr: 0.00063 | Time used(s): 87.3 | Training loss: 0.0433
Epoch: 42 | Batch:   2 | Lr: 0.00063 | Time used(s): 84.8 | Training loss: 0.0503
Epoch: 42 | Batch:   3 | Lr: 0.00063 | Time used(s): 86.2 | Training loss: 0.0434
Epoch: 42 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.0 | Training loss: 0.0493
--------------------------------------------------
Epoch: 42 | [Train] | Loss: 0.0457
Epoch: 42 |   [Val] | Loss: 0.0736 | [CCC]:  0.4085 [' 0.4085'] | PCC: 0.4107 ['0.4107'] | RMSE: 0.4376 ['0.4376']
Epoch: 43 | Batch:   1 | Lr: 0.00063 | Time used(s): 86.8 | Training loss: 0.0450
Epoch: 43 | Batch:   2 | Lr: 0.00063 | Time used(s): 86.2 | Training loss: 0.0430
Epoch: 43 | Batch:   3 | Lr: 0.00063 | Time used(s): 85.5 | Training loss: 0.0431
Epoch: 43 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.0 | Training loss: 0.0439
--------------------------------------------------
Epoch: 43 | [Train] | Loss: 0.0437
Epoch: 43 |   [Val] | Loss: 0.0719 | [CCC]:  0.4103 [' 0.4103'] | PCC: 0.4171 ['0.4171'] | RMSE: 0.4313 ['0.4313']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 323 | Best [Val CCC]: 0.4418 [' 0.4418']| Loss: 0.0732 | PCC: 0.4445 ['0.4445'] | RMSE: 0.4241 ['0.4241']
On Test: CCC  0.5665 | PCC  0.5775 | RMSE  0.3844
****************************************************************************************************
Seed "323" over!
****************************************************************************************************
****************************************************************************************************
Using seed "324"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 324]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.0 | Training loss: 0.2498
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.9 | Training loss: 0.2449
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.7 | Training loss: 0.2385
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.2415
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2444
Epoch:  1 |   [Val] | Loss: 0.2384 | [CCC]:  0.0247 [' 0.0247'] | PCC: 0.1223 ['0.1223'] | RMSE: 0.8696 ['0.8696']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_324_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.6 | Training loss: 0.2371
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.0 | Training loss: 0.2156
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.8 | Training loss: 0.1955
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.2118
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2160
Epoch:  2 |   [Val] | Loss: 0.1565 | [CCC]:  0.1946 [' 0.1946'] | PCC: 0.2509 ['0.2509'] | RMSE: 0.5104 ['0.5104']
Epoch:  2 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_324_None_None].pth"!
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.6 | Training loss: 0.1840
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.5 | Training loss: 0.1799
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.2 | Training loss: 0.2170
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.1600
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.1931
Epoch:  3 |   [Val] | Loss: 0.1943 | [CCC]:  0.1189 [' 0.1189'] | PCC: 0.3047 ['0.3047'] | RMSE: 0.6573 ['0.6573']
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.0 | Training loss: 0.2105
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.2 | Training loss: 0.2016
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.2 | Training loss: 0.1762
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1745
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1958
Epoch:  4 |   [Val] | Loss: 0.1473 | [CCC]:  0.2198 [' 0.2198'] | PCC: 0.2621 ['0.2621'] | RMSE: 0.5752 ['0.5752']
Epoch:  4 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_324_None_None].pth"!
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.7 | Training loss: 0.1835
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.8 | Training loss: 0.1537
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.6 | Training loss: 0.1757
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1122
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1700
Epoch:  5 |   [Val] | Loss: 0.1128 | [CCC]:  0.3046 [' 0.3046'] | PCC: 0.3159 ['0.3159'] | RMSE: 0.4741 ['0.4741']
Epoch:  5 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_324_None_None].pth"!
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.3 | Training loss: 0.1510
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.3 | Training loss: 0.1585
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.1 | Training loss: 0.1411
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1444
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1501
Epoch:  6 |   [Val] | Loss: 0.1170 | [CCC]:  0.2682 [' 0.2682'] | PCC: 0.2972 ['0.2972'] | RMSE: 0.5450 ['0.5450']
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.6 | Training loss: 0.1410
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.1 | Training loss: 0.1492
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.1 | Training loss: 0.1347
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1047
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1410
Epoch:  7 |   [Val] | Loss: 0.1066 | [CCC]:  0.3198 [' 0.3198'] | PCC: 0.3205 ['0.3205'] | RMSE: 0.4766 ['0.4766']
Epoch:  7 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_324_None_None].pth"!
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.9 | Training loss: 0.1380
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.4 | Training loss: 0.1374
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.9 | Training loss: 0.1387
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1280
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1379
Epoch:  8 |   [Val] | Loss: 0.1000 | [CCC]:  0.3125 [' 0.3125'] | PCC: 0.3331 ['0.3331'] | RMSE: 0.4996 ['0.4996']
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.1 | Training loss: 0.1265
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.9 | Training loss: 0.1255
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.0 | Training loss: 0.1283
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1027
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1264
Epoch:  9 |   [Val] | Loss: 0.0894 | [CCC]:  0.3340 [' 0.3340'] | PCC: 0.3344 ['0.3344'] | RMSE: 0.4802 ['0.4802']
Epoch:  9 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_324_None_None].pth"!
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.3 | Training loss: 0.1213
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.3 | Training loss: 0.1170
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.6 | Training loss: 0.1254
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1120
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1211
Epoch: 10 |   [Val] | Loss: 0.1310 | [CCC]:  0.2271 [' 0.2271'] | PCC: 0.3203 ['0.3203'] | RMSE: 0.4953 ['0.4953']
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.1 | Training loss: 0.1468
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.2 | Training loss: 0.1302
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.1242
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1083
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1333
Epoch: 11 |   [Val] | Loss: 0.0972 | [CCC]:  0.3519 [' 0.3519'] | PCC: 0.3700 ['0.3700'] | RMSE: 0.5035 ['0.5035']
Epoch: 11 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_324_None_None].pth"!
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.9 | Training loss: 0.1231
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.7 | Training loss: 0.1266
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.2 | Training loss: 0.1199
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1382
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1235
Epoch: 12 |   [Val] | Loss: 0.0887 | [CCC]:  0.3272 [' 0.3272'] | PCC: 0.3463 ['0.3463'] | RMSE: 0.4127 ['0.4127']
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.7 | Training loss: 0.1264
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.2 | Training loss: 0.1327
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.8 | Training loss: 0.1212
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1093
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1265
Epoch: 13 |   [Val] | Loss: 0.0844 | [CCC]:  0.3596 [' 0.3596'] | PCC: 0.3648 ['0.3648'] | RMSE: 0.4794 ['0.4794']
Epoch: 13 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_324_None_None].pth"!
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.0 | Training loss: 0.1116
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.9 | Training loss: 0.1271
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.1 | Training loss: 0.1127
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1784
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1181
Epoch: 14 |   [Val] | Loss: 0.1012 | [CCC]:  0.3163 [' 0.3163'] | PCC: 0.3494 ['0.3494'] | RMSE: 0.4076 ['0.4076']
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.7 | Training loss: 0.1218
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.5 | Training loss: 0.1213
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.1 | Training loss: 0.1109
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0754
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1173
Epoch: 15 |   [Val] | Loss: 0.1136 | [CCC]:  0.2883 [' 0.2883'] | PCC: 0.3680 ['0.3680'] | RMSE: 0.5197 ['0.5197']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.1324
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.2 | Training loss: 0.1069
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.4 | Training loss: 0.1154
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0841
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.1177
Epoch: 16 |   [Val] | Loss: 0.0873 | [CCC]:  0.3349 [' 0.3349'] | PCC: 0.3847 ['0.3847'] | RMSE: 0.3856 ['0.3856']
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.4 | Training loss: 0.1082
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.9 | Training loss: 0.1169
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.4 | Training loss: 0.1099
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0911
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.1113
Epoch: 17 |   [Val] | Loss: 0.0885 | [CCC]:  0.3639 [' 0.3639'] | PCC: 0.3765 ['0.3765'] | RMSE: 0.5245 ['0.5245']
Epoch: 17 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_324_None_None].pth"!
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.7 | Training loss: 0.1109
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.9 | Training loss: 0.1022
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.8 | Training loss: 0.1039
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0634
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.1050
Epoch: 18 |   [Val] | Loss: 0.0773 | [CCC]:  0.3885 [' 0.3885'] | PCC: 0.4083 ['0.4083'] | RMSE: 0.4045 ['0.4045']
Epoch: 18 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_324_None_None].pth"!
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.1 | Training loss: 0.1070
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.8 | Training loss: 0.1016
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.0 | Training loss: 0.1003
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1113
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.1031
Epoch: 19 |   [Val] | Loss: 0.0841 | [CCC]:  0.4149 [' 0.4149'] | PCC: 0.4159 ['0.4159'] | RMSE: 0.4482 ['0.4482']
Epoch: 19 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_324_None_None].pth"!
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.1017
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.3 | Training loss: 0.0992
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 83.8 | Training loss: 0.0927
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0960
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0978
Epoch: 20 |   [Val] | Loss: 0.0749 | [CCC]:  0.3954 [' 0.3954'] | PCC: 0.4101 ['0.4101'] | RMSE: 0.3996 ['0.3996']
Epoch: 21 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.1 | Training loss: 0.0964
Epoch: 21 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.9 | Training loss: 0.0934
Epoch: 21 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.8 | Training loss: 0.0928
Epoch: 21 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1265
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0947
Epoch: 21 |   [Val] | Loss: 0.0763 | [CCC]:  0.4113 [' 0.4113'] | PCC: 0.4124 ['0.4124'] | RMSE: 0.4540 ['0.4540']
Epoch: 22 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.5 | Training loss: 0.0906
Epoch: 22 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.5 | Training loss: 0.0873
Epoch: 22 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.0902
Epoch: 22 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0647
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0890
Epoch: 22 |   [Val] | Loss: 0.0764 | [CCC]:  0.3955 [' 0.3955'] | PCC: 0.4098 ['0.4098'] | RMSE: 0.4190 ['0.4190']
Epoch: 23 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.1 | Training loss: 0.0854
Epoch: 23 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.5 | Training loss: 0.0899
Epoch: 23 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.7 | Training loss: 0.0832
Epoch: 23 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0646
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0858
Epoch: 23 |   [Val] | Loss: 0.0805 | [CCC]:  0.3837 [' 0.3837'] | PCC: 0.4084 ['0.4084'] | RMSE: 0.4950 ['0.4950']
Epoch: 24 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.0866
Epoch: 24 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.9 | Training loss: 0.0931
Epoch: 24 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.0905
Epoch: 24 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0892
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0901
Epoch: 24 |   [Val] | Loss: 0.0770 | [CCC]:  0.3985 [' 0.3985'] | PCC: 0.4008 ['0.4008'] | RMSE: 0.4312 ['0.4312']
Epoch: 25 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.2 | Training loss: 0.0758
Epoch: 25 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.9 | Training loss: 0.0831
Epoch: 25 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.3 | Training loss: 0.0796
Epoch: 25 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1414
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0805
Epoch: 25 |   [Val] | Loss: 0.0709 | [CCC]:  0.4191 [' 0.4191'] | PCC: 0.4198 ['0.4198'] | RMSE: 0.4327 ['0.4327']
Epoch: 25 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_324_None_None].pth"!
Epoch: 26 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.5 | Training loss: 0.0705
Epoch: 26 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.0 | Training loss: 0.0765
Epoch: 26 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.1 | Training loss: 0.0799
Epoch: 26 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0639
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0754
Epoch: 26 |   [Val] | Loss: 0.0788 | [CCC]:  0.4094 [' 0.4094'] | PCC: 0.4191 ['0.4191'] | RMSE: 0.4427 ['0.4427']
Epoch: 27 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.6 | Training loss: 0.0719
Epoch: 27 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.7 | Training loss: 0.0799
Epoch: 27 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.4 | Training loss: 0.0812
Epoch: 27 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0906
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0779
Epoch: 27 |   [Val] | Loss: 0.0712 | [CCC]:  0.4282 [' 0.4282'] | PCC: 0.4328 ['0.4328'] | RMSE: 0.4256 ['0.4256']
Epoch: 27 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_324_None_None].pth"!
Epoch: 28 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.4 | Training loss: 0.0672
Epoch: 28 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.6 | Training loss: 0.0705
Epoch: 28 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.0735
Epoch: 28 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0822
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0706
Epoch: 28 |   [Val] | Loss: 0.0708 | [CCC]:  0.4358 [' 0.4358'] | PCC: 0.4387 ['0.4387'] | RMSE: 0.4152 ['0.4152']
Epoch: 28 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_324_None_None].pth"!
Epoch: 29 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.0 | Training loss: 0.0751
Epoch: 29 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.8 | Training loss: 0.0639
Epoch: 29 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.5 | Training loss: 0.0661
Epoch: 29 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0444
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0680
Epoch: 29 |   [Val] | Loss: 0.0800 | [CCC]:  0.3664 [' 0.3664'] | PCC: 0.3896 ['0.3896'] | RMSE: 0.4439 ['0.4439']
Epoch: 30 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.3 | Training loss: 0.0803
Epoch: 30 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.0 | Training loss: 0.0802
Epoch: 30 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.0 | Training loss: 0.0733
Epoch: 30 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0542
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0776
Epoch: 30 |   [Val] | Loss: 0.0833 | [CCC]:  0.4005 [' 0.4005'] | PCC: 0.4245 ['0.4245'] | RMSE: 0.4518 ['0.4518']
Epoch: 31 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.7 | Training loss: 0.0748
Epoch: 31 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.5 | Training loss: 0.0753
Epoch: 31 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.4 | Training loss: 0.0702
Epoch: 31 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0644
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0733
Epoch: 31 |   [Val] | Loss: 0.0788 | [CCC]:  0.3898 [' 0.3898'] | PCC: 0.4286 ['0.4286'] | RMSE: 0.4236 ['0.4236']
Epoch: 32 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.1 | Training loss: 0.0739
Epoch: 32 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.5 | Training loss: 0.0753
Epoch: 32 | Batch:   3 | Lr: 0.00500 | Time used(s): 83.6 | Training loss: 0.0708
Epoch: 32 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0967
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0737
Epoch: 32 |   [Val] | Loss: 0.0826 | [CCC]:  0.3936 [' 0.3936'] | PCC: 0.4144 ['0.4144'] | RMSE: 0.4815 ['0.4815']
Epoch: 33 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.1 | Training loss: 0.0762
Epoch: 33 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.1 | Training loss: 0.0652
Epoch: 33 | Batch:   3 | Lr: 0.00500 | Time used(s): 83.1 | Training loss: 0.0789
Epoch: 33 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.0741
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0735
Epoch: 33 |   [Val] | Loss: 0.0834 | [CCC]:  0.3849 [' 0.3849'] | PCC: 0.4134 ['0.4134'] | RMSE: 0.4763 ['0.4763']
Epoch: 34 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.8 | Training loss: 0.0783
Epoch: 34 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.9 | Training loss: 0.0784
Epoch: 34 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.0 | Training loss: 0.0649
Epoch: 34 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0562
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0735
Epoch: 34 |   [Val] | Loss: 0.0823 | [CCC]:  0.3902 [' 0.3902'] | PCC: 0.4089 ['0.4089'] | RMSE: 0.4661 ['0.4661']
Epoch    34: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 35 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.1 | Training loss: 0.0719
Epoch: 35 | Batch:   2 | Lr: 0.00250 | Time used(s): 83.0 | Training loss: 0.0641
Epoch: 35 | Batch:   3 | Lr: 0.00250 | Time used(s): 84.3 | Training loss: 0.0626
Epoch: 35 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0467
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0659
Epoch: 35 |   [Val] | Loss: 0.0737 | [CCC]:  0.4128 [' 0.4128'] | PCC: 0.4144 ['0.4144'] | RMSE: 0.4287 ['0.4287']
Epoch: 36 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.3 | Training loss: 0.0577
Epoch: 36 | Batch:   2 | Lr: 0.00250 | Time used(s): 83.3 | Training loss: 0.0554
Epoch: 36 | Batch:   3 | Lr: 0.00250 | Time used(s): 84.2 | Training loss: 0.0612
Epoch: 36 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0773
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0584
Epoch: 36 |   [Val] | Loss: 0.0765 | [CCC]:  0.4235 [' 0.4235'] | PCC: 0.4285 ['0.4285'] | RMSE: 0.4146 ['0.4146']
Epoch: 37 | Batch:   1 | Lr: 0.00250 | Time used(s): 84.8 | Training loss: 0.0588
Epoch: 37 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.0 | Training loss: 0.0599
Epoch: 37 | Batch:   3 | Lr: 0.00250 | Time used(s): 84.0 | Training loss: 0.0655
Epoch: 37 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0458
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0611
Epoch: 37 |   [Val] | Loss: 0.0763 | [CCC]:  0.4100 [' 0.4100'] | PCC: 0.4186 ['0.4186'] | RMSE: 0.4274 ['0.4274']
Epoch: 38 | Batch:   1 | Lr: 0.00250 | Time used(s): 85.5 | Training loss: 0.0562
Epoch: 38 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.2 | Training loss: 0.0551
Epoch: 38 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.1 | Training loss: 0.0537
Epoch: 38 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0662
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0552
Epoch: 38 |   [Val] | Loss: 0.0781 | [CCC]:  0.3996 [' 0.3996'] | PCC: 0.4055 ['0.4055'] | RMSE: 0.4215 ['0.4215']
Epoch: 39 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.6 | Training loss: 0.0560
Epoch: 39 | Batch:   2 | Lr: 0.00250 | Time used(s): 83.9 | Training loss: 0.0545
Epoch: 39 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.2 | Training loss: 0.0483
Epoch: 39 | Batch:   4 | Lr: 0.00250 | Time used(s): 1.9 | Training loss: 0.0395
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0527
Epoch: 39 |   [Val] | Loss: 0.0800 | [CCC]:  0.4215 [' 0.4215'] | PCC: 0.4225 ['0.4225'] | RMSE: 0.4256 ['0.4256']
Epoch: 40 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.1 | Training loss: 0.0518
Epoch: 40 | Batch:   2 | Lr: 0.00250 | Time used(s): 83.6 | Training loss: 0.0407
Epoch: 40 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.1 | Training loss: 0.0504
Epoch: 40 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0597
--------------------------------------------------
Epoch: 40 | [Train] | Loss: 0.0478
Epoch: 40 |   [Val] | Loss: 0.0790 | [CCC]:  0.3924 [' 0.3924'] | PCC: 0.4150 ['0.4150'] | RMSE: 0.4091 ['0.4091']
Epoch    40: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 41 | Batch:   1 | Lr: 0.00125 | Time used(s): 85.5 | Training loss: 0.0449
Epoch: 41 | Batch:   2 | Lr: 0.00125 | Time used(s): 83.4 | Training loss: 0.0471
Epoch: 41 | Batch:   3 | Lr: 0.00125 | Time used(s): 83.2 | Training loss: 0.0490
Epoch: 41 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0416
--------------------------------------------------
Epoch: 41 | [Train] | Loss: 0.0469
Epoch: 41 |   [Val] | Loss: 0.0763 | [CCC]:  0.4167 [' 0.4167'] | PCC: 0.4168 ['0.4168'] | RMSE: 0.4406 ['0.4406']
Epoch: 42 | Batch:   1 | Lr: 0.00125 | Time used(s): 87.1 | Training loss: 0.0439
Epoch: 42 | Batch:   2 | Lr: 0.00125 | Time used(s): 83.1 | Training loss: 0.0430
Epoch: 42 | Batch:   3 | Lr: 0.00125 | Time used(s): 84.2 | Training loss: 0.0439
Epoch: 42 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0586
--------------------------------------------------
Epoch: 42 | [Train] | Loss: 0.0439
Epoch: 42 |   [Val] | Loss: 0.0779 | [CCC]:  0.4128 [' 0.4128'] | PCC: 0.4193 ['0.4193'] | RMSE: 0.4341 ['0.4341']
Epoch: 43 | Batch:   1 | Lr: 0.00125 | Time used(s): 84.8 | Training loss: 0.0441
Epoch: 43 | Batch:   2 | Lr: 0.00125 | Time used(s): 84.7 | Training loss: 0.0388
Epoch: 43 | Batch:   3 | Lr: 0.00125 | Time used(s): 85.7 | Training loss: 0.0330
Epoch: 43 | Batch:   4 | Lr: 0.00125 | Time used(s): 1.9 | Training loss: 0.0526
--------------------------------------------------
Epoch: 43 | [Train] | Loss: 0.0389
Epoch: 43 |   [Val] | Loss: 0.0777 | [CCC]:  0.4144 [' 0.4144'] | PCC: 0.4184 ['0.4184'] | RMSE: 0.4142 ['0.4142']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 324 | Best [Val CCC]: 0.4358 [' 0.4358']| Loss: 0.0708 | PCC: 0.4387 ['0.4387'] | RMSE: 0.4152 ['0.4152']
On Test: CCC  0.5406 | PCC  0.5546 | RMSE  0.3809
****************************************************************************************************
Seed "324" over!
****************************************************************************************************
****************************************************************************************************
Using seed "325"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 325]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 83.9 | Training loss: 0.2497
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.9 | Training loss: 0.2453
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.0 | Training loss: 0.2369
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.2404
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2439
Epoch:  1 |   [Val] | Loss: 0.1873 | [CCC]:  0.1521 [' 0.1521'] | PCC: 0.1861 ['0.1861'] | RMSE: 0.5352 ['0.5352']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_325_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.4 | Training loss: 0.2140
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 82.8 | Training loss: 0.2319
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 82.3 | Training loss: 0.2153
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.2008
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2201
Epoch:  2 |   [Val] | Loss: 0.1571 | [CCC]:  0.2182 [' 0.2182'] | PCC: 0.2246 ['0.2246'] | RMSE: 0.5062 ['0.5062']
Epoch:  2 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_325_None_None].pth"!
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.4 | Training loss: 0.1927
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.1 | Training loss: 0.1940
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.5 | Training loss: 0.1805
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1868
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.1890
Epoch:  3 |   [Val] | Loss: 0.1454 | [CCC]:  0.2302 [' 0.2302'] | PCC: 0.2850 ['0.2850'] | RMSE: 0.5189 ['0.5189']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_325_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.0 | Training loss: 0.1682
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.5 | Training loss: 0.1870
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.7 | Training loss: 0.1678
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1279
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1736
Epoch:  4 |   [Val] | Loss: 0.1082 | [CCC]:  0.3134 [' 0.3134'] | PCC: 0.3181 ['0.3181'] | RMSE: 0.5250 ['0.5250']
Epoch:  4 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_325_None_None].pth"!
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.7 | Training loss: 0.1442
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.6 | Training loss: 0.1478
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.6 | Training loss: 0.1506
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1409
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1474
Epoch:  5 |   [Val] | Loss: 0.1444 | [CCC]:  0.1924 [' 0.1924'] | PCC: 0.2573 ['0.2573'] | RMSE: 0.4315 ['0.4315']
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.9 | Training loss: 0.1642
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.0 | Training loss: 0.1549
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.1 | Training loss: 0.1317
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1214
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1498
Epoch:  6 |   [Val] | Loss: 0.1813 | [CCC]:  0.1369 [' 0.1369'] | PCC: 0.2903 ['0.2903'] | RMSE: 0.8104 ['0.8104']
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.6 | Training loss: 0.1866
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.1935
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.6 | Training loss: 0.1570
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1331
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1783
Epoch:  7 |   [Val] | Loss: 0.0978 | [CCC]:  0.3244 [' 0.3244'] | PCC: 0.3580 ['0.3580'] | RMSE: 0.4312 ['0.4312']
Epoch:  7 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_325_None_None].pth"!
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.0 | Training loss: 0.1440
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.5 | Training loss: 0.1344
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.8 | Training loss: 0.1273
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1530
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1355
Epoch:  8 |   [Val] | Loss: 0.0872 | [CCC]:  0.3678 [' 0.3678'] | PCC: 0.3755 ['0.3755'] | RMSE: 0.5002 ['0.5002']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_325_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.4 | Training loss: 0.1262
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.5 | Training loss: 0.1231
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.7 | Training loss: 0.1247
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1143
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1245
Epoch:  9 |   [Val] | Loss: 0.0895 | [CCC]:  0.3607 [' 0.3607'] | PCC: 0.3624 ['0.3624'] | RMSE: 0.4431 ['0.4431']
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.9 | Training loss: 0.1277
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.6 | Training loss: 0.1258
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.2 | Training loss: 0.1073
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1318
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1205
Epoch: 10 |   [Val] | Loss: 0.0981 | [CCC]:  0.3390 [' 0.3390'] | PCC: 0.3905 ['0.3905'] | RMSE: 0.5033 ['0.5033']
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.0 | Training loss: 0.1269
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.5 | Training loss: 0.1087
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.8 | Training loss: 0.1241
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1183
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1198
Epoch: 11 |   [Val] | Loss: 0.0814 | [CCC]:  0.3981 [' 0.3981'] | PCC: 0.3996 ['0.3996'] | RMSE: 0.4312 ['0.4312']
Epoch: 11 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_325_None_None].pth"!
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.4 | Training loss: 0.1039
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.5 | Training loss: 0.1112
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.8 | Training loss: 0.1090
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1228
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1083
Epoch: 12 |   [Val] | Loss: 0.0793 | [CCC]:  0.3958 [' 0.3958'] | PCC: 0.3985 ['0.3985'] | RMSE: 0.4345 ['0.4345']
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.6 | Training loss: 0.1051
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.7 | Training loss: 0.1105
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.0 | Training loss: 0.0979
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0689
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1039
Epoch: 13 |   [Val] | Loss: 0.0808 | [CCC]:  0.4023 [' 0.4023'] | PCC: 0.4186 ['0.4186'] | RMSE: 0.4811 ['0.4811']
Epoch: 13 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_325_None_None].pth"!
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.7 | Training loss: 0.0900
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.3 | Training loss: 0.1137
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.6 | Training loss: 0.0992
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1123
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1011
Epoch: 14 |   [Val] | Loss: 0.0738 | [CCC]:  0.4128 [' 0.4128'] | PCC: 0.4192 ['0.4192'] | RMSE: 0.4103 ['0.4103']
Epoch: 14 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_325_None_None].pth"!
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.6 | Training loss: 0.0959
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.6 | Training loss: 0.0990
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.1 | Training loss: 0.0865
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1083
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.0940
Epoch: 15 |   [Val] | Loss: 0.0898 | [CCC]:  0.3606 [' 0.3606'] | PCC: 0.3868 ['0.3868'] | RMSE: 0.4559 ['0.4559']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.1 | Training loss: 0.0936
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.4 | Training loss: 0.0974
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.5 | Training loss: 0.0919
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1277
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.0948
Epoch: 16 |   [Val] | Loss: 0.0903 | [CCC]:  0.3492 [' 0.3492'] | PCC: 0.4039 ['0.4039'] | RMSE: 0.4873 ['0.4873']
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.3 | Training loss: 0.1020
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.8 | Training loss: 0.0987
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.6 | Training loss: 0.0863
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0593
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.0951
Epoch: 17 |   [Val] | Loss: 0.0842 | [CCC]:  0.3892 [' 0.3892'] | PCC: 0.4157 ['0.4157'] | RMSE: 0.3955 ['0.3955']
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.1 | Training loss: 0.0957
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.3 | Training loss: 0.0845
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.6 | Training loss: 0.0905
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0440
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.0895
Epoch: 18 |   [Val] | Loss: 0.0778 | [CCC]:  0.3883 [' 0.3883'] | PCC: 0.4009 ['0.4009'] | RMSE: 0.4880 ['0.4880']
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.1 | Training loss: 0.0898
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.3 | Training loss: 0.0798
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.5 | Training loss: 0.0833
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0343
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.0835
Epoch: 19 |   [Val] | Loss: 0.0795 | [CCC]:  0.3930 [' 0.3930'] | PCC: 0.4039 ['0.4039'] | RMSE: 0.4653 ['0.4653']
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.9 | Training loss: 0.0832
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.8 | Training loss: 0.0667
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.9 | Training loss: 0.0985
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0641
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0825
Epoch: 20 |   [Val] | Loss: 0.0852 | [CCC]:  0.3681 [' 0.3681'] | PCC: 0.3977 ['0.3977'] | RMSE: 0.4903 ['0.4903']
Epoch    20: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 21 | Batch:   1 | Lr: 0.00250 | Time used(s): 89.0 | Training loss: 0.0940
Epoch: 21 | Batch:   2 | Lr: 0.00250 | Time used(s): 86.4 | Training loss: 0.0799
Epoch: 21 | Batch:   3 | Lr: 0.00250 | Time used(s): 87.5 | Training loss: 0.0747
Epoch: 21 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0561
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0824
Epoch: 21 |   [Val] | Loss: 0.0848 | [CCC]:  0.3558 [' 0.3558'] | PCC: 0.3881 ['0.3881'] | RMSE: 0.4337 ['0.4337']
Epoch: 22 | Batch:   1 | Lr: 0.00250 | Time used(s): 88.5 | Training loss: 0.0889
Epoch: 22 | Batch:   2 | Lr: 0.00250 | Time used(s): 86.2 | Training loss: 0.0814
Epoch: 22 | Batch:   3 | Lr: 0.00250 | Time used(s): 87.3 | Training loss: 0.0735
Epoch: 22 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0933
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0815
Epoch: 22 |   [Val] | Loss: 0.0776 | [CCC]:  0.4089 [' 0.4089'] | PCC: 0.4100 ['0.4100'] | RMSE: 0.4595 ['0.4595']
Epoch: 23 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.9 | Training loss: 0.0725
Epoch: 23 | Batch:   2 | Lr: 0.00250 | Time used(s): 86.1 | Training loss: 0.0711
Epoch: 23 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.9 | Training loss: 0.0780
Epoch: 23 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0701
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0738
Epoch: 23 |   [Val] | Loss: 0.0694 | [CCC]:  0.4075 [' 0.4075'] | PCC: 0.4094 ['0.4094'] | RMSE: 0.4257 ['0.4257']
Epoch: 24 | Batch:   1 | Lr: 0.00250 | Time used(s): 87.0 | Training loss: 0.0616
Epoch: 24 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.5 | Training loss: 0.0640
Epoch: 24 | Batch:   3 | Lr: 0.00250 | Time used(s): 87.7 | Training loss: 0.0731
Epoch: 24 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0909
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0667
Epoch: 24 |   [Val] | Loss: 0.0732 | [CCC]:  0.4051 [' 0.4051'] | PCC: 0.4096 ['0.4096'] | RMSE: 0.4190 ['0.4190']
Epoch: 25 | Batch:   1 | Lr: 0.00250 | Time used(s): 88.8 | Training loss: 0.0618
Epoch: 25 | Batch:   2 | Lr: 0.00250 | Time used(s): 87.1 | Training loss: 0.0570
Epoch: 25 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.6 | Training loss: 0.0662
Epoch: 25 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0492
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0615
Epoch: 25 |   [Val] | Loss: 0.0744 | [CCC]:  0.3977 [' 0.3977'] | PCC: 0.4038 ['0.4038'] | RMSE: 0.4680 ['0.4680']
Epoch: 26 | Batch:   1 | Lr: 0.00250 | Time used(s): 88.2 | Training loss: 0.0615
Epoch: 26 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.8 | Training loss: 0.0589
Epoch: 26 | Batch:   3 | Lr: 0.00250 | Time used(s): 87.4 | Training loss: 0.0599
Epoch: 26 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0667
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0602
Epoch: 26 |   [Val] | Loss: 0.0747 | [CCC]:  0.3865 [' 0.3865'] | PCC: 0.3931 ['0.3931'] | RMSE: 0.4306 ['0.4306']
Epoch    26: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 27 | Batch:   1 | Lr: 0.00125 | Time used(s): 88.7 | Training loss: 0.0579
Epoch: 27 | Batch:   2 | Lr: 0.00125 | Time used(s): 86.4 | Training loss: 0.0589
Epoch: 27 | Batch:   3 | Lr: 0.00125 | Time used(s): 86.8 | Training loss: 0.0542
Epoch: 27 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0596
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0571
Epoch: 27 |   [Val] | Loss: 0.0774 | [CCC]:  0.4127 [' 0.4127'] | PCC: 0.4169 ['0.4169'] | RMSE: 0.4497 ['0.4497']
Epoch: 28 | Batch:   1 | Lr: 0.00125 | Time used(s): 87.4 | Training loss: 0.0485
Epoch: 28 | Batch:   2 | Lr: 0.00125 | Time used(s): 87.8 | Training loss: 0.0548
Epoch: 28 | Batch:   3 | Lr: 0.00125 | Time used(s): 86.5 | Training loss: 0.0562
Epoch: 28 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0186
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0526
Epoch: 28 |   [Val] | Loss: 0.0746 | [CCC]:  0.4181 [' 0.4181'] | PCC: 0.4204 ['0.4204'] | RMSE: 0.4297 ['0.4297']
Epoch: 28 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_325_None_None].pth"!
Epoch: 29 | Batch:   1 | Lr: 0.00125 | Time used(s): 88.7 | Training loss: 0.0543
Epoch: 29 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.9 | Training loss: 0.0471
Epoch: 29 | Batch:   3 | Lr: 0.00125 | Time used(s): 85.3 | Training loss: 0.0505
Epoch: 29 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0414
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0505
Epoch: 29 |   [Val] | Loss: 0.0738 | [CCC]:  0.4130 [' 0.4130'] | PCC: 0.4175 ['0.4175'] | RMSE: 0.4229 ['0.4229']
Epoch: 30 | Batch:   1 | Lr: 0.00125 | Time used(s): 87.7 | Training loss: 0.0481
Epoch: 30 | Batch:   2 | Lr: 0.00125 | Time used(s): 84.8 | Training loss: 0.0476
Epoch: 30 | Batch:   3 | Lr: 0.00125 | Time used(s): 86.6 | Training loss: 0.0443
Epoch: 30 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0543
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0468
Epoch: 30 |   [Val] | Loss: 0.0753 | [CCC]:  0.4102 [' 0.4102'] | PCC: 0.4115 ['0.4115'] | RMSE: 0.4438 ['0.4438']
Epoch: 31 | Batch:   1 | Lr: 0.00125 | Time used(s): 86.8 | Training loss: 0.0500
Epoch: 31 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.7 | Training loss: 0.0435
Epoch: 31 | Batch:   3 | Lr: 0.00125 | Time used(s): 86.2 | Training loss: 0.0397
Epoch: 31 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0298
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0442
Epoch: 31 |   [Val] | Loss: 0.0752 | [CCC]:  0.3980 [' 0.3980'] | PCC: 0.3998 ['0.3998'] | RMSE: 0.4354 ['0.4354']
Epoch: 32 | Batch:   1 | Lr: 0.00125 | Time used(s): 86.9 | Training loss: 0.0360
Epoch: 32 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.1 | Training loss: 0.0404
Epoch: 32 | Batch:   3 | Lr: 0.00125 | Time used(s): 86.6 | Training loss: 0.0484
Epoch: 32 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0330
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0415
Epoch: 32 |   [Val] | Loss: 0.0774 | [CCC]:  0.4044 [' 0.4044'] | PCC: 0.4057 ['0.4057'] | RMSE: 0.4417 ['0.4417']
Epoch: 33 | Batch:   1 | Lr: 0.00125 | Time used(s): 87.0 | Training loss: 0.0400
Epoch: 33 | Batch:   2 | Lr: 0.00125 | Time used(s): 84.9 | Training loss: 0.0455
Epoch: 33 | Batch:   3 | Lr: 0.00125 | Time used(s): 87.6 | Training loss: 0.0303
Epoch: 33 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0513
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0388
Epoch: 33 |   [Val] | Loss: 0.0810 | [CCC]:  0.4073 [' 0.4073'] | PCC: 0.4127 ['0.4127'] | RMSE: 0.4499 ['0.4499']
Epoch: 34 | Batch:   1 | Lr: 0.00125 | Time used(s): 87.7 | Training loss: 0.0427
Epoch: 34 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.7 | Training loss: 0.0362
Epoch: 34 | Batch:   3 | Lr: 0.00125 | Time used(s): 87.6 | Training loss: 0.0435
Epoch: 34 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0299
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0406
Epoch: 34 |   [Val] | Loss: 0.0778 | [CCC]:  0.4056 [' 0.4056'] | PCC: 0.4079 ['0.4079'] | RMSE: 0.4329 ['0.4329']
Epoch    34: reducing learning rate of group 0 to 6.2500e-04.
Epoch: 35 | Batch:   1 | Lr: 0.00063 | Time used(s): 87.2 | Training loss: 0.0397
Epoch: 35 | Batch:   2 | Lr: 0.00063 | Time used(s): 85.4 | Training loss: 0.0351
Epoch: 35 | Batch:   3 | Lr: 0.00063 | Time used(s): 86.8 | Training loss: 0.0398
Epoch: 35 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.1 | Training loss: 0.0314
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0381
Epoch: 35 |   [Val] | Loss: 0.0786 | [CCC]:  0.4037 [' 0.4037'] | PCC: 0.4076 ['0.4076'] | RMSE: 0.4228 ['0.4228']
Epoch: 36 | Batch:   1 | Lr: 0.00063 | Time used(s): 87.1 | Training loss: 0.0355
Epoch: 36 | Batch:   2 | Lr: 0.00063 | Time used(s): 86.3 | Training loss: 0.0389
Epoch: 36 | Batch:   3 | Lr: 0.00063 | Time used(s): 87.5 | Training loss: 0.0304
Epoch: 36 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.1 | Training loss: 0.0352
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0349
Epoch: 36 |   [Val] | Loss: 0.0790 | [CCC]:  0.4037 [' 0.4037'] | PCC: 0.4054 ['0.4054'] | RMSE: 0.4520 ['0.4520']
Epoch: 37 | Batch:   1 | Lr: 0.00063 | Time used(s): 87.3 | Training loss: 0.0363
Epoch: 37 | Batch:   2 | Lr: 0.00063 | Time used(s): 86.5 | Training loss: 0.0290
Epoch: 37 | Batch:   3 | Lr: 0.00063 | Time used(s): 86.5 | Training loss: 0.0345
Epoch: 37 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.1 | Training loss: 0.0333
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0333
Epoch: 37 |   [Val] | Loss: 0.0773 | [CCC]:  0.4004 [' 0.4004'] | PCC: 0.4034 ['0.4034'] | RMSE: 0.4400 ['0.4400']
Epoch: 38 | Batch:   1 | Lr: 0.00063 | Time used(s): 87.8 | Training loss: 0.0372
Epoch: 38 | Batch:   2 | Lr: 0.00063 | Time used(s): 85.3 | Training loss: 0.0308
Epoch: 38 | Batch:   3 | Lr: 0.00063 | Time used(s): 87.2 | Training loss: 0.0254
Epoch: 38 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.1 | Training loss: 0.0069
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0307
Epoch: 38 |   [Val] | Loss: 0.0775 | [CCC]:  0.4048 [' 0.4048'] | PCC: 0.4053 ['0.4053'] | RMSE: 0.4369 ['0.4369']
Epoch: 39 | Batch:   1 | Lr: 0.00063 | Time used(s): 87.3 | Training loss: 0.0262
Epoch: 39 | Batch:   2 | Lr: 0.00063 | Time used(s): 85.6 | Training loss: 0.0242
Epoch: 39 | Batch:   3 | Lr: 0.00063 | Time used(s): 87.6 | Training loss: 0.0333
Epoch: 39 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.0 | Training loss: 0.0378
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0281
Epoch: 39 |   [Val] | Loss: 0.0800 | [CCC]:  0.4083 [' 0.4083'] | PCC: 0.4083 ['0.4083'] | RMSE: 0.4423 ['0.4423']
Epoch: 40 | Batch:   1 | Lr: 0.00063 | Time used(s): 88.2 | Training loss: 0.0304
Epoch: 40 | Batch:   2 | Lr: 0.00063 | Time used(s): 85.5 | Training loss: 0.0228
Epoch: 40 | Batch:   3 | Lr: 0.00063 | Time used(s): 86.9 | Training loss: 0.0279
Epoch: 40 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.0 | Training loss: 0.0341
--------------------------------------------------
Epoch: 40 | [Train] | Loss: 0.0272
Epoch: 40 |   [Val] | Loss: 0.0790 | [CCC]:  0.4018 [' 0.4018'] | PCC: 0.4048 ['0.4048'] | RMSE: 0.4258 ['0.4258']
Epoch    40: reducing learning rate of group 0 to 3.1250e-04.
Epoch: 41 | Batch:   1 | Lr: 0.00031 | Time used(s): 87.7 | Training loss: 0.0263
Epoch: 41 | Batch:   2 | Lr: 0.00031 | Time used(s): 86.9 | Training loss: 0.0281
Epoch: 41 | Batch:   3 | Lr: 0.00031 | Time used(s): 86.7 | Training loss: 0.0242
Epoch: 41 | Batch:   4 | Lr: 0.00031 | Time used(s): 2.0 | Training loss: 0.0336
--------------------------------------------------
Epoch: 41 | [Train] | Loss: 0.0263
Epoch: 41 |   [Val] | Loss: 0.0791 | [CCC]:  0.4030 [' 0.4030'] | PCC: 0.4045 ['0.4045'] | RMSE: 0.4324 ['0.4324']
Epoch: 42 | Batch:   1 | Lr: 0.00031 | Time used(s): 88.9 | Training loss: 0.0280
Epoch: 42 | Batch:   2 | Lr: 0.00031 | Time used(s): 89.6 | Training loss: 0.0262
Epoch: 42 | Batch:   3 | Lr: 0.00031 | Time used(s): 90.2 | Training loss: 0.0201
Epoch: 42 | Batch:   4 | Lr: 0.00031 | Time used(s): 2.2 | Training loss: 0.0286
--------------------------------------------------
Epoch: 42 | [Train] | Loss: 0.0248
Epoch: 42 |   [Val] | Loss: 0.0796 | [CCC]:  0.4044 [' 0.4044'] | PCC: 0.4047 ['0.4047'] | RMSE: 0.4476 ['0.4476']
Epoch: 43 | Batch:   1 | Lr: 0.00031 | Time used(s): 97.3 | Training loss: 0.0210
Epoch: 43 | Batch:   2 | Lr: 0.00031 | Time used(s): 87.2 | Training loss: 0.0255
Epoch: 43 | Batch:   3 | Lr: 0.00031 | Time used(s): 86.0 | Training loss: 0.0258
Epoch: 43 | Batch:   4 | Lr: 0.00031 | Time used(s): 2.1 | Training loss: 0.0483
--------------------------------------------------
Epoch: 43 | [Train] | Loss: 0.0245
Epoch: 43 |   [Val] | Loss: 0.0792 | [CCC]:  0.3998 [' 0.3998'] | PCC: 0.4014 ['0.4014'] | RMSE: 0.4494 ['0.4494']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 325 | Best [Val CCC]: 0.4181 [' 0.4181']| Loss: 0.0746 | PCC: 0.4204 ['0.4204'] | RMSE: 0.4297 ['0.4297']
On Test: CCC  0.5739 | PCC  0.5759 | RMSE  0.3948
****************************************************************************************************
Seed "325" over!
****************************************************************************************************
****************************************************************************************************
Using seed "326"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 326]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.4 | Training loss: 0.2488
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.9 | Training loss: 0.2406
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.1 | Training loss: 0.2284
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.2291
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2391
Epoch:  1 |   [Val] | Loss: 0.2433 | [CCC]:  0.0144 [' 0.0144'] | PCC: 0.1377 ['0.1377'] | RMSE: 1.1183 ['1.1183']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.4 | Training loss: 0.2447
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.3 | Training loss: 0.2326
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.7 | Training loss: 0.2252
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.2230
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2340
Epoch:  2 |   [Val] | Loss: 0.2043 | [CCC]:  0.0797 [' 0.0797'] | PCC: 0.1541 ['0.1541'] | RMSE: 0.7531 ['0.7531']
Epoch:  2 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.9 | Training loss: 0.2124
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.0 | Training loss: 0.2011
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 83.2 | Training loss: 0.1955
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1824
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.2027
Epoch:  3 |   [Val] | Loss: 0.1479 | [CCC]:  0.2121 [' 0.2121'] | PCC: 0.2592 ['0.2592'] | RMSE: 0.4984 ['0.4984']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.2 | Training loss: 0.1765
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.7 | Training loss: 0.1638
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.4 | Training loss: 0.1646
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1628
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1682
Epoch:  4 |   [Val] | Loss: 0.2254 | [CCC]:  0.0559 [' 0.0559'] | PCC: 0.2164 ['0.2164'] | RMSE: 0.7464 ['0.7464']
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.6 | Training loss: 0.2238
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.1 | Training loss: 0.2178
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.0 | Training loss: 0.2074
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1736
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.2157
Epoch:  5 |   [Val] | Loss: 0.1780 | [CCC]:  0.1174 [' 0.1174'] | PCC: 0.2156 ['0.2156'] | RMSE: 0.6467 ['0.6467']
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.5 | Training loss: 0.1832
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.9 | Training loss: 0.1852
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.5 | Training loss: 0.1792
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1818
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1825
Epoch:  6 |   [Val] | Loss: 0.1213 | [CCC]:  0.2612 [' 0.2612'] | PCC: 0.2694 ['0.2694'] | RMSE: 0.4487 ['0.4487']
Epoch:  6 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.9 | Training loss: 0.1669
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.3 | Training loss: 0.1739
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.9 | Training loss: 0.1561
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1540
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1654
Epoch:  7 |   [Val] | Loss: 0.1026 | [CCC]:  0.2927 [' 0.2927'] | PCC: 0.2933 ['0.2933'] | RMSE: 0.4736 ['0.4736']
Epoch:  7 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.7 | Training loss: 0.1501
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.1 | Training loss: 0.1593
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.4 | Training loss: 0.1500
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1849
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1536
Epoch:  8 |   [Val] | Loss: 0.0968 | [CCC]:  0.3030 [' 0.3030'] | PCC: 0.3154 ['0.3154'] | RMSE: 0.4847 ['0.4847']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.4 | Training loss: 0.1490
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.1587
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.2 | Training loss: 0.1339
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1426
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1471
Epoch:  9 |   [Val] | Loss: 0.0915 | [CCC]:  0.3517 [' 0.3517'] | PCC: 0.3594 ['0.3594'] | RMSE: 0.4989 ['0.4989']
Epoch:  9 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.5 | Training loss: 0.1358
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.2 | Training loss: 0.1580
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.6 | Training loss: 0.1361
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1554
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1435
Epoch: 10 |   [Val] | Loss: 0.1339 | [CCC]:  0.2275 [' 0.2275'] | PCC: 0.3504 ['0.3504'] | RMSE: 0.6598 ['0.6598']
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.1 | Training loss: 0.1613
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.9 | Training loss: 0.1535
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.2 | Training loss: 0.1407
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1836
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1523
Epoch: 11 |   [Val] | Loss: 0.1403 | [CCC]:  0.2205 [' 0.2205'] | PCC: 0.3550 ['0.3550'] | RMSE: 0.5347 ['0.5347']
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.9 | Training loss: 0.1670
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.1 | Training loss: 0.1359
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.9 | Training loss: 0.1604
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1763
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1548
Epoch: 12 |   [Val] | Loss: 0.1056 | [CCC]:  0.3011 [' 0.3011'] | PCC: 0.3385 ['0.3385'] | RMSE: 0.5203 ['0.5203']
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 94.1 | Training loss: 0.1343
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.9 | Training loss: 0.1349
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.1 | Training loss: 0.1459
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0890
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1376
Epoch: 13 |   [Val] | Loss: 0.1050 | [CCC]:  0.2903 [' 0.2903'] | PCC: 0.3559 ['0.3559'] | RMSE: 0.5417 ['0.5417']
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.7 | Training loss: 0.1367
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.3 | Training loss: 0.1233
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.9 | Training loss: 0.1250
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1420
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1286
Epoch: 14 |   [Val] | Loss: 0.0897 | [CCC]:  0.3393 [' 0.3393'] | PCC: 0.3570 ['0.3570'] | RMSE: 0.5020 ['0.5020']
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.2 | Training loss: 0.1189
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.3 | Training loss: 0.1314
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.6 | Training loss: 0.1142
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.1669
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1222
Epoch: 15 |   [Val] | Loss: 0.0894 | [CCC]:  0.3531 [' 0.3531'] | PCC: 0.3645 ['0.3645'] | RMSE: 0.4270 ['0.4270']
Epoch: 15 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.0 | Training loss: 0.1241
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.3 | Training loss: 0.1182
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.5 | Training loss: 0.1138
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1146
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.1186
Epoch: 16 |   [Val] | Loss: 0.0836 | [CCC]:  0.3934 [' 0.3934'] | PCC: 0.3975 ['0.3975'] | RMSE: 0.4736 ['0.4736']
Epoch: 16 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.7 | Training loss: 0.1075
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.1 | Training loss: 0.1196
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.0 | Training loss: 0.1163
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1372
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.1148
Epoch: 17 |   [Val] | Loss: 0.0787 | [CCC]:  0.4019 [' 0.4019'] | PCC: 0.4102 ['0.4102'] | RMSE: 0.4588 ['0.4588']
Epoch: 17 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.5 | Training loss: 0.1094
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.3 | Training loss: 0.1064
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.3 | Training loss: 0.1081
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1017
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.1079
Epoch: 18 |   [Val] | Loss: 0.0777 | [CCC]:  0.3905 [' 0.3905'] | PCC: 0.3970 ['0.3970'] | RMSE: 0.4453 ['0.4453']
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.2 | Training loss: 0.1071
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.7 | Training loss: 0.1065
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.8 | Training loss: 0.1025
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.0908
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.1052
Epoch: 19 |   [Val] | Loss: 0.1005 | [CCC]:  0.3142 [' 0.3142'] | PCC: 0.3720 ['0.3720'] | RMSE: 0.5455 ['0.5455']
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.5 | Training loss: 0.1141
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.4 | Training loss: 0.1053
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 82.9 | Training loss: 0.1141
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.1209
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.1113
Epoch: 20 |   [Val] | Loss: 0.0782 | [CCC]:  0.4039 [' 0.4039'] | PCC: 0.4104 ['0.4104'] | RMSE: 0.4387 ['0.4387']
Epoch: 20 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch: 21 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.0 | Training loss: 0.0911
Epoch: 21 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.4 | Training loss: 0.1093
Epoch: 21 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.5 | Training loss: 0.1053
Epoch: 21 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.0805
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.1015
Epoch: 21 |   [Val] | Loss: 0.0817 | [CCC]:  0.4096 [' 0.4096'] | PCC: 0.4136 ['0.4136'] | RMSE: 0.4511 ['0.4511']
Epoch: 21 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch: 22 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.8 | Training loss: 0.0967
Epoch: 22 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.9 | Training loss: 0.0961
Epoch: 22 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.0 | Training loss: 0.0991
Epoch: 22 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0840
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0971
Epoch: 22 |   [Val] | Loss: 0.0843 | [CCC]:  0.3815 [' 0.3815'] | PCC: 0.4124 ['0.4124'] | RMSE: 0.4491 ['0.4491']
Epoch: 23 | Batch:   1 | Lr: 0.00500 | Time used(s): 84.8 | Training loss: 0.1014
Epoch: 23 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.0 | Training loss: 0.1000
Epoch: 23 | Batch:   3 | Lr: 0.00500 | Time used(s): 83.8 | Training loss: 0.0928
Epoch: 23 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.0857
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0979
Epoch: 23 |   [Val] | Loss: 0.0816 | [CCC]:  0.3982 [' 0.3982'] | PCC: 0.4169 ['0.4169'] | RMSE: 0.4317 ['0.4317']
Epoch: 24 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.5 | Training loss: 0.0903
Epoch: 24 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.3 | Training loss: 0.0962
Epoch: 24 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.0 | Training loss: 0.0926
Epoch: 24 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0746
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0928
Epoch: 24 |   [Val] | Loss: 0.0738 | [CCC]:  0.4173 [' 0.4173'] | PCC: 0.4186 ['0.4186'] | RMSE: 0.4398 ['0.4398']
Epoch: 24 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch: 25 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.8 | Training loss: 0.0876
Epoch: 25 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.4 | Training loss: 0.0930
Epoch: 25 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.1 | Training loss: 0.0807
Epoch: 25 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1359
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0879
Epoch: 25 |   [Val] | Loss: 0.0769 | [CCC]:  0.4025 [' 0.4025'] | PCC: 0.4078 ['0.4078'] | RMSE: 0.4343 ['0.4343']
Epoch: 26 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.0 | Training loss: 0.0868
Epoch: 26 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.8 | Training loss: 0.0840
Epoch: 26 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.8 | Training loss: 0.0914
Epoch: 26 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.0587
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0869
Epoch: 26 |   [Val] | Loss: 0.0825 | [CCC]:  0.4031 [' 0.4031'] | PCC: 0.4119 ['0.4119'] | RMSE: 0.4684 ['0.4684']
Epoch: 27 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.3 | Training loss: 0.0861
Epoch: 27 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.2 | Training loss: 0.0772
Epoch: 27 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.1 | Training loss: 0.0836
Epoch: 27 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0959
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0825
Epoch: 27 |   [Val] | Loss: 0.0764 | [CCC]:  0.4035 [' 0.4035'] | PCC: 0.4080 ['0.4080'] | RMSE: 0.4366 ['0.4366']
Epoch: 28 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.8 | Training loss: 0.0799
Epoch: 28 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.9 | Training loss: 0.0775
Epoch: 28 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.3 | Training loss: 0.0876
Epoch: 28 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0735
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0816
Epoch: 28 |   [Val] | Loss: 0.1039 | [CCC]:  0.3071 [' 0.3071'] | PCC: 0.3972 ['0.3972'] | RMSE: 0.5462 ['0.5462']
Epoch: 29 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.5 | Training loss: 0.1222
Epoch: 29 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.5 | Training loss: 0.0988
Epoch: 29 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.3 | Training loss: 0.0799
Epoch: 29 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1243
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.1007
Epoch: 29 |   [Val] | Loss: 0.0803 | [CCC]:  0.3979 [' 0.3979'] | PCC: 0.4075 ['0.4075'] | RMSE: 0.4063 ['0.4063']
Epoch: 30 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.0 | Training loss: 0.0859
Epoch: 30 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.4 | Training loss: 0.1004
Epoch: 30 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.8 | Training loss: 0.0780
Epoch: 30 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0645
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0877
Epoch: 30 |   [Val] | Loss: 0.0815 | [CCC]:  0.4066 [' 0.4066'] | PCC: 0.4158 ['0.4158'] | RMSE: 0.4859 ['0.4859']
Epoch    30: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 31 | Batch:   1 | Lr: 0.00250 | Time used(s): 87.5 | Training loss: 0.0848
Epoch: 31 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.1 | Training loss: 0.0770
Epoch: 31 | Batch:   3 | Lr: 0.00250 | Time used(s): 84.0 | Training loss: 0.0761
Epoch: 31 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0882
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0794
Epoch: 31 |   [Val] | Loss: 0.0742 | [CCC]:  0.4091 [' 0.4091'] | PCC: 0.4164 ['0.4164'] | RMSE: 0.4333 ['0.4333']
Epoch: 32 | Batch:   1 | Lr: 0.00250 | Time used(s): 91.5 | Training loss: 0.0759
Epoch: 32 | Batch:   2 | Lr: 0.00250 | Time used(s): 83.7 | Training loss: 0.0750
Epoch: 32 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.1 | Training loss: 0.0720
Epoch: 32 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0703
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0742
Epoch: 32 |   [Val] | Loss: 0.0762 | [CCC]:  0.4130 [' 0.4130'] | PCC: 0.4256 ['0.4256'] | RMSE: 0.4511 ['0.4511']
Epoch: 33 | Batch:   1 | Lr: 0.00250 | Time used(s): 88.3 | Training loss: 0.0812
Epoch: 33 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.0 | Training loss: 0.0672
Epoch: 33 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.2 | Training loss: 0.0661
Epoch: 33 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0602
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0713
Epoch: 33 |   [Val] | Loss: 0.0723 | [CCC]:  0.4249 [' 0.4249'] | PCC: 0.4261 ['0.4261'] | RMSE: 0.4507 ['0.4507']
Epoch: 33 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch: 34 | Batch:   1 | Lr: 0.00250 | Time used(s): 85.9 | Training loss: 0.0643
Epoch: 34 | Batch:   2 | Lr: 0.00250 | Time used(s): 82.4 | Training loss: 0.0721
Epoch: 34 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.3 | Training loss: 0.0668
Epoch: 34 | Batch:   4 | Lr: 0.00250 | Time used(s): 1.9 | Training loss: 0.1165
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0685
Epoch: 34 |   [Val] | Loss: 0.0703 | [CCC]:  0.4084 [' 0.4084'] | PCC: 0.4224 ['0.4224'] | RMSE: 0.4147 ['0.4147']
Epoch: 35 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.1 | Training loss: 0.0692
Epoch: 35 | Batch:   2 | Lr: 0.00250 | Time used(s): 82.9 | Training loss: 0.0753
Epoch: 35 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.2 | Training loss: 0.0676
Epoch: 35 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0814
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0709
Epoch: 35 |   [Val] | Loss: 0.0766 | [CCC]:  0.3995 [' 0.3995'] | PCC: 0.4088 ['0.4088'] | RMSE: 0.4559 ['0.4559']
Epoch: 36 | Batch:   1 | Lr: 0.00250 | Time used(s): 85.5 | Training loss: 0.0725
Epoch: 36 | Batch:   2 | Lr: 0.00250 | Time used(s): 83.5 | Training loss: 0.0636
Epoch: 36 | Batch:   3 | Lr: 0.00250 | Time used(s): 83.7 | Training loss: 0.0636
Epoch: 36 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0333
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0660
Epoch: 36 |   [Val] | Loss: 0.0746 | [CCC]:  0.4195 [' 0.4195'] | PCC: 0.4196 ['0.4196'] | RMSE: 0.4392 ['0.4392']
Epoch: 37 | Batch:   1 | Lr: 0.00250 | Time used(s): 85.6 | Training loss: 0.0571
Epoch: 37 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.4 | Training loss: 0.0606
Epoch: 37 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.4 | Training loss: 0.0665
Epoch: 37 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0028
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0605
Epoch: 37 |   [Val] | Loss: 0.0754 | [CCC]:  0.4274 [' 0.4274'] | PCC: 0.4402 ['0.4402'] | RMSE: 0.4136 ['0.4136']
Epoch: 37 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch: 38 | Batch:   1 | Lr: 0.00250 | Time used(s): 85.3 | Training loss: 0.0641
Epoch: 38 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.1 | Training loss: 0.0573
Epoch: 38 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.8 | Training loss: 0.0613
Epoch: 38 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0628
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0609
Epoch: 38 |   [Val] | Loss: 0.0751 | [CCC]:  0.4130 [' 0.4130'] | PCC: 0.4156 ['0.4156'] | RMSE: 0.4615 ['0.4615']
Epoch: 39 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.4 | Training loss: 0.0557
Epoch: 39 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.3 | Training loss: 0.0573
Epoch: 39 | Batch:   3 | Lr: 0.00250 | Time used(s): 84.7 | Training loss: 0.0624
Epoch: 39 | Batch:   4 | Lr: 0.00250 | Time used(s): 1.9 | Training loss: 0.0480
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0583
Epoch: 39 |   [Val] | Loss: 0.0769 | [CCC]:  0.3968 [' 0.3968'] | PCC: 0.3970 ['0.3970'] | RMSE: 0.4436 ['0.4436']
Epoch: 40 | Batch:   1 | Lr: 0.00250 | Time used(s): 87.7 | Training loss: 0.0540
Epoch: 40 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.5 | Training loss: 0.0501
Epoch: 40 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.4 | Training loss: 0.0567
Epoch: 40 | Batch:   4 | Lr: 0.00250 | Time used(s): 1.9 | Training loss: 0.0585
--------------------------------------------------
Epoch: 40 | [Train] | Loss: 0.0537
Epoch: 40 |   [Val] | Loss: 0.0748 | [CCC]:  0.4067 [' 0.4067'] | PCC: 0.4086 ['0.4086'] | RMSE: 0.4276 ['0.4276']
Epoch: 41 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.7 | Training loss: 0.0557
Epoch: 41 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.5 | Training loss: 0.0472
Epoch: 41 | Batch:   3 | Lr: 0.00250 | Time used(s): 84.6 | Training loss: 0.0516
Epoch: 41 | Batch:   4 | Lr: 0.00250 | Time used(s): 1.9 | Training loss: 0.0704
--------------------------------------------------
Epoch: 41 | [Train] | Loss: 0.0518
Epoch: 41 |   [Val] | Loss: 0.0747 | [CCC]:  0.4077 [' 0.4077'] | PCC: 0.4089 ['0.4089'] | RMSE: 0.4569 ['0.4569']
Epoch: 42 | Batch:   1 | Lr: 0.00250 | Time used(s): 85.9 | Training loss: 0.0500
Epoch: 42 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.3 | Training loss: 0.0571
Epoch: 42 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.5 | Training loss: 0.0471
Epoch: 42 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0655
--------------------------------------------------
Epoch: 42 | [Train] | Loss: 0.0516
Epoch: 42 |   [Val] | Loss: 0.0772 | [CCC]:  0.4033 [' 0.4033'] | PCC: 0.4050 ['0.4050'] | RMSE: 0.4276 ['0.4276']
Epoch: 43 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.2 | Training loss: 0.0462
Epoch: 43 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.4 | Training loss: 0.0485
Epoch: 43 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.1 | Training loss: 0.0528
Epoch: 43 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0047
--------------------------------------------------
Epoch: 43 | [Train] | Loss: 0.0485
Epoch: 43 |   [Val] | Loss: 0.0751 | [CCC]:  0.4307 [' 0.4307'] | PCC: 0.4311 ['0.4311'] | RMSE: 0.4421 ['0.4421']
Epoch: 43 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_326_None_None].pth"!
Epoch: 44 | Batch:   1 | Lr: 0.00250 | Time used(s): 85.9 | Training loss: 0.0492
Epoch: 44 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.7 | Training loss: 0.0551
Epoch: 44 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.4 | Training loss: 0.0481
Epoch: 44 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0751
--------------------------------------------------
Epoch: 44 | [Train] | Loss: 0.0512
Epoch: 44 |   [Val] | Loss: 0.0744 | [CCC]:  0.4169 [' 0.4169'] | PCC: 0.4235 ['0.4235'] | RMSE: 0.4192 ['0.4192']
Epoch: 45 | Batch:   1 | Lr: 0.00250 | Time used(s): 85.9 | Training loss: 0.0531
Epoch: 45 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.0 | Training loss: 0.0453
Epoch: 45 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.4 | Training loss: 0.0502
Epoch: 45 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0625
--------------------------------------------------
Epoch: 45 | [Train] | Loss: 0.0497
Epoch: 45 |   [Val] | Loss: 0.0738 | [CCC]:  0.4070 [' 0.4070'] | PCC: 0.4075 ['0.4075'] | RMSE: 0.4543 ['0.4543']
Epoch: 46 | Batch:   1 | Lr: 0.00250 | Time used(s): 87.4 | Training loss: 0.0429
Epoch: 46 | Batch:   2 | Lr: 0.00250 | Time used(s): 85.7 | Training loss: 0.0367
Epoch: 46 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.7 | Training loss: 0.0551
Epoch: 46 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0414
--------------------------------------------------
Epoch: 46 | [Train] | Loss: 0.0448
Epoch: 46 |   [Val] | Loss: 0.0756 | [CCC]:  0.4071 [' 0.4071'] | PCC: 0.4133 ['0.4133'] | RMSE: 0.4162 ['0.4162']
Epoch: 47 | Batch:   1 | Lr: 0.00250 | Time used(s): 87.6 | Training loss: 0.0455
Epoch: 47 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.3 | Training loss: 0.0406
Epoch: 47 | Batch:   3 | Lr: 0.00250 | Time used(s): 85.3 | Training loss: 0.0410
Epoch: 47 | Batch:   4 | Lr: 0.00250 | Time used(s): 1.9 | Training loss: 0.0578
--------------------------------------------------
Epoch: 47 | [Train] | Loss: 0.0426
Epoch: 47 |   [Val] | Loss: 0.0767 | [CCC]:  0.4030 [' 0.4030'] | PCC: 0.4118 ['0.4118'] | RMSE: 0.4791 ['0.4791']
Epoch: 48 | Batch:   1 | Lr: 0.00250 | Time used(s): 85.6 | Training loss: 0.0458
Epoch: 48 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.4 | Training loss: 0.0390
Epoch: 48 | Batch:   3 | Lr: 0.00250 | Time used(s): 86.4 | Training loss: 0.0421
Epoch: 48 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0394
--------------------------------------------------
Epoch: 48 | [Train] | Loss: 0.0423
Epoch: 48 |   [Val] | Loss: 0.0774 | [CCC]:  0.4068 [' 0.4068'] | PCC: 0.4169 ['0.4169'] | RMSE: 0.4333 ['0.4333']
Epoch: 49 | Batch:   1 | Lr: 0.00250 | Time used(s): 86.7 | Training loss: 0.0534
Epoch: 49 | Batch:   2 | Lr: 0.00250 | Time used(s): 84.8 | Training loss: 0.0369
Epoch: 49 | Batch:   3 | Lr: 0.00250 | Time used(s): 84.9 | Training loss: 0.0348
Epoch: 49 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.0 | Training loss: 0.0302
--------------------------------------------------
Epoch: 49 | [Train] | Loss: 0.0415
Epoch: 49 |   [Val] | Loss: 0.0794 | [CCC]:  0.4014 [' 0.4014'] | PCC: 0.4046 ['0.4046'] | RMSE: 0.4537 ['0.4537']
Epoch    49: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 50 | Batch:   1 | Lr: 0.00125 | Time used(s): 86.5 | Training loss: 0.0388
Epoch: 50 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.9 | Training loss: 0.0376
Epoch: 50 | Batch:   3 | Lr: 0.00125 | Time used(s): 84.9 | Training loss: 0.0348
Epoch: 50 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0287
--------------------------------------------------
Epoch: 50 | [Train] | Loss: 0.0369
Epoch: 50 |   [Val] | Loss: 0.0794 | [CCC]:  0.4008 [' 0.4008'] | PCC: 0.4016 ['0.4016'] | RMSE: 0.4348 ['0.4348']
Epoch: 51 | Batch:   1 | Lr: 0.00125 | Time used(s): 86.9 | Training loss: 0.0278
Epoch: 51 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.5 | Training loss: 0.0380
Epoch: 51 | Batch:   3 | Lr: 0.00125 | Time used(s): 84.6 | Training loss: 0.0366
Epoch: 51 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0519
--------------------------------------------------
Epoch: 51 | [Train] | Loss: 0.0344
Epoch: 51 |   [Val] | Loss: 0.0752 | [CCC]:  0.4104 [' 0.4104'] | PCC: 0.4112 ['0.4112'] | RMSE: 0.4304 ['0.4304']
Epoch: 52 | Batch:   1 | Lr: 0.00125 | Time used(s): 87.3 | Training loss: 0.0222
Epoch: 52 | Batch:   2 | Lr: 0.00125 | Time used(s): 86.1 | Training loss: 0.0304
Epoch: 52 | Batch:   3 | Lr: 0.00125 | Time used(s): 84.6 | Training loss: 0.0342
Epoch: 52 | Batch:   4 | Lr: 0.00125 | Time used(s): 1.9 | Training loss: 0.0490
--------------------------------------------------
Epoch: 52 | [Train] | Loss: 0.0292
Epoch: 52 |   [Val] | Loss: 0.0751 | [CCC]:  0.4177 [' 0.4177'] | PCC: 0.4179 ['0.4179'] | RMSE: 0.4398 ['0.4398']
Epoch: 53 | Batch:   1 | Lr: 0.00125 | Time used(s): 87.7 | Training loss: 0.0286
Epoch: 53 | Batch:   2 | Lr: 0.00125 | Time used(s): 84.3 | Training loss: 0.0240
Epoch: 53 | Batch:   3 | Lr: 0.00125 | Time used(s): 85.0 | Training loss: 0.0285
Epoch: 53 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0310
--------------------------------------------------
Epoch: 53 | [Train] | Loss: 0.0271
Epoch: 53 |   [Val] | Loss: 0.0784 | [CCC]:  0.4108 [' 0.4108'] | PCC: 0.4114 ['0.4114'] | RMSE: 0.4347 ['0.4347']
Epoch: 54 | Batch:   1 | Lr: 0.00125 | Time used(s): 86.9 | Training loss: 0.0246
Epoch: 54 | Batch:   2 | Lr: 0.00125 | Time used(s): 84.5 | Training loss: 0.0259
Epoch: 54 | Batch:   3 | Lr: 0.00125 | Time used(s): 85.9 | Training loss: 0.0333
Epoch: 54 | Batch:   4 | Lr: 0.00125 | Time used(s): 1.9 | Training loss: 0.0191
--------------------------------------------------
Epoch: 54 | [Train] | Loss: 0.0278
Epoch: 54 |   [Val] | Loss: 0.0777 | [CCC]:  0.4044 [' 0.4044'] | PCC: 0.4081 ['0.4081'] | RMSE: 0.4481 ['0.4481']
Epoch: 55 | Batch:   1 | Lr: 0.00125 | Time used(s): 87.3 | Training loss: 0.0273
Epoch: 55 | Batch:   2 | Lr: 0.00125 | Time used(s): 85.0 | Training loss: 0.0264
Epoch: 55 | Batch:   3 | Lr: 0.00125 | Time used(s): 85.1 | Training loss: 0.0256
Epoch: 55 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.0 | Training loss: 0.0881
--------------------------------------------------
Epoch: 55 | [Train] | Loss: 0.0274
Epoch: 55 |   [Val] | Loss: 0.0775 | [CCC]:  0.4096 [' 0.4096'] | PCC: 0.4101 ['0.4101'] | RMSE: 0.4341 ['0.4341']
Epoch    55: reducing learning rate of group 0 to 6.2500e-04.
Epoch: 56 | Batch:   1 | Lr: 0.00063 | Time used(s): 87.5 | Training loss: 0.0233
Epoch: 56 | Batch:   2 | Lr: 0.00063 | Time used(s): 85.3 | Training loss: 0.0191
Epoch: 56 | Batch:   3 | Lr: 0.00063 | Time used(s): 84.7 | Training loss: 0.0255
Epoch: 56 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.0 | Training loss: 0.0548
--------------------------------------------------
Epoch: 56 | [Train] | Loss: 0.0232
Epoch: 56 |   [Val] | Loss: 0.0785 | [CCC]:  0.4059 [' 0.4059'] | PCC: 0.4070 ['0.4070'] | RMSE: 0.4324 ['0.4324']
Epoch: 57 | Batch:   1 | Lr: 0.00063 | Time used(s): 87.0 | Training loss: 0.0244
Epoch: 57 | Batch:   2 | Lr: 0.00063 | Time used(s): 84.1 | Training loss: 0.0185
Epoch: 57 | Batch:   3 | Lr: 0.00063 | Time used(s): 85.1 | Training loss: 0.0205
Epoch: 57 | Batch:   4 | Lr: 0.00063 | Time used(s): 1.9 | Training loss: 0.0288
--------------------------------------------------
Epoch: 57 | [Train] | Loss: 0.0213
Epoch: 57 |   [Val] | Loss: 0.0750 | [CCC]:  0.3969 [' 0.3969'] | PCC: 0.3978 ['0.3978'] | RMSE: 0.4339 ['0.4339']
Epoch: 58 | Batch:   1 | Lr: 0.00063 | Time used(s): 87.0 | Training loss: 0.0222
Epoch: 58 | Batch:   2 | Lr: 0.00063 | Time used(s): 85.4 | Training loss: 0.0242
Epoch: 58 | Batch:   3 | Lr: 0.00063 | Time used(s): 84.6 | Training loss: 0.0204
Epoch: 58 | Batch:   4 | Lr: 0.00063 | Time used(s): 1.9 | Training loss: 0.0043
--------------------------------------------------
Epoch: 58 | [Train] | Loss: 0.0220
Epoch: 58 |   [Val] | Loss: 0.0778 | [CCC]:  0.3969 [' 0.3969'] | PCC: 0.3978 ['0.3978'] | RMSE: 0.4460 ['0.4460']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 326 | Best [Val CCC]: 0.4307 [' 0.4307']| Loss: 0.0751 | PCC: 0.4311 ['0.4311'] | RMSE: 0.4421 ['0.4421']
On Test: CCC  0.5418 | PCC  0.5518 | RMSE  0.4013
****************************************************************************************************
Seed "326" over!
****************************************************************************************************
****************************************************************************************************
Using seed "327"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 327]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.8 | Training loss: 0.2491
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 84.7 | Training loss: 0.2446
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.7 | Training loss: 0.2384
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.2304
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2438
Epoch:  1 |   [Val] | Loss: 0.1705 | [CCC]:  0.1640 [' 0.1640'] | PCC: 0.1957 ['0.1957'] | RMSE: 0.4876 ['0.4876']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_327_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 85.7 | Training loss: 0.2166
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 83.9 | Training loss: 0.2017
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 84.2 | Training loss: 0.2403
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 1.9 | Training loss: 0.2375
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2198
Epoch:  2 |   [Val] | Loss: 0.1983 | [CCC]:  0.0926 [' 0.0926'] | PCC: 0.1864 ['0.1864'] | RMSE: 0.6269 ['0.6269']
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.2108
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.5 | Training loss: 0.2078
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 83.9 | Training loss: 0.1970
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1629
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.2045
Epoch:  3 |   [Val] | Loss: 0.1827 | [CCC]:  0.1557 [' 0.1557'] | PCC: 0.2719 ['0.2719'] | RMSE: 0.6535 ['0.6535']
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.9 | Training loss: 0.2058
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.2 | Training loss: 0.1791
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.7 | Training loss: 0.1954
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1997
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1935
Epoch:  4 |   [Val] | Loss: 0.1194 | [CCC]:  0.2768 [' 0.2768'] | PCC: 0.2887 ['0.2887'] | RMSE: 0.4630 ['0.4630']
Epoch:  4 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_327_None_None].pth"!
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.0 | Training loss: 0.1612
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.5 | Training loss: 0.1826
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.2 | Training loss: 0.1626
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1716
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1689
Epoch:  5 |   [Val] | Loss: 0.1194 | [CCC]:  0.2729 [' 0.2729'] | PCC: 0.2786 ['0.2786'] | RMSE: 0.5186 ['0.5186']
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.6 | Training loss: 0.1535
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.7 | Training loss: 0.1597
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.2 | Training loss: 0.1538
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1349
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1553
Epoch:  6 |   [Val] | Loss: 0.1653 | [CCC]:  0.1655 [' 0.1655'] | PCC: 0.2554 ['0.2554'] | RMSE: 0.5886 ['0.5886']
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.7 | Training loss: 0.1811
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.2 | Training loss: 0.1743
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.5 | Training loss: 0.1324
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1672
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1627
Epoch:  7 |   [Val] | Loss: 0.0985 | [CCC]:  0.3208 [' 0.3208'] | PCC: 0.3379 ['0.3379'] | RMSE: 0.4349 ['0.4349']
Epoch:  7 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_327_None_None].pth"!
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.0 | Training loss: 0.1417
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.7 | Training loss: 0.1513
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.1 | Training loss: 0.1416
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1375
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1448
Epoch:  8 |   [Val] | Loss: 0.0882 | [CCC]:  0.3458 [' 0.3458'] | PCC: 0.3519 ['0.3519'] | RMSE: 0.5157 ['0.5157']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_327_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.6 | Training loss: 0.1292
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.7 | Training loss: 0.1317
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.6 | Training loss: 0.1302
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1571
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1308
Epoch:  9 |   [Val] | Loss: 0.0879 | [CCC]:  0.3454 [' 0.3454'] | PCC: 0.3477 ['0.3477'] | RMSE: 0.4492 ['0.4492']
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.7 | Training loss: 0.1250
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.2 | Training loss: 0.1175
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.6 | Training loss: 0.1233
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1211
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1219
Epoch: 10 |   [Val] | Loss: 0.1064 | [CCC]:  0.3133 [' 0.3133'] | PCC: 0.3407 ['0.3407'] | RMSE: 0.4815 ['0.4815']
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.1 | Training loss: 0.1356
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.7 | Training loss: 0.1159
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.0 | Training loss: 0.1205
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1262
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1240
Epoch: 11 |   [Val] | Loss: 0.0819 | [CCC]:  0.3731 [' 0.3731'] | PCC: 0.3739 ['0.3739'] | RMSE: 0.4470 ['0.4470']
Epoch: 11 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_327_None_None].pth"!
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.3 | Training loss: 0.1131
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.5 | Training loss: 0.1162
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.1 | Training loss: 0.1102
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1050
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1130
Epoch: 12 |   [Val] | Loss: 0.1146 | [CCC]:  0.2821 [' 0.2821'] | PCC: 0.3602 ['0.3602'] | RMSE: 0.5925 ['0.5925']
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.4 | Training loss: 0.1222
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.6 | Training loss: 0.1074
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.1181
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0803
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1153
Epoch: 13 |   [Val] | Loss: 0.0957 | [CCC]:  0.3172 [' 0.3172'] | PCC: 0.3599 ['0.3599'] | RMSE: 0.5435 ['0.5435']
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.7 | Training loss: 0.1102
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.7 | Training loss: 0.1114
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.4 | Training loss: 0.1096
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1133
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1104
Epoch: 14 |   [Val] | Loss: 0.0759 | [CCC]:  0.3546 [' 0.3546'] | PCC: 0.3592 ['0.3592'] | RMSE: 0.4312 ['0.4312']
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.4 | Training loss: 0.1073
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.6 | Training loss: 0.1057
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.2 | Training loss: 0.0988
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1288
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1043
Epoch: 15 |   [Val] | Loss: 0.0973 | [CCC]:  0.3435 [' 0.3435'] | PCC: 0.3779 ['0.3779'] | RMSE: 0.5013 ['0.5013']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 88.3 | Training loss: 0.1283
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.1 | Training loss: 0.1060
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.5 | Training loss: 0.1086
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.0638
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.1135
Epoch: 16 |   [Val] | Loss: 0.0706 | [CCC]:  0.3797 [' 0.3797'] | PCC: 0.3937 ['0.3937'] | RMSE: 0.4125 ['0.4125']
Epoch: 16 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_327_None_None].pth"!
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.2 | Training loss: 0.1033
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.8 | Training loss: 0.1152
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.0 | Training loss: 0.0966
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1244
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.1053
Epoch: 17 |   [Val] | Loss: 0.0911 | [CCC]:  0.3283 [' 0.3283'] | PCC: 0.3843 ['0.3843'] | RMSE: 0.5574 ['0.5574']
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.5 | Training loss: 0.1040
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.8 | Training loss: 0.0923
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.3 | Training loss: 0.1005
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1147
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.0992
Epoch: 18 |   [Val] | Loss: 0.0753 | [CCC]:  0.3671 [' 0.3671'] | PCC: 0.3907 ['0.3907'] | RMSE: 0.4265 ['0.4265']
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 87.7 | Training loss: 0.0987
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 85.5 | Training loss: 0.0926
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 85.6 | Training loss: 0.0894
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1286
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.0941
Epoch: 19 |   [Val] | Loss: 0.0806 | [CCC]:  0.3684 [' 0.3684'] | PCC: 0.3844 ['0.3844'] | RMSE: 0.5054 ['0.5054']
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 86.6 | Training loss: 0.1005
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.4 | Training loss: 0.0806
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.3 | Training loss: 0.0898
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1042
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0905
Epoch: 20 |   [Val] | Loss: 0.0792 | [CCC]:  0.3838 [' 0.3838'] | PCC: 0.3909 ['0.3909'] | RMSE: 0.4348 ['0.4348']
Epoch: 20 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_327_None_None].pth"!
Epoch: 21 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.2 | Training loss: 0.0859
Epoch: 21 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.7 | Training loss: 0.0846
Epoch: 21 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.1 | Training loss: 0.0803
Epoch: 21 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0552
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0832
Epoch: 21 |   [Val] | Loss: 0.0915 | [CCC]:  0.3675 [' 0.3675'] | PCC: 0.3758 ['0.3758'] | RMSE: 0.4728 ['0.4728']
Epoch: 22 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.1 | Training loss: 0.0896
Epoch: 22 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.4 | Training loss: 0.0838
Epoch: 22 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.4 | Training loss: 0.0893
Epoch: 22 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0899
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0876
Epoch: 22 |   [Val] | Loss: 0.0853 | [CCC]:  0.3613 [' 0.3613'] | PCC: 0.3707 ['0.3707'] | RMSE: 0.4383 ['0.4383']
Epoch: 23 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.9 | Training loss: 0.0917
Epoch: 23 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.7 | Training loss: 0.0913
Epoch: 23 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.9 | Training loss: 0.0747
Epoch: 23 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0769
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0858
Epoch: 23 |   [Val] | Loss: 0.0786 | [CCC]:  0.3743 [' 0.3743'] | PCC: 0.3846 ['0.3846'] | RMSE: 0.4535 ['0.4535']
Epoch: 24 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.7 | Training loss: 0.0765
Epoch: 24 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.0 | Training loss: 0.0733
Epoch: 24 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.1 | Training loss: 0.0813
Epoch: 24 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1001
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0774
Epoch: 24 |   [Val] | Loss: 0.0894 | [CCC]:  0.3363 [' 0.3363'] | PCC: 0.3831 ['0.3831'] | RMSE: 0.5161 ['0.5161']
Epoch: 25 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.1 | Training loss: 0.0791
Epoch: 25 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.1 | Training loss: 0.0789
Epoch: 25 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.0 | Training loss: 0.0741
Epoch: 25 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0879
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0775
Epoch: 25 |   [Val] | Loss: 0.0744 | [CCC]:  0.4022 [' 0.4022'] | PCC: 0.4103 ['0.4103'] | RMSE: 0.4368 ['0.4368']
Epoch: 25 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_327_None_None].pth"!
Epoch: 26 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.8 | Training loss: 0.0654
Epoch: 26 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.6 | Training loss: 0.0765
Epoch: 26 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.4 | Training loss: 0.0749
Epoch: 26 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0385
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0717
Epoch: 26 |   [Val] | Loss: 0.0737 | [CCC]:  0.4073 [' 0.4073'] | PCC: 0.4076 ['0.4076'] | RMSE: 0.4408 ['0.4408']
Epoch: 26 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_327_None_None].pth"!
Epoch: 27 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.5 | Training loss: 0.0658
Epoch: 27 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.4 | Training loss: 0.0738
Epoch: 27 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.9 | Training loss: 0.0715
Epoch: 27 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0673
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0703
Epoch: 27 |   [Val] | Loss: 0.0745 | [CCC]:  0.4020 [' 0.4020'] | PCC: 0.4057 ['0.4057'] | RMSE: 0.4295 ['0.4295']
Epoch: 28 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.1 | Training loss: 0.0643
Epoch: 28 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.5 | Training loss: 0.0707
Epoch: 28 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.7 | Training loss: 0.0686
Epoch: 28 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0769
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0680
Epoch: 28 |   [Val] | Loss: 0.0783 | [CCC]:  0.3699 [' 0.3699'] | PCC: 0.4002 ['0.4002'] | RMSE: 0.5073 ['0.5073']
Epoch: 29 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.8 | Training loss: 0.0724
Epoch: 29 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.0 | Training loss: 0.0647
Epoch: 29 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.0 | Training loss: 0.0669
Epoch: 29 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0563
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0678
Epoch: 29 |   [Val] | Loss: 0.0971 | [CCC]:  0.3280 [' 0.3280'] | PCC: 0.3805 ['0.3805'] | RMSE: 0.5326 ['0.5326']
Epoch: 30 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.8 | Training loss: 0.0713
Epoch: 30 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.0 | Training loss: 0.0748
Epoch: 30 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.7 | Training loss: 0.0697
Epoch: 30 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0478
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0715
Epoch: 30 |   [Val] | Loss: 0.0773 | [CCC]:  0.3968 [' 0.3968'] | PCC: 0.3984 ['0.3984'] | RMSE: 0.4514 ['0.4514']
Epoch: 31 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.6 | Training loss: 0.0510
Epoch: 31 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.9 | Training loss: 0.0578
Epoch: 31 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.4 | Training loss: 0.0572
Epoch: 31 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0638
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0555
Epoch: 31 |   [Val] | Loss: 0.0800 | [CCC]:  0.3713 [' 0.3713'] | PCC: 0.3824 ['0.3824'] | RMSE: 0.4277 ['0.4277']
Epoch: 32 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.9 | Training loss: 0.0627
Epoch: 32 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.9 | Training loss: 0.0447
Epoch: 32 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.5 | Training loss: 0.0611
Epoch: 32 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0658
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0563
Epoch: 32 |   [Val] | Loss: 0.0815 | [CCC]:  0.3788 [' 0.3788'] | PCC: 0.3805 ['0.3805'] | RMSE: 0.4713 ['0.4713']
Epoch    32: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 33 | Batch:   1 | Lr: 0.00250 | Time used(s): 90.7 | Training loss: 0.0589
Epoch: 33 | Batch:   2 | Lr: 0.00250 | Time used(s): 88.7 | Training loss: 0.0544
Epoch: 33 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.3 | Training loss: 0.0542
Epoch: 33 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0180
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0553
Epoch: 33 |   [Val] | Loss: 0.0804 | [CCC]:  0.3451 [' 0.3451'] | PCC: 0.3589 ['0.3589'] | RMSE: 0.4254 ['0.4254']
Epoch: 34 | Batch:   1 | Lr: 0.00250 | Time used(s): 92.4 | Training loss: 0.0532
Epoch: 34 | Batch:   2 | Lr: 0.00250 | Time used(s): 89.3 | Training loss: 0.0489
Epoch: 34 | Batch:   3 | Lr: 0.00250 | Time used(s): 89.8 | Training loss: 0.0550
Epoch: 34 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0318
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0520
Epoch: 34 |   [Val] | Loss: 0.0832 | [CCC]:  0.3547 [' 0.3547'] | PCC: 0.3705 ['0.3705'] | RMSE: 0.5065 ['0.5065']
Epoch: 35 | Batch:   1 | Lr: 0.00250 | Time used(s): 91.8 | Training loss: 0.0501
Epoch: 35 | Batch:   2 | Lr: 0.00250 | Time used(s): 89.9 | Training loss: 0.0495
Epoch: 35 | Batch:   3 | Lr: 0.00250 | Time used(s): 89.3 | Training loss: 0.0493
Epoch: 35 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0581
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0498
Epoch: 35 |   [Val] | Loss: 0.0777 | [CCC]:  0.3555 [' 0.3555'] | PCC: 0.3672 ['0.3672'] | RMSE: 0.4271 ['0.4271']
Epoch: 36 | Batch:   1 | Lr: 0.00250 | Time used(s): 90.6 | Training loss: 0.0568
Epoch: 36 | Batch:   2 | Lr: 0.00250 | Time used(s): 88.7 | Training loss: 0.0544
Epoch: 36 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.6 | Training loss: 0.0390
Epoch: 36 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0710
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0504
Epoch: 36 |   [Val] | Loss: 0.0783 | [CCC]:  0.3734 [' 0.3734'] | PCC: 0.3814 ['0.3814'] | RMSE: 0.4557 ['0.4557']
Epoch: 37 | Batch:   1 | Lr: 0.00250 | Time used(s): 91.3 | Training loss: 0.0443
Epoch: 37 | Batch:   2 | Lr: 0.00250 | Time used(s): 89.7 | Training loss: 0.0444
Epoch: 37 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.9 | Training loss: 0.0464
Epoch: 37 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0285
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0448
Epoch: 37 |   [Val] | Loss: 0.0791 | [CCC]:  0.3735 [' 0.3735'] | PCC: 0.3816 ['0.3816'] | RMSE: 0.4409 ['0.4409']
Epoch: 38 | Batch:   1 | Lr: 0.00250 | Time used(s): 91.5 | Training loss: 0.0384
Epoch: 38 | Batch:   2 | Lr: 0.00250 | Time used(s): 90.9 | Training loss: 0.0424
Epoch: 38 | Batch:   3 | Lr: 0.00250 | Time used(s): 88.1 | Training loss: 0.0393
Epoch: 38 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0192
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0397
Epoch: 38 |   [Val] | Loss: 0.0784 | [CCC]:  0.3740 [' 0.3740'] | PCC: 0.3888 ['0.3888'] | RMSE: 0.4221 ['0.4221']
Epoch    38: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 39 | Batch:   1 | Lr: 0.00125 | Time used(s): 90.4 | Training loss: 0.0448
Epoch: 39 | Batch:   2 | Lr: 0.00125 | Time used(s): 89.4 | Training loss: 0.0388
Epoch: 39 | Batch:   3 | Lr: 0.00125 | Time used(s): 90.1 | Training loss: 0.0357
Epoch: 39 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0507
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0399
Epoch: 39 |   [Val] | Loss: 0.0805 | [CCC]:  0.3726 [' 0.3726'] | PCC: 0.3935 ['0.3935'] | RMSE: 0.4676 ['0.4676']
Epoch: 40 | Batch:   1 | Lr: 0.00125 | Time used(s): 90.7 | Training loss: 0.0420
Epoch: 40 | Batch:   2 | Lr: 0.00125 | Time used(s): 89.4 | Training loss: 0.0346
Epoch: 40 | Batch:   3 | Lr: 0.00125 | Time used(s): 90.9 | Training loss: 0.0364
Epoch: 40 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0505
--------------------------------------------------
Epoch: 40 | [Train] | Loss: 0.0379
Epoch: 40 |   [Val] | Loss: 0.0784 | [CCC]:  0.3835 [' 0.3835'] | PCC: 0.3908 ['0.3908'] | RMSE: 0.4281 ['0.4281']
Epoch: 41 | Batch:   1 | Lr: 0.00125 | Time used(s): 92.2 | Training loss: 0.0338
Epoch: 41 | Batch:   2 | Lr: 0.00125 | Time used(s): 88.0 | Training loss: 0.0255
Epoch: 41 | Batch:   3 | Lr: 0.00125 | Time used(s): 90.4 | Training loss: 0.0372
Epoch: 41 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0704
--------------------------------------------------
Epoch: 41 | [Train] | Loss: 0.0328
Epoch: 41 |   [Val] | Loss: 0.0878 | [CCC]:  0.3539 [' 0.3539'] | PCC: 0.3775 ['0.3775'] | RMSE: 0.4482 ['0.4482']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 327 | Best [Val CCC]: 0.4073 [' 0.4073']| Loss: 0.0737 | PCC: 0.4076 ['0.4076'] | RMSE: 0.4408 ['0.4408']
On Test: CCC  0.4806 | PCC  0.5193 | RMSE  0.4312
****************************************************************************************************
Seed "327" over!
****************************************************************************************************
****************************************************************************************************
Using seed "328"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 328]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.7 | Training loss: 0.2495
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.0 | Training loss: 0.2448
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.4 | Training loss: 0.2354
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.2447
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2433
Epoch:  1 |   [Val] | Loss: 0.2128 | [CCC]:  0.0725 [' 0.0725'] | PCC: 0.1609 ['0.1609'] | RMSE: 0.7585 ['0.7585']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_328_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.3 | Training loss: 0.2226
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.2 | Training loss: 0.2362
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.4 | Training loss: 0.2300
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1896
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2290
Epoch:  2 |   [Val] | Loss: 0.1934 | [CCC]:  0.1217 [' 0.1217'] | PCC: 0.1848 ['0.1848'] | RMSE: 0.5070 ['0.5070']
Epoch:  2 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_328_None_None].pth"!
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.3 | Training loss: 0.2142
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.4 | Training loss: 0.2049
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.4 | Training loss: 0.2012
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1893
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.2065
Epoch:  3 |   [Val] | Loss: 0.1376 | [CCC]:  0.2359 [' 0.2359'] | PCC: 0.2467 ['0.2467'] | RMSE: 0.5907 ['0.5907']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_328_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.8 | Training loss: 0.1709
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 91.3 | Training loss: 0.2129
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.0 | Training loss: 0.1721
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1854
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1853
Epoch:  4 |   [Val] | Loss: 0.1373 | [CCC]:  0.2316 [' 0.2316'] | PCC: 0.2818 ['0.2818'] | RMSE: 0.4346 ['0.4346']
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.5 | Training loss: 0.1679
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 92.7 | Training loss: 0.1577
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 93.2 | Training loss: 0.1523
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1836
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1597
Epoch:  5 |   [Val] | Loss: 0.1013 | [CCC]:  0.3192 [' 0.3192'] | PCC: 0.3262 ['0.3262'] | RMSE: 0.5308 ['0.5308']
Epoch:  5 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_328_None_None].pth"!
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 94.2 | Training loss: 0.1497
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 93.6 | Training loss: 0.1711
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.4 | Training loss: 0.1503
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1541
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1570
Epoch:  6 |   [Val] | Loss: 0.0964 | [CCC]:  0.3268 [' 0.3268'] | PCC: 0.3407 ['0.3407'] | RMSE: 0.4261 ['0.4261']
Epoch:  6 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_328_None_None].pth"!
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 95.7 | Training loss: 0.1509
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 94.2 | Training loss: 0.1354
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.5 | Training loss: 0.1469
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1500
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1445
Epoch:  7 |   [Val] | Loss: 0.1221 | [CCC]:  0.2517 [' 0.2517'] | PCC: 0.3457 ['0.3457'] | RMSE: 0.5468 ['0.5468']
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 95.6 | Training loss: 0.1586
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 93.5 | Training loss: 0.1335
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.4 | Training loss: 0.1511
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1717
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1481
Epoch:  8 |   [Val] | Loss: 0.0892 | [CCC]:  0.3406 [' 0.3406'] | PCC: 0.3459 ['0.3459'] | RMSE: 0.4389 ['0.4389']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_328_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 95.2 | Training loss: 0.1295
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 92.8 | Training loss: 0.1347
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 93.9 | Training loss: 0.1400
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1197
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1345
Epoch:  9 |   [Val] | Loss: 0.0955 | [CCC]:  0.3327 [' 0.3327'] | PCC: 0.3715 ['0.3715'] | RMSE: 0.5506 ['0.5506']
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 95.1 | Training loss: 0.1319
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 91.3 | Training loss: 0.1248
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 93.2 | Training loss: 0.1187
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1470
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1255
Epoch: 10 |   [Val] | Loss: 0.0798 | [CCC]:  0.3850 [' 0.3850'] | PCC: 0.4079 ['0.4079'] | RMSE: 0.3945 ['0.3945']
Epoch: 10 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_328_None_None].pth"!
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.4 | Training loss: 0.1219
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 90.7 | Training loss: 0.1257
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.6 | Training loss: 0.1201
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1359
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1228
Epoch: 11 |   [Val] | Loss: 0.0784 | [CCC]:  0.4146 [' 0.4146'] | PCC: 0.4176 ['0.4176'] | RMSE: 0.4373 ['0.4373']
Epoch: 11 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_328_None_None].pth"!
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.3 | Training loss: 0.1135
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 91.2 | Training loss: 0.1136
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.9 | Training loss: 0.1081
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1216
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1119
Epoch: 12 |   [Val] | Loss: 0.0825 | [CCC]:  0.3815 [' 0.3815'] | PCC: 0.3962 ['0.3962'] | RMSE: 0.4254 ['0.4254']
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.9 | Training loss: 0.1102
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.7 | Training loss: 0.1052
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.7 | Training loss: 0.1095
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0795
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1079
Epoch: 13 |   [Val] | Loss: 0.0798 | [CCC]:  0.3839 [' 0.3839'] | PCC: 0.4028 ['0.4028'] | RMSE: 0.4486 ['0.4486']
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.2 | Training loss: 0.1019
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 89.6 | Training loss: 0.1066
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.0 | Training loss: 0.1076
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1168
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1056
Epoch: 14 |   [Val] | Loss: 0.0773 | [CCC]:  0.3824 [' 0.3824'] | PCC: 0.3849 ['0.3849'] | RMSE: 0.4410 ['0.4410']
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.6 | Training loss: 0.0966
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 90.6 | Training loss: 0.1065
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.7 | Training loss: 0.1000
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1003
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1011
Epoch: 15 |   [Val] | Loss: 0.0746 | [CCC]:  0.3921 [' 0.3921'] | PCC: 0.3978 ['0.3978'] | RMSE: 0.4386 ['0.4386']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.7 | Training loss: 0.0940
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 91.2 | Training loss: 0.0905
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.5 | Training loss: 0.0934
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1192
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.0931
Epoch: 16 |   [Val] | Loss: 0.0855 | [CCC]:  0.3588 [' 0.3588'] | PCC: 0.3985 ['0.3985'] | RMSE: 0.5241 ['0.5241']
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.3 | Training loss: 0.0950
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 90.4 | Training loss: 0.0845
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.0 | Training loss: 0.0938
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1079
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.0914
Epoch: 17 |   [Val] | Loss: 0.0764 | [CCC]:  0.3751 [' 0.3751'] | PCC: 0.3983 ['0.3983'] | RMSE: 0.4050 ['0.4050']
Epoch    17: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 18 | Batch:   1 | Lr: 0.00250 | Time used(s): 92.6 | Training loss: 0.0957
Epoch: 18 | Batch:   2 | Lr: 0.00250 | Time used(s): 90.4 | Training loss: 0.0895
Epoch: 18 | Batch:   3 | Lr: 0.00250 | Time used(s): 91.1 | Training loss: 0.0797
Epoch: 18 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0476
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.0876
Epoch: 18 |   [Val] | Loss: 0.0806 | [CCC]:  0.4061 [' 0.4061'] | PCC: 0.4167 ['0.4167'] | RMSE: 0.4718 ['0.4718']
Epoch: 19 | Batch:   1 | Lr: 0.00250 | Time used(s): 92.8 | Training loss: 0.0884
Epoch: 19 | Batch:   2 | Lr: 0.00250 | Time used(s): 91.0 | Training loss: 0.0901
Epoch: 19 | Batch:   3 | Lr: 0.00250 | Time used(s): 91.8 | Training loss: 0.0821
Epoch: 19 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0746
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.0867
Epoch: 19 |   [Val] | Loss: 0.0844 | [CCC]:  0.3549 [' 0.3549'] | PCC: 0.3914 ['0.3914'] | RMSE: 0.4824 ['0.4824']
Epoch: 20 | Batch:   1 | Lr: 0.00250 | Time used(s): 92.9 | Training loss: 0.0834
Epoch: 20 | Batch:   2 | Lr: 0.00250 | Time used(s): 91.1 | Training loss: 0.0925
Epoch: 20 | Batch:   3 | Lr: 0.00250 | Time used(s): 91.8 | Training loss: 0.0793
Epoch: 20 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.1221
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0857
Epoch: 20 |   [Val] | Loss: 0.0768 | [CCC]:  0.4045 [' 0.4045'] | PCC: 0.4073 ['0.4073'] | RMSE: 0.4581 ['0.4581']
Epoch: 21 | Batch:   1 | Lr: 0.00250 | Time used(s): 92.4 | Training loss: 0.0745
Epoch: 21 | Batch:   2 | Lr: 0.00250 | Time used(s): 90.6 | Training loss: 0.0846
Epoch: 21 | Batch:   3 | Lr: 0.00250 | Time used(s): 91.1 | Training loss: 0.0860
Epoch: 21 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0615
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0814
Epoch: 21 |   [Val] | Loss: 0.0753 | [CCC]:  0.4024 [' 0.4024'] | PCC: 0.4028 ['0.4028'] | RMSE: 0.4406 ['0.4406']
Epoch: 22 | Batch:   1 | Lr: 0.00250 | Time used(s): 93.2 | Training loss: 0.0700
Epoch: 22 | Batch:   2 | Lr: 0.00250 | Time used(s): 90.5 | Training loss: 0.0782
Epoch: 22 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.6 | Training loss: 0.0718
Epoch: 22 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0405
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0728
Epoch: 22 |   [Val] | Loss: 0.0833 | [CCC]:  0.3780 [' 0.3780'] | PCC: 0.3987 ['0.3987'] | RMSE: 0.4753 ['0.4753']
Epoch: 23 | Batch:   1 | Lr: 0.00250 | Time used(s): 92.4 | Training loss: 0.0731
Epoch: 23 | Batch:   2 | Lr: 0.00250 | Time used(s): 90.8 | Training loss: 0.0741
Epoch: 23 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.6 | Training loss: 0.0684
Epoch: 23 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0838
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0720
Epoch: 23 |   [Val] | Loss: 0.0765 | [CCC]:  0.3982 [' 0.3982'] | PCC: 0.4044 ['0.4044'] | RMSE: 0.4661 ['0.4661']
Epoch    23: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 24 | Batch:   1 | Lr: 0.00125 | Time used(s): 92.2 | Training loss: 0.0625
Epoch: 24 | Batch:   2 | Lr: 0.00125 | Time used(s): 90.5 | Training loss: 0.0678
Epoch: 24 | Batch:   3 | Lr: 0.00125 | Time used(s): 90.7 | Training loss: 0.0683
Epoch: 24 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0526
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0660
Epoch: 24 |   [Val] | Loss: 0.0719 | [CCC]:  0.4078 [' 0.4078'] | PCC: 0.4084 ['0.4084'] | RMSE: 0.4332 ['0.4332']
Epoch: 25 | Batch:   1 | Lr: 0.00125 | Time used(s): 92.3 | Training loss: 0.0690
Epoch: 25 | Batch:   2 | Lr: 0.00125 | Time used(s): 89.4 | Training loss: 0.0699
Epoch: 25 | Batch:   3 | Lr: 0.00125 | Time used(s): 91.6 | Training loss: 0.0566
Epoch: 25 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0435
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0648
Epoch: 25 |   [Val] | Loss: 0.0865 | [CCC]:  0.3665 [' 0.3665'] | PCC: 0.3970 ['0.3970'] | RMSE: 0.5021 ['0.5021']
Epoch: 26 | Batch:   1 | Lr: 0.00125 | Time used(s): 93.0 | Training loss: 0.0681
Epoch: 26 | Batch:   2 | Lr: 0.00125 | Time used(s): 89.8 | Training loss: 0.0768
Epoch: 26 | Batch:   3 | Lr: 0.00125 | Time used(s): 91.6 | Training loss: 0.0612
Epoch: 26 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0552
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0685
Epoch: 26 |   [Val] | Loss: 0.0753 | [CCC]:  0.4002 [' 0.4002'] | PCC: 0.4121 ['0.4121'] | RMSE: 0.4261 ['0.4261']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 328 | Best [Val CCC]: 0.4146 [' 0.4146']| Loss: 0.0784 | PCC: 0.4176 ['0.4176'] | RMSE: 0.4373 ['0.4373']
On Test: CCC  0.5321 | PCC  0.5635 | RMSE  0.3897
****************************************************************************************************
Seed "328" over!
****************************************************************************************************
****************************************************************************************************
Using seed "329"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 329]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.3 | Training loss: 0.2503
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.8 | Training loss: 0.2447
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.4 | Training loss: 0.2354
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.2251
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2432
Epoch:  1 |   [Val] | Loss: 0.1880 | [CCC]:  0.1287 [' 0.1287'] | PCC: 0.1652 ['0.1652'] | RMSE: 0.4521 ['0.4521']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_329_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.9 | Training loss: 0.2146
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.1 | Training loss: 0.1988
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.7 | Training loss: 0.2001
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.2403
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2051
Epoch:  2 |   [Val] | Loss: 0.2395 | [CCC]:  0.0226 [' 0.0226'] | PCC: 0.1412 ['0.1412'] | RMSE: 0.9027 ['0.9027']
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.9 | Training loss: 0.2372
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 88.2 | Training loss: 0.1997
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.3 | Training loss: 0.2183
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1901
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.2179
Epoch:  3 |   [Val] | Loss: 0.1496 | [CCC]:  0.2117 [' 0.2117'] | PCC: 0.2432 ['0.2432'] | RMSE: 0.4800 ['0.4800']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_329_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 91.1 | Training loss: 0.1875
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 90.6 | Training loss: 0.1907
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.2 | Training loss: 0.1690
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1818
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1824
Epoch:  4 |   [Val] | Loss: 0.1284 | [CCC]:  0.2562 [' 0.2562'] | PCC: 0.2738 ['0.2738'] | RMSE: 0.5405 ['0.5405']
Epoch:  4 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_329_None_None].pth"!
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 94.8 | Training loss: 0.1554
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 93.4 | Training loss: 0.1521
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.8 | Training loss: 0.1518
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1622
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1533
Epoch:  5 |   [Val] | Loss: 0.1090 | [CCC]:  0.2967 [' 0.2967'] | PCC: 0.3010 ['0.3010'] | RMSE: 0.4952 ['0.4952']
Epoch:  5 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_329_None_None].pth"!
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 94.6 | Training loss: 0.1354
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 91.8 | Training loss: 0.1414
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.7 | Training loss: 0.1390
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1162
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1382
Epoch:  6 |   [Val] | Loss: 0.0931 | [CCC]:  0.3328 [' 0.3328'] | PCC: 0.3381 ['0.3381'] | RMSE: 0.4356 ['0.4356']
Epoch:  6 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_329_None_None].pth"!
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 94.0 | Training loss: 0.1362
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 91.6 | Training loss: 0.1323
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.6 | Training loss: 0.1308
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1523
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1334
Epoch:  7 |   [Val] | Loss: 0.0956 | [CCC]:  0.3283 [' 0.3283'] | PCC: 0.3595 ['0.3595'] | RMSE: 0.4624 ['0.4624']
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 94.5 | Training loss: 0.1263
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 93.1 | Training loss: 0.1175
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 93.2 | Training loss: 0.1228
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0996
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1218
Epoch:  8 |   [Val] | Loss: 0.0817 | [CCC]:  0.3902 [' 0.3902'] | PCC: 0.3959 ['0.3959'] | RMSE: 0.4349 ['0.4349']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_329_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 93.2 | Training loss: 0.1182
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 91.2 | Training loss: 0.1166
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.9 | Training loss: 0.1094
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0898
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1144
Epoch:  9 |   [Val] | Loss: 0.0943 | [CCC]:  0.3343 [' 0.3343'] | PCC: 0.3799 ['0.3799'] | RMSE: 0.4818 ['0.4818']
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 92.2 | Training loss: 0.1211
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 90.8 | Training loss: 0.1068
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.4 | Training loss: 0.1065
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1451
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1120
Epoch: 10 |   [Val] | Loss: 0.1205 | [CCC]:  0.2667 [' 0.2667'] | PCC: 0.3838 ['0.3838'] | RMSE: 0.6202 ['0.6202']
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.7 | Training loss: 0.1444
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.7 | Training loss: 0.1004
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.7 | Training loss: 0.1217
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1075
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1220
Epoch: 11 |   [Val] | Loss: 0.0893 | [CCC]:  0.3438 [' 0.3438'] | PCC: 0.3966 ['0.3966'] | RMSE: 0.4988 ['0.4988']
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 90.2 | Training loss: 0.1052
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.5 | Training loss: 0.1092
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 86.6 | Training loss: 0.1021
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1170
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1057
Epoch: 12 |   [Val] | Loss: 0.0864 | [CCC]:  0.3570 [' 0.3570'] | PCC: 0.3986 ['0.3986'] | RMSE: 0.4613 ['0.4613']
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.5 | Training loss: 0.1026
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 87.2 | Training loss: 0.1043
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.3 | Training loss: 0.0878
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.0 | Training loss: 0.1225
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.0986
Epoch: 13 |   [Val] | Loss: 0.0731 | [CCC]:  0.3987 [' 0.3987'] | PCC: 0.4028 ['0.4028'] | RMSE: 0.4204 ['0.4204']
Epoch: 13 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_329_None_None].pth"!
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 89.3 | Training loss: 0.0893
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 86.7 | Training loss: 0.0988
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 87.5 | Training loss: 0.0956
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0586
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.0940
Epoch: 14 |   [Val] | Loss: 0.0782 | [CCC]:  0.4071 [' 0.4071'] | PCC: 0.4099 ['0.4099'] | RMSE: 0.4590 ['0.4590']
Epoch: 14 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_329_None_None].pth"!
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 96.3 | Training loss: 0.0954
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 95.9 | Training loss: 0.0934
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.2 | Training loss: 0.0850
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1005
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.0914
Epoch: 15 |   [Val] | Loss: 0.0921 | [CCC]:  0.3341 [' 0.3341'] | PCC: 0.4012 ['0.4012'] | RMSE: 0.4537 ['0.4537']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 105.6 | Training loss: 0.0910
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 97.8 | Training loss: 0.0934
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.6 | Training loss: 0.0857
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1055
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.0903
Epoch: 16 |   [Val] | Loss: 0.0762 | [CCC]:  0.4047 [' 0.4047'] | PCC: 0.4096 ['0.4096'] | RMSE: 0.4417 ['0.4417']
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 108.5 | Training loss: 0.0801
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 100.8 | Training loss: 0.0941
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.0 | Training loss: 0.0830
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0934
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.0859
Epoch: 17 |   [Val] | Loss: 0.0773 | [CCC]:  0.3799 [' 0.3799'] | PCC: 0.4068 ['0.4068'] | RMSE: 0.4174 ['0.4174']
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 103.3 | Training loss: 0.0856
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 94.1 | Training loss: 0.0840
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.0 | Training loss: 0.0727
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0938
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.0810
Epoch: 18 |   [Val] | Loss: 0.0751 | [CCC]:  0.3932 [' 0.3932'] | PCC: 0.4249 ['0.4249'] | RMSE: 0.4743 ['0.4743']
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 106.7 | Training loss: 0.0878
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 94.4 | Training loss: 0.0764
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 88.6 | Training loss: 0.0859
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0465
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.0828
Epoch: 19 |   [Val] | Loss: 0.0680 | [CCC]:  0.4148 [' 0.4148'] | PCC: 0.4174 ['0.4174'] | RMSE: 0.4283 ['0.4283']
Epoch: 19 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_329_None_None].pth"!
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 102.2 | Training loss: 0.0748
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 93.5 | Training loss: 0.0735
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.1 | Training loss: 0.0685
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0728
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0723
Epoch: 20 |   [Val] | Loss: 0.0791 | [CCC]:  0.3883 [' 0.3883'] | PCC: 0.4184 ['0.4184'] | RMSE: 0.4438 ['0.4438']
Epoch: 21 | Batch:   1 | Lr: 0.00500 | Time used(s): 106.3 | Training loss: 0.0921
Epoch: 21 | Batch:   2 | Lr: 0.00500 | Time used(s): 96.8 | Training loss: 0.0717
Epoch: 21 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.4 | Training loss: 0.0845
Epoch: 21 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0712
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0826
Epoch: 21 |   [Val] | Loss: 0.0781 | [CCC]:  0.3753 [' 0.3753'] | PCC: 0.3979 ['0.3979'] | RMSE: 0.4046 ['0.4046']
Epoch: 22 | Batch:   1 | Lr: 0.00500 | Time used(s): 104.4 | Training loss: 0.0788
Epoch: 22 | Batch:   2 | Lr: 0.00500 | Time used(s): 95.9 | Training loss: 0.0799
Epoch: 22 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.7 | Training loss: 0.0723
Epoch: 22 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0688
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0769
Epoch: 22 |   [Val] | Loss: 0.0745 | [CCC]:  0.4105 [' 0.4105'] | PCC: 0.4128 ['0.4128'] | RMSE: 0.4296 ['0.4296']
Epoch: 23 | Batch:   1 | Lr: 0.00500 | Time used(s): 102.7 | Training loss: 0.0675
Epoch: 23 | Batch:   2 | Lr: 0.00500 | Time used(s): 93.2 | Training loss: 0.0761
Epoch: 23 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.4 | Training loss: 0.0631
Epoch: 23 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0802
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0691
Epoch: 23 |   [Val] | Loss: 0.0732 | [CCC]:  0.4024 [' 0.4024'] | PCC: 0.4097 ['0.4097'] | RMSE: 0.4410 ['0.4410']
Epoch: 24 | Batch:   1 | Lr: 0.00500 | Time used(s): 104.1 | Training loss: 0.0614
Epoch: 24 | Batch:   2 | Lr: 0.00500 | Time used(s): 94.8 | Training loss: 0.0697
Epoch: 24 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.6 | Training loss: 0.0645
Epoch: 24 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0495
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0649
Epoch: 24 |   [Val] | Loss: 0.0860 | [CCC]:  0.3613 [' 0.3613'] | PCC: 0.3825 ['0.3825'] | RMSE: 0.4978 ['0.4978']
Epoch: 25 | Batch:   1 | Lr: 0.00500 | Time used(s): 102.7 | Training loss: 0.0696
Epoch: 25 | Batch:   2 | Lr: 0.00500 | Time used(s): 93.0 | Training loss: 0.0590
Epoch: 25 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.5 | Training loss: 0.0622
Epoch: 25 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0714
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0637
Epoch: 25 |   [Val] | Loss: 0.0765 | [CCC]:  0.3890 [' 0.3890'] | PCC: 0.3903 ['0.3903'] | RMSE: 0.4345 ['0.4345']
Epoch    25: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 26 | Batch:   1 | Lr: 0.00250 | Time used(s): 106.1 | Training loss: 0.0574
Epoch: 26 | Batch:   2 | Lr: 0.00250 | Time used(s): 95.5 | Training loss: 0.0542
Epoch: 26 | Batch:   3 | Lr: 0.00250 | Time used(s): 91.6 | Training loss: 0.0558
Epoch: 26 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0740
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0561
Epoch: 26 |   [Val] | Loss: 0.0736 | [CCC]:  0.4067 [' 0.4067'] | PCC: 0.4073 ['0.4073'] | RMSE: 0.4485 ['0.4485']
Epoch: 27 | Batch:   1 | Lr: 0.00250 | Time used(s): 104.9 | Training loss: 0.0504
Epoch: 27 | Batch:   2 | Lr: 0.00250 | Time used(s): 97.9 | Training loss: 0.0461
Epoch: 27 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.9 | Training loss: 0.0568
Epoch: 27 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0791
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0515
Epoch: 27 |   [Val] | Loss: 0.0742 | [CCC]:  0.3973 [' 0.3973'] | PCC: 0.4056 ['0.4056'] | RMSE: 0.4295 ['0.4295']
Epoch: 28 | Batch:   1 | Lr: 0.00250 | Time used(s): 125.8 | Training loss: 0.0551
Epoch: 28 | Batch:   2 | Lr: 0.00250 | Time used(s): 120.4 | Training loss: 0.0480
Epoch: 28 | Batch:   3 | Lr: 0.00250 | Time used(s): 97.6 | Training loss: 0.0457
Epoch: 28 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0509
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0496
Epoch: 28 |   [Val] | Loss: 0.0732 | [CCC]:  0.4135 [' 0.4135'] | PCC: 0.4187 ['0.4187'] | RMSE: 0.4338 ['0.4338']
Epoch: 29 | Batch:   1 | Lr: 0.00250 | Time used(s): 128.5 | Training loss: 0.0492
Epoch: 29 | Batch:   2 | Lr: 0.00250 | Time used(s): 124.0 | Training loss: 0.0513
Epoch: 29 | Batch:   3 | Lr: 0.00250 | Time used(s): 98.0 | Training loss: 0.0423
Epoch: 29 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0296
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0473
Epoch: 29 |   [Val] | Loss: 0.0798 | [CCC]:  0.3761 [' 0.3761'] | PCC: 0.3924 ['0.3924'] | RMSE: 0.4661 ['0.4661']
Epoch: 30 | Batch:   1 | Lr: 0.00250 | Time used(s): 135.0 | Training loss: 0.0527
Epoch: 30 | Batch:   2 | Lr: 0.00250 | Time used(s): 128.4 | Training loss: 0.0441
Epoch: 30 | Batch:   3 | Lr: 0.00250 | Time used(s): 99.1 | Training loss: 0.0405
Epoch: 30 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0521
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0458
Epoch: 30 |   [Val] | Loss: 0.0766 | [CCC]:  0.3951 [' 0.3951'] | PCC: 0.3953 ['0.3953'] | RMSE: 0.4414 ['0.4414']
Epoch: 31 | Batch:   1 | Lr: 0.00250 | Time used(s): 129.6 | Training loss: 0.0419
Epoch: 31 | Batch:   2 | Lr: 0.00250 | Time used(s): 126.0 | Training loss: 0.0403
Epoch: 31 | Batch:   3 | Lr: 0.00250 | Time used(s): 100.6 | Training loss: 0.0441
Epoch: 31 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0287
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0419
Epoch: 31 |   [Val] | Loss: 0.0761 | [CCC]:  0.3881 [' 0.3881'] | PCC: 0.3940 ['0.3940'] | RMSE: 0.4277 ['0.4277']
Epoch    31: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 32 | Batch:   1 | Lr: 0.00125 | Time used(s): 126.4 | Training loss: 0.0420
Epoch: 32 | Batch:   2 | Lr: 0.00125 | Time used(s): 115.3 | Training loss: 0.0346
Epoch: 32 | Batch:   3 | Lr: 0.00125 | Time used(s): 97.9 | Training loss: 0.0345
Epoch: 32 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0846
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0378
Epoch: 32 |   [Val] | Loss: 0.0794 | [CCC]:  0.3770 [' 0.3770'] | PCC: 0.3818 ['0.3818'] | RMSE: 0.4665 ['0.4665']
Epoch: 33 | Batch:   1 | Lr: 0.00125 | Time used(s): 120.7 | Training loss: 0.0317
Epoch: 33 | Batch:   2 | Lr: 0.00125 | Time used(s): 101.8 | Training loss: 0.0394
Epoch: 33 | Batch:   3 | Lr: 0.00125 | Time used(s): 90.9 | Training loss: 0.0391
Epoch: 33 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0411
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0368
Epoch: 33 |   [Val] | Loss: 0.0772 | [CCC]:  0.3866 [' 0.3866'] | PCC: 0.3870 ['0.3870'] | RMSE: 0.4446 ['0.4446']
Epoch: 34 | Batch:   1 | Lr: 0.00125 | Time used(s): 107.4 | Training loss: 0.0308
Epoch: 34 | Batch:   2 | Lr: 0.00125 | Time used(s): 98.2 | Training loss: 0.0347
Epoch: 34 | Batch:   3 | Lr: 0.00125 | Time used(s): 91.2 | Training loss: 0.0345
Epoch: 34 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0211
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0332
Epoch: 34 |   [Val] | Loss: 0.0785 | [CCC]:  0.3941 [' 0.3941'] | PCC: 0.3948 ['0.3948'] | RMSE: 0.4446 ['0.4446']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 329 | Best [Val CCC]: 0.4148 [' 0.4148']| Loss: 0.0680 | PCC: 0.4174 ['0.4174'] | RMSE: 0.4283 ['0.4283']
On Test: CCC  0.5643 | PCC  0.5665 | RMSE  0.3937
****************************************************************************************************
Seed "329" over!
****************************************************************************************************
****************************************************************************************************
Using seed "330"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 330]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 102.3 | Training loss: 0.2504
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 93.0 | Training loss: 0.2475
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 89.8 | Training loss: 0.2366
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2119
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2443
Epoch:  1 |   [Val] | Loss: 0.2476 | [CCC]:  0.0049 [' 0.0049'] | PCC: 0.0918 ['0.0918'] | RMSE: 0.7577 ['0.7577']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 104.4 | Training loss: 0.2477
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 92.3 | Training loss: 0.2482
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.7 | Training loss: 0.2485
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.2480
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2481
Epoch:  2 |   [Val] | Loss: 0.2454 | [CCC]:  0.0121 [' 0.0121'] | PCC: 0.1350 ['0.1350'] | RMSE: 1.1760 ['1.1760']
Epoch:  2 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 105.4 | Training loss: 0.2460
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 94.3 | Training loss: 0.2436
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.0 | Training loss: 0.2366
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.2309
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.2419
Epoch:  3 |   [Val] | Loss: 0.2040 | [CCC]:  0.1049 [' 0.1049'] | PCC: 0.1320 ['0.1320'] | RMSE: 0.5408 ['0.5408']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 108.8 | Training loss: 0.2180
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 97.6 | Training loss: 0.2161
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 95.5 | Training loss: 0.2145
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.2111
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.2161
Epoch:  4 |   [Val] | Loss: 0.1806 | [CCC]:  0.1505 [' 0.1505'] | PCC: 0.2400 ['0.2400'] | RMSE: 0.7503 ['0.7503']
Epoch:  4 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 111.1 | Training loss: 0.2052
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 100.2 | Training loss: 0.2122
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 96.5 | Training loss: 0.2069
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1674
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.2074
Epoch:  5 |   [Val] | Loss: 0.1808 | [CCC]:  0.1453 [' 0.1453'] | PCC: 0.2917 ['0.2917'] | RMSE: 0.6231 ['0.6231']
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 108.1 | Training loss: 0.2024
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 99.2 | Training loss: 0.1859
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 96.7 | Training loss: 0.1744
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1792
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1874
Epoch:  6 |   [Val] | Loss: 0.1283 | [CCC]:  0.2825 [' 0.2825'] | PCC: 0.2855 ['0.2855'] | RMSE: 0.5278 ['0.5278']
Epoch:  6 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 120.4 | Training loss: 0.1665
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 104.9 | Training loss: 0.1618
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 96.2 | Training loss: 0.1625
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1539
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1634
Epoch:  7 |   [Val] | Loss: 0.1344 | [CCC]:  0.2653 [' 0.2653'] | PCC: 0.2898 ['0.2898'] | RMSE: 0.4844 ['0.4844']
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 112.7 | Training loss: 0.1640
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 105.3 | Training loss: 0.1638
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 95.7 | Training loss: 0.1515
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1511
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1596
Epoch:  8 |   [Val] | Loss: 0.1229 | [CCC]:  0.2761 [' 0.2761'] | PCC: 0.3055 ['0.3055'] | RMSE: 0.4684 ['0.4684']
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 111.7 | Training loss: 0.1544
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 99.8 | Training loss: 0.1443
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 100.6 | Training loss: 0.1388
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1561
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1460
Epoch:  9 |   [Val] | Loss: 0.1261 | [CCC]:  0.2682 [' 0.2682'] | PCC: 0.2997 ['0.2997'] | RMSE: 0.5132 ['0.5132']
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 137.9 | Training loss: 0.1454
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 132.9 | Training loss: 0.1358
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 108.5 | Training loss: 0.1434
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1400
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1415
Epoch: 10 |   [Val] | Loss: 0.1410 | [CCC]:  0.2189 [' 0.2189'] | PCC: 0.2821 ['0.2821'] | RMSE: 0.6084 ['0.6084']
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.9 | Training loss: 0.1447
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 129.0 | Training loss: 0.1323
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 101.1 | Training loss: 0.1339
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1275
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1368
Epoch: 11 |   [Val] | Loss: 0.1061 | [CCC]:  0.2985 [' 0.2985'] | PCC: 0.3107 ['0.3107'] | RMSE: 0.4278 ['0.4278']
Epoch: 11 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 140.6 | Training loss: 0.1331
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 134.6 | Training loss: 0.1308
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 121.3 | Training loss: 0.1255
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0987
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1293
Epoch: 12 |   [Val] | Loss: 0.1053 | [CCC]:  0.3314 [' 0.3314'] | PCC: 0.3340 ['0.3340'] | RMSE: 0.4970 ['0.4970']
Epoch: 12 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 132.1 | Training loss: 0.1266
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 124.9 | Training loss: 0.1166
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 99.5 | Training loss: 0.1256
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1221
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1229
Epoch: 13 |   [Val] | Loss: 0.0970 | [CCC]:  0.3318 [' 0.3318'] | PCC: 0.3318 ['0.3318'] | RMSE: 0.4760 ['0.4760']
Epoch: 13 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 132.8 | Training loss: 0.1194
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 123.3 | Training loss: 0.1218
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 102.8 | Training loss: 0.1142
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1336
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1187
Epoch: 14 |   [Val] | Loss: 0.1087 | [CCC]:  0.2886 [' 0.2886'] | PCC: 0.3360 ['0.3360'] | RMSE: 0.4961 ['0.4961']
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 135.0 | Training loss: 0.1255
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 114.6 | Training loss: 0.1207
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 98.1 | Training loss: 0.1341
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0879
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1262
Epoch: 15 |   [Val] | Loss: 0.1261 | [CCC]:  0.2527 [' 0.2527'] | PCC: 0.3426 ['0.3426'] | RMSE: 0.5633 ['0.5633']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.7 | Training loss: 0.1410
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 117.9 | Training loss: 0.1483
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.1 | Training loss: 0.1221
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1262
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.1370
Epoch: 16 |   [Val] | Loss: 0.0999 | [CCC]:  0.3179 [' 0.3179'] | PCC: 0.3446 ['0.3446'] | RMSE: 0.5620 ['0.5620']
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 122.8 | Training loss: 0.1199
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 111.5 | Training loss: 0.1370
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.6 | Training loss: 0.1333
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1036
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.1296
Epoch: 17 |   [Val] | Loss: 0.1333 | [CCC]:  0.2435 [' 0.2435'] | PCC: 0.3555 ['0.3555'] | RMSE: 0.4974 ['0.4974']
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 122.1 | Training loss: 0.1485
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 106.8 | Training loss: 0.1407
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 93.0 | Training loss: 0.1324
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1177
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.1402
Epoch: 18 |   [Val] | Loss: 0.0881 | [CCC]:  0.3759 [' 0.3759'] | PCC: 0.3776 ['0.3776'] | RMSE: 0.4591 ['0.4591']
Epoch: 18 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 118.5 | Training loss: 0.1186
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 106.8 | Training loss: 0.1284
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.4 | Training loss: 0.1246
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0956
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.1235
Epoch: 19 |   [Val] | Loss: 0.0856 | [CCC]:  0.3666 [' 0.3666'] | PCC: 0.3724 ['0.3724'] | RMSE: 0.4237 ['0.4237']
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 112.5 | Training loss: 0.1140
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 94.7 | Training loss: 0.1302
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.5 | Training loss: 0.1270
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1267
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.1238
Epoch: 20 |   [Val] | Loss: 0.0795 | [CCC]:  0.3713 [' 0.3713'] | PCC: 0.3739 ['0.3739'] | RMSE: 0.4366 ['0.4366']
Epoch: 21 | Batch:   1 | Lr: 0.00500 | Time used(s): 106.6 | Training loss: 0.1164
Epoch: 21 | Batch:   2 | Lr: 0.00500 | Time used(s): 94.7 | Training loss: 0.1150
Epoch: 21 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.5 | Training loss: 0.1249
Epoch: 21 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1044
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.1185
Epoch: 21 |   [Val] | Loss: 0.0870 | [CCC]:  0.3530 [' 0.3530'] | PCC: 0.3753 ['0.3753'] | RMSE: 0.4874 ['0.4874']
Epoch: 22 | Batch:   1 | Lr: 0.00500 | Time used(s): 103.2 | Training loss: 0.1188
Epoch: 22 | Batch:   2 | Lr: 0.00500 | Time used(s): 94.6 | Training loss: 0.1106
Epoch: 22 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.8 | Training loss: 0.1147
Epoch: 22 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0851
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.1143
Epoch: 22 |   [Val] | Loss: 0.0817 | [CCC]:  0.3742 [' 0.3742'] | PCC: 0.3787 ['0.3787'] | RMSE: 0.4552 ['0.4552']
Epoch: 23 | Batch:   1 | Lr: 0.00500 | Time used(s): 104.2 | Training loss: 0.1118
Epoch: 23 | Batch:   2 | Lr: 0.00500 | Time used(s): 92.8 | Training loss: 0.1082
Epoch: 23 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.0 | Training loss: 0.1069
Epoch: 23 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.0890
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.1086
Epoch: 23 |   [Val] | Loss: 0.0857 | [CCC]:  0.3709 [' 0.3709'] | PCC: 0.3994 ['0.3994'] | RMSE: 0.4465 ['0.4465']
Epoch: 24 | Batch:   1 | Lr: 0.00500 | Time used(s): 103.7 | Training loss: 0.1108
Epoch: 24 | Batch:   2 | Lr: 0.00500 | Time used(s): 93.2 | Training loss: 0.1059
Epoch: 24 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.7 | Training loss: 0.1063
Epoch: 24 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1185
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.1078
Epoch: 24 |   [Val] | Loss: 0.0836 | [CCC]:  0.3723 [' 0.3723'] | PCC: 0.3862 ['0.3862'] | RMSE: 0.4427 ['0.4427']
Epoch    24: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 25 | Batch:   1 | Lr: 0.00250 | Time used(s): 103.8 | Training loss: 0.1029
Epoch: 25 | Batch:   2 | Lr: 0.00250 | Time used(s): 93.3 | Training loss: 0.1049
Epoch: 25 | Batch:   3 | Lr: 0.00250 | Time used(s): 92.0 | Training loss: 0.0988
Epoch: 25 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0940
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.1021
Epoch: 25 |   [Val] | Loss: 0.0811 | [CCC]:  0.3661 [' 0.3661'] | PCC: 0.3815 ['0.3815'] | RMSE: 0.4733 ['0.4733']
Epoch: 26 | Batch:   1 | Lr: 0.00250 | Time used(s): 124.0 | Training loss: 0.1091
Epoch: 26 | Batch:   2 | Lr: 0.00250 | Time used(s): 112.9 | Training loss: 0.0983
Epoch: 26 | Batch:   3 | Lr: 0.00250 | Time used(s): 95.6 | Training loss: 0.0969
Epoch: 26 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.1049
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.1015
Epoch: 26 |   [Val] | Loss: 0.0852 | [CCC]:  0.3699 [' 0.3699'] | PCC: 0.3988 ['0.3988'] | RMSE: 0.4254 ['0.4254']
Epoch: 27 | Batch:   1 | Lr: 0.00250 | Time used(s): 102.8 | Training loss: 0.1014
Epoch: 27 | Batch:   2 | Lr: 0.00250 | Time used(s): 95.6 | Training loss: 0.0997
Epoch: 27 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.4 | Training loss: 0.1010
Epoch: 27 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.1065
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.1008
Epoch: 27 |   [Val] | Loss: 0.0754 | [CCC]:  0.3850 [' 0.3850'] | PCC: 0.3971 ['0.3971'] | RMSE: 0.4474 ['0.4474']
Epoch: 27 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch: 28 | Batch:   1 | Lr: 0.00250 | Time used(s): 105.0 | Training loss: 0.1094
Epoch: 28 | Batch:   2 | Lr: 0.00250 | Time used(s): 97.0 | Training loss: 0.0985
Epoch: 28 | Batch:   3 | Lr: 0.00250 | Time used(s): 91.9 | Training loss: 0.0887
Epoch: 28 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0641
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0983
Epoch: 28 |   [Val] | Loss: 0.0776 | [CCC]:  0.3934 [' 0.3934'] | PCC: 0.4044 ['0.4044'] | RMSE: 0.4548 ['0.4548']
Epoch: 28 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch: 29 | Batch:   1 | Lr: 0.00250 | Time used(s): 111.6 | Training loss: 0.0987
Epoch: 29 | Batch:   2 | Lr: 0.00250 | Time used(s): 102.8 | Training loss: 0.0895
Epoch: 29 | Batch:   3 | Lr: 0.00250 | Time used(s): 92.6 | Training loss: 0.0919
Epoch: 29 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0714
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0930
Epoch: 29 |   [Val] | Loss: 0.0732 | [CCC]:  0.4041 [' 0.4041'] | PCC: 0.4099 ['0.4099'] | RMSE: 0.4193 ['0.4193']
Epoch: 29 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch: 30 | Batch:   1 | Lr: 0.00250 | Time used(s): 113.2 | Training loss: 0.0898
Epoch: 30 | Batch:   2 | Lr: 0.00250 | Time used(s): 101.0 | Training loss: 0.0978
Epoch: 30 | Batch:   3 | Lr: 0.00250 | Time used(s): 91.4 | Training loss: 0.0822
Epoch: 30 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0730
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0896
Epoch: 30 |   [Val] | Loss: 0.0717 | [CCC]:  0.4126 [' 0.4126'] | PCC: 0.4153 ['0.4153'] | RMSE: 0.4451 ['0.4451']
Epoch: 30 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch: 31 | Batch:   1 | Lr: 0.00250 | Time used(s): 115.7 | Training loss: 0.0970
Epoch: 31 | Batch:   2 | Lr: 0.00250 | Time used(s): 101.1 | Training loss: 0.0909
Epoch: 31 | Batch:   3 | Lr: 0.00250 | Time used(s): 91.1 | Training loss: 0.0868
Epoch: 31 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0714
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0913
Epoch: 31 |   [Val] | Loss: 0.0723 | [CCC]:  0.4053 [' 0.4053'] | PCC: 0.4153 ['0.4153'] | RMSE: 0.4306 ['0.4306']
Epoch: 32 | Batch:   1 | Lr: 0.00250 | Time used(s): 122.2 | Training loss: 0.0805
Epoch: 32 | Batch:   2 | Lr: 0.00250 | Time used(s): 104.1 | Training loss: 0.0820
Epoch: 32 | Batch:   3 | Lr: 0.00250 | Time used(s): 91.6 | Training loss: 0.0961
Epoch: 32 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0537
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0857
Epoch: 32 |   [Val] | Loss: 0.0792 | [CCC]:  0.3933 [' 0.3933'] | PCC: 0.4061 ['0.4061'] | RMSE: 0.4313 ['0.4313']
Epoch: 33 | Batch:   1 | Lr: 0.00250 | Time used(s): 105.8 | Training loss: 0.0900
Epoch: 33 | Batch:   2 | Lr: 0.00250 | Time used(s): 91.3 | Training loss: 0.0879
Epoch: 33 | Batch:   3 | Lr: 0.00250 | Time used(s): 91.0 | Training loss: 0.0843
Epoch: 33 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0899
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0874
Epoch: 33 |   [Val] | Loss: 0.0857 | [CCC]:  0.3694 [' 0.3694'] | PCC: 0.3966 ['0.3966'] | RMSE: 0.4875 ['0.4875']
Epoch: 34 | Batch:   1 | Lr: 0.00250 | Time used(s): 98.1 | Training loss: 0.1020
Epoch: 34 | Batch:   2 | Lr: 0.00250 | Time used(s): 90.7 | Training loss: 0.0945
Epoch: 34 | Batch:   3 | Lr: 0.00250 | Time used(s): 92.3 | Training loss: 0.0853
Epoch: 34 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0930
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0939
Epoch: 34 |   [Val] | Loss: 0.0868 | [CCC]:  0.3687 [' 0.3687'] | PCC: 0.4187 ['0.4187'] | RMSE: 0.4496 ['0.4496']
Epoch: 35 | Batch:   1 | Lr: 0.00250 | Time used(s): 98.9 | Training loss: 0.0947
Epoch: 35 | Batch:   2 | Lr: 0.00250 | Time used(s): 91.4 | Training loss: 0.0775
Epoch: 35 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.8 | Training loss: 0.0854
Epoch: 35 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0802
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0858
Epoch: 35 |   [Val] | Loss: 0.0749 | [CCC]:  0.4189 [' 0.4189'] | PCC: 0.4200 ['0.4200'] | RMSE: 0.4324 ['0.4324']
Epoch: 35 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch: 36 | Batch:   1 | Lr: 0.00250 | Time used(s): 99.1 | Training loss: 0.0790
Epoch: 36 | Batch:   2 | Lr: 0.00250 | Time used(s): 90.7 | Training loss: 0.0847
Epoch: 36 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.8 | Training loss: 0.0841
Epoch: 36 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0495
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0821
Epoch: 36 |   [Val] | Loss: 0.0736 | [CCC]:  0.4133 [' 0.4133'] | PCC: 0.4144 ['0.4144'] | RMSE: 0.4281 ['0.4281']
Epoch: 37 | Batch:   1 | Lr: 0.00250 | Time used(s): 97.5 | Training loss: 0.0820
Epoch: 37 | Batch:   2 | Lr: 0.00250 | Time used(s): 91.2 | Training loss: 0.0887
Epoch: 37 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.3 | Training loss: 0.0780
Epoch: 37 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0756
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0828
Epoch: 37 |   [Val] | Loss: 0.0802 | [CCC]:  0.3892 [' 0.3892'] | PCC: 0.4117 ['0.4117'] | RMSE: 0.4282 ['0.4282']
Epoch: 38 | Batch:   1 | Lr: 0.00250 | Time used(s): 97.0 | Training loss: 0.0863
Epoch: 38 | Batch:   2 | Lr: 0.00250 | Time used(s): 91.5 | Training loss: 0.0750
Epoch: 38 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.2 | Training loss: 0.0739
Epoch: 38 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0530
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0780
Epoch: 38 |   [Val] | Loss: 0.0725 | [CCC]:  0.4097 [' 0.4097'] | PCC: 0.4117 ['0.4117'] | RMSE: 0.4307 ['0.4307']
Epoch: 39 | Batch:   1 | Lr: 0.00250 | Time used(s): 129.4 | Training loss: 0.0749
Epoch: 39 | Batch:   2 | Lr: 0.00250 | Time used(s): 102.8 | Training loss: 0.0782
Epoch: 39 | Batch:   3 | Lr: 0.00250 | Time used(s): 90.5 | Training loss: 0.0784
Epoch: 39 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0579
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0768
Epoch: 39 |   [Val] | Loss: 0.0770 | [CCC]:  0.4010 [' 0.4010'] | PCC: 0.4045 ['0.4045'] | RMSE: 0.4468 ['0.4468']
Epoch: 40 | Batch:   1 | Lr: 0.00250 | Time used(s): 116.1 | Training loss: 0.0714
Epoch: 40 | Batch:   2 | Lr: 0.00250 | Time used(s): 104.4 | Training loss: 0.0751
Epoch: 40 | Batch:   3 | Lr: 0.00250 | Time used(s): 91.3 | Training loss: 0.0807
Epoch: 40 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0696
--------------------------------------------------
Epoch: 40 | [Train] | Loss: 0.0756
Epoch: 40 |   [Val] | Loss: 0.0741 | [CCC]:  0.4132 [' 0.4132'] | PCC: 0.4205 ['0.4205'] | RMSE: 0.4291 ['0.4291']
Epoch: 41 | Batch:   1 | Lr: 0.00250 | Time used(s): 137.5 | Training loss: 0.0735
Epoch: 41 | Batch:   2 | Lr: 0.00250 | Time used(s): 134.4 | Training loss: 0.0688
Epoch: 41 | Batch:   3 | Lr: 0.00250 | Time used(s): 115.1 | Training loss: 0.0721
Epoch: 41 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0337
--------------------------------------------------
Epoch: 41 | [Train] | Loss: 0.0709
Epoch: 41 |   [Val] | Loss: 0.0720 | [CCC]:  0.4189 [' 0.4189'] | PCC: 0.4259 ['0.4259'] | RMSE: 0.4227 ['0.4227']
Epoch: 41 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch    41: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 42 | Batch:   1 | Lr: 0.00125 | Time used(s): 135.8 | Training loss: 0.0741
Epoch: 42 | Batch:   2 | Lr: 0.00125 | Time used(s): 133.1 | Training loss: 0.0634
Epoch: 42 | Batch:   3 | Lr: 0.00125 | Time used(s): 115.7 | Training loss: 0.0728
Epoch: 42 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0818
--------------------------------------------------
Epoch: 42 | [Train] | Loss: 0.0703
Epoch: 42 |   [Val] | Loss: 0.0758 | [CCC]:  0.4142 [' 0.4142'] | PCC: 0.4213 ['0.4213'] | RMSE: 0.4397 ['0.4397']
Epoch: 43 | Batch:   1 | Lr: 0.00125 | Time used(s): 136.1 | Training loss: 0.0675
Epoch: 43 | Batch:   2 | Lr: 0.00125 | Time used(s): 130.9 | Training loss: 0.0721
Epoch: 43 | Batch:   3 | Lr: 0.00125 | Time used(s): 112.2 | Training loss: 0.0615
Epoch: 43 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.1052
--------------------------------------------------
Epoch: 43 | [Train] | Loss: 0.0676
Epoch: 43 |   [Val] | Loss: 0.0756 | [CCC]:  0.4032 [' 0.4032'] | PCC: 0.4038 ['0.4038'] | RMSE: 0.4395 ['0.4395']
Epoch: 44 | Batch:   1 | Lr: 0.00125 | Time used(s): 133.8 | Training loss: 0.0669
Epoch: 44 | Batch:   2 | Lr: 0.00125 | Time used(s): 134.2 | Training loss: 0.0685
Epoch: 44 | Batch:   3 | Lr: 0.00125 | Time used(s): 109.4 | Training loss: 0.0610
Epoch: 44 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0748
--------------------------------------------------
Epoch: 44 | [Train] | Loss: 0.0656
Epoch: 44 |   [Val] | Loss: 0.0734 | [CCC]:  0.4112 [' 0.4112'] | PCC: 0.4126 ['0.4126'] | RMSE: 0.4260 ['0.4260']
Epoch: 45 | Batch:   1 | Lr: 0.00125 | Time used(s): 136.5 | Training loss: 0.0621
Epoch: 45 | Batch:   2 | Lr: 0.00125 | Time used(s): 117.2 | Training loss: 0.0667
Epoch: 45 | Batch:   3 | Lr: 0.00125 | Time used(s): 91.5 | Training loss: 0.0663
Epoch: 45 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0797
--------------------------------------------------
Epoch: 45 | [Train] | Loss: 0.0652
Epoch: 45 |   [Val] | Loss: 0.0726 | [CCC]:  0.4150 [' 0.4150'] | PCC: 0.4191 ['0.4191'] | RMSE: 0.4258 ['0.4258']
Epoch: 46 | Batch:   1 | Lr: 0.00125 | Time used(s): 112.1 | Training loss: 0.0638
Epoch: 46 | Batch:   2 | Lr: 0.00125 | Time used(s): 105.3 | Training loss: 0.0610
Epoch: 46 | Batch:   3 | Lr: 0.00125 | Time used(s): 93.7 | Training loss: 0.0598
Epoch: 46 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0160
--------------------------------------------------
Epoch: 46 | [Train] | Loss: 0.0608
Epoch: 46 |   [Val] | Loss: 0.0729 | [CCC]:  0.4154 [' 0.4154'] | PCC: 0.4188 ['0.4188'] | RMSE: 0.4326 ['0.4326']
Epoch: 47 | Batch:   1 | Lr: 0.00125 | Time used(s): 127.5 | Training loss: 0.0563
Epoch: 47 | Batch:   2 | Lr: 0.00125 | Time used(s): 113.4 | Training loss: 0.0572
Epoch: 47 | Batch:   3 | Lr: 0.00125 | Time used(s): 91.9 | Training loss: 0.0621
Epoch: 47 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0870
--------------------------------------------------
Epoch: 47 | [Train] | Loss: 0.0590
Epoch: 47 |   [Val] | Loss: 0.0739 | [CCC]:  0.4202 [' 0.4202'] | PCC: 0.4205 ['0.4205'] | RMSE: 0.4473 ['0.4473']
Epoch: 47 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_330_None_None].pth"!
Epoch: 48 | Batch:   1 | Lr: 0.00125 | Time used(s): 122.2 | Training loss: 0.0663
Epoch: 48 | Batch:   2 | Lr: 0.00125 | Time used(s): 114.7 | Training loss: 0.0590
Epoch: 48 | Batch:   3 | Lr: 0.00125 | Time used(s): 91.7 | Training loss: 0.0553
Epoch: 48 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0455
--------------------------------------------------
Epoch: 48 | [Train] | Loss: 0.0600
Epoch: 48 |   [Val] | Loss: 0.0724 | [CCC]:  0.4139 [' 0.4139'] | PCC: 0.4158 ['0.4158'] | RMSE: 0.4279 ['0.4279']
Epoch: 49 | Batch:   1 | Lr: 0.00125 | Time used(s): 126.9 | Training loss: 0.0574
Epoch: 49 | Batch:   2 | Lr: 0.00125 | Time used(s): 113.2 | Training loss: 0.0626
Epoch: 49 | Batch:   3 | Lr: 0.00125 | Time used(s): 91.0 | Training loss: 0.0482
Epoch: 49 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0697
--------------------------------------------------
Epoch: 49 | [Train] | Loss: 0.0563
Epoch: 49 |   [Val] | Loss: 0.0762 | [CCC]:  0.4022 [' 0.4022'] | PCC: 0.4126 ['0.4126'] | RMSE: 0.4307 ['0.4307']
Epoch: 50 | Batch:   1 | Lr: 0.00125 | Time used(s): 109.2 | Training loss: 0.0633
Epoch: 50 | Batch:   2 | Lr: 0.00125 | Time used(s): 99.1 | Training loss: 0.0532
Epoch: 50 | Batch:   3 | Lr: 0.00125 | Time used(s): 91.7 | Training loss: 0.0486
Epoch: 50 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0602
--------------------------------------------------
Epoch: 50 | [Train] | Loss: 0.0551
Epoch: 50 |   [Val] | Loss: 0.0736 | [CCC]:  0.4138 [' 0.4138'] | PCC: 0.4141 ['0.4141'] | RMSE: 0.4508 ['0.4508']
Epoch: 51 | Batch:   1 | Lr: 0.00125 | Time used(s): 109.8 | Training loss: 0.0493
Epoch: 51 | Batch:   2 | Lr: 0.00125 | Time used(s): 101.2 | Training loss: 0.0547
Epoch: 51 | Batch:   3 | Lr: 0.00125 | Time used(s): 91.8 | Training loss: 0.0580
Epoch: 51 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0274
--------------------------------------------------
Epoch: 51 | [Train] | Loss: 0.0536
Epoch: 51 |   [Val] | Loss: 0.0718 | [CCC]:  0.4120 [' 0.4120'] | PCC: 0.4141 ['0.4141'] | RMSE: 0.4283 ['0.4283']
Epoch: 52 | Batch:   1 | Lr: 0.00125 | Time used(s): 123.0 | Training loss: 0.0526
Epoch: 52 | Batch:   2 | Lr: 0.00125 | Time used(s): 107.5 | Training loss: 0.0480
Epoch: 52 | Batch:   3 | Lr: 0.00125 | Time used(s): 91.7 | Training loss: 0.0487
Epoch: 52 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0921
--------------------------------------------------
Epoch: 52 | [Train] | Loss: 0.0505
Epoch: 52 |   [Val] | Loss: 0.0747 | [CCC]:  0.4116 [' 0.4116'] | PCC: 0.4151 ['0.4151'] | RMSE: 0.4441 ['0.4441']
Epoch: 53 | Batch:   1 | Lr: 0.00125 | Time used(s): 116.6 | Training loss: 0.0579
Epoch: 53 | Batch:   2 | Lr: 0.00125 | Time used(s): 100.1 | Training loss: 0.0419
Epoch: 53 | Batch:   3 | Lr: 0.00125 | Time used(s): 90.7 | Training loss: 0.0501
Epoch: 53 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.1 | Training loss: 0.0268
--------------------------------------------------
Epoch: 53 | [Train] | Loss: 0.0496
Epoch: 53 |   [Val] | Loss: 0.0737 | [CCC]:  0.4050 [' 0.4050'] | PCC: 0.4094 ['0.4094'] | RMSE: 0.4274 ['0.4274']
Epoch    53: reducing learning rate of group 0 to 6.2500e-04.
Epoch: 54 | Batch:   1 | Lr: 0.00063 | Time used(s): 122.6 | Training loss: 0.0457
Epoch: 54 | Batch:   2 | Lr: 0.00063 | Time used(s): 108.7 | Training loss: 0.0569
Epoch: 54 | Batch:   3 | Lr: 0.00063 | Time used(s): 90.9 | Training loss: 0.0400
Epoch: 54 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.1 | Training loss: 0.0506
--------------------------------------------------
Epoch: 54 | [Train] | Loss: 0.0476
Epoch: 54 |   [Val] | Loss: 0.0753 | [CCC]:  0.4040 [' 0.4040'] | PCC: 0.4138 ['0.4138'] | RMSE: 0.4280 ['0.4280']
Epoch: 55 | Batch:   1 | Lr: 0.00063 | Time used(s): 119.6 | Training loss: 0.0465
Epoch: 55 | Batch:   2 | Lr: 0.00063 | Time used(s): 106.3 | Training loss: 0.0441
Epoch: 55 | Batch:   3 | Lr: 0.00063 | Time used(s): 91.6 | Training loss: 0.0507
Epoch: 55 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.2 | Training loss: 0.0377
--------------------------------------------------
Epoch: 55 | [Train] | Loss: 0.0470
Epoch: 55 |   [Val] | Loss: 0.0719 | [CCC]:  0.4130 [' 0.4130'] | PCC: 0.4139 ['0.4139'] | RMSE: 0.4376 ['0.4376']
Epoch: 56 | Batch:   1 | Lr: 0.00063 | Time used(s): 117.7 | Training loss: 0.0373
Epoch: 56 | Batch:   2 | Lr: 0.00063 | Time used(s): 107.4 | Training loss: 0.0481
Epoch: 56 | Batch:   3 | Lr: 0.00063 | Time used(s): 91.9 | Training loss: 0.0499
Epoch: 56 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.1 | Training loss: 0.0305
--------------------------------------------------
Epoch: 56 | [Train] | Loss: 0.0449
Epoch: 56 |   [Val] | Loss: 0.0723 | [CCC]:  0.4057 [' 0.4057'] | PCC: 0.4108 ['0.4108'] | RMSE: 0.4282 ['0.4282']
Epoch: 57 | Batch:   1 | Lr: 0.00063 | Time used(s): 125.3 | Training loss: 0.0454
Epoch: 57 | Batch:   2 | Lr: 0.00063 | Time used(s): 111.2 | Training loss: 0.0432
Epoch: 57 | Batch:   3 | Lr: 0.00063 | Time used(s): 94.5 | Training loss: 0.0411
Epoch: 57 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.2 | Training loss: 0.0765
--------------------------------------------------
Epoch: 57 | [Train] | Loss: 0.0438
Epoch: 57 |   [Val] | Loss: 0.0723 | [CCC]:  0.4092 [' 0.4092'] | PCC: 0.4104 ['0.4104'] | RMSE: 0.4332 ['0.4332']
Epoch: 58 | Batch:   1 | Lr: 0.00063 | Time used(s): 127.6 | Training loss: 0.0373
Epoch: 58 | Batch:   2 | Lr: 0.00063 | Time used(s): 114.4 | Training loss: 0.0411
Epoch: 58 | Batch:   3 | Lr: 0.00063 | Time used(s): 94.1 | Training loss: 0.0469
Epoch: 58 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.2 | Training loss: 0.0321
--------------------------------------------------
Epoch: 58 | [Train] | Loss: 0.0416
Epoch: 58 |   [Val] | Loss: 0.0775 | [CCC]:  0.4041 [' 0.4041'] | PCC: 0.4106 ['0.4106'] | RMSE: 0.4396 ['0.4396']
Epoch: 59 | Batch:   1 | Lr: 0.00063 | Time used(s): 119.6 | Training loss: 0.0416
Epoch: 59 | Batch:   2 | Lr: 0.00063 | Time used(s): 111.0 | Training loss: 0.0337
Epoch: 59 | Batch:   3 | Lr: 0.00063 | Time used(s): 92.9 | Training loss: 0.0526
Epoch: 59 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.2 | Training loss: 0.0119
--------------------------------------------------
Epoch: 59 | [Train] | Loss: 0.0422
Epoch: 59 |   [Val] | Loss: 0.0736 | [CCC]:  0.4089 [' 0.4089'] | PCC: 0.4094 ['0.4094'] | RMSE: 0.4342 ['0.4342']
Epoch    59: reducing learning rate of group 0 to 3.1250e-04.
Epoch: 60 | Batch:   1 | Lr: 0.00031 | Time used(s): 115.5 | Training loss: 0.0290
Epoch: 60 | Batch:   2 | Lr: 0.00031 | Time used(s): 102.3 | Training loss: 0.0472
Epoch: 60 | Batch:   3 | Lr: 0.00031 | Time used(s): 92.2 | Training loss: 0.0454
Epoch: 60 | Batch:   4 | Lr: 0.00031 | Time used(s): 2.1 | Training loss: 0.0427
--------------------------------------------------
Epoch: 60 | [Train] | Loss: 0.0406
Epoch: 60 |   [Val] | Loss: 0.0770 | [CCC]:  0.4050 [' 0.4050'] | PCC: 0.4088 ['0.4088'] | RMSE: 0.4336 ['0.4336']
Epoch: 61 | Batch:   1 | Lr: 0.00031 | Time used(s): 104.6 | Training loss: 0.0372
Epoch: 61 | Batch:   2 | Lr: 0.00031 | Time used(s): 99.8 | Training loss: 0.0321
Epoch: 61 | Batch:   3 | Lr: 0.00031 | Time used(s): 91.1 | Training loss: 0.0452
Epoch: 61 | Batch:   4 | Lr: 0.00031 | Time used(s): 2.2 | Training loss: 0.0830
--------------------------------------------------
Epoch: 61 | [Train] | Loss: 0.0389
Epoch: 61 |   [Val] | Loss: 0.0757 | [CCC]:  0.4079 [' 0.4079'] | PCC: 0.4088 ['0.4088'] | RMSE: 0.4378 ['0.4378']
Epoch: 62 | Batch:   1 | Lr: 0.00031 | Time used(s): 112.8 | Training loss: 0.0469
Epoch: 62 | Batch:   2 | Lr: 0.00031 | Time used(s): 97.6 | Training loss: 0.0325
Epoch: 62 | Batch:   3 | Lr: 0.00031 | Time used(s): 92.9 | Training loss: 0.0370
Epoch: 62 | Batch:   4 | Lr: 0.00031 | Time used(s): 2.1 | Training loss: 0.0055
--------------------------------------------------
Epoch: 62 | [Train] | Loss: 0.0383
Epoch: 62 |   [Val] | Loss: 0.0750 | [CCC]:  0.4055 [' 0.4055'] | PCC: 0.4062 ['0.4062'] | RMSE: 0.4331 ['0.4331']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 330 | Best [Val CCC]: 0.4202 [' 0.4202']| Loss: 0.0739 | PCC: 0.4205 ['0.4205'] | RMSE: 0.4473 ['0.4473']
On Test: CCC  0.5416 | PCC  0.5502 | RMSE  0.4037
****************************************************************************************************
Seed "330" over!
****************************************************************************************************
****************************************************************************************************
Using seed "331"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 331]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 104.9 | Training loss: 0.2500
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 94.6 | Training loss: 0.2456
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 90.2 | Training loss: 0.2367
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2258
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2438
Epoch:  1 |   [Val] | Loss: 0.1831 | [CCC]:  0.1583 [' 0.1583'] | PCC: 0.1951 ['0.1951'] | RMSE: 0.5302 ['0.5302']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_331_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 114.6 | Training loss: 0.2029
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 103.1 | Training loss: 0.2108
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 91.7 | Training loss: 0.2302
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2229
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2148
Epoch:  2 |   [Val] | Loss: 0.1478 | [CCC]:  0.2277 [' 0.2277'] | PCC: 0.2532 ['0.2532'] | RMSE: 0.4448 ['0.4448']
Epoch:  2 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_331_None_None].pth"!
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 113.8 | Training loss: 0.1920
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 98.7 | Training loss: 0.1888
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 92.1 | Training loss: 0.1844
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1737
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.1882
Epoch:  3 |   [Val] | Loss: 0.1205 | [CCC]:  0.2873 [' 0.2873'] | PCC: 0.2883 ['0.2883'] | RMSE: 0.5023 ['0.5023']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_331_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 113.0 | Training loss: 0.1557
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 100.2 | Training loss: 0.1731
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.9 | Training loss: 0.1933
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1532
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1737
Epoch:  4 |   [Val] | Loss: 0.1242 | [CCC]:  0.2544 [' 0.2544'] | PCC: 0.3029 ['0.3029'] | RMSE: 0.5084 ['0.5084']
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 116.1 | Training loss: 0.1677
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 103.7 | Training loss: 0.1540
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 96.0 | Training loss: 0.1562
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1371
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1589
Epoch:  5 |   [Val] | Loss: 0.1028 | [CCC]:  0.3123 [' 0.3123'] | PCC: 0.3206 ['0.3206'] | RMSE: 0.5438 ['0.5438']
Epoch:  5 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_331_None_None].pth"!
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 120.8 | Training loss: 0.1468
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 105.7 | Training loss: 0.1464
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 94.9 | Training loss: 0.1455
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1714
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1466
Epoch:  6 |   [Val] | Loss: 0.1220 | [CCC]:  0.2541 [' 0.2541'] | PCC: 0.3198 ['0.3198'] | RMSE: 0.4735 ['0.4735']
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 141.1 | Training loss: 0.1514
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 136.7 | Training loss: 0.1435
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 131.3 | Training loss: 0.1449
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1483
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1466
Epoch:  7 |   [Val] | Loss: 0.1081 | [CCC]:  0.2987 [' 0.2987'] | PCC: 0.3210 ['0.3210'] | RMSE: 0.4953 ['0.4953']
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 142.8 | Training loss: 0.1368
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 137.1 | Training loss: 0.1342
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 138.4 | Training loss: 0.1476
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.5 | Training loss: 0.1115
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1391
Epoch:  8 |   [Val] | Loss: 0.0911 | [CCC]:  0.3504 [' 0.3504'] | PCC: 0.3516 ['0.3516'] | RMSE: 0.4499 ['0.4499']
Epoch:  8 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_331_None_None].pth"!
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 138.3 | Training loss: 0.1198
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 137.9 | Training loss: 0.1308
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 137.4 | Training loss: 0.1280
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.7 | Training loss: 0.1460
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1265
Epoch:  9 |   [Val] | Loss: 0.0873 | [CCC]:  0.3515 [' 0.3515'] | PCC: 0.3634 ['0.3634'] | RMSE: 0.4861 ['0.4861']
Epoch:  9 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_331_None_None].pth"!
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 138.3 | Training loss: 0.1224
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 129.6 | Training loss: 0.1223
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 139.5 | Training loss: 0.1240
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.5 | Training loss: 0.1045
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1226
Epoch: 10 |   [Val] | Loss: 0.0897 | [CCC]:  0.3618 [' 0.3618'] | PCC: 0.3692 ['0.3692'] | RMSE: 0.4456 ['0.4456']
Epoch: 10 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_331_None_None].pth"!
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 141.2 | Training loss: 0.1244
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 142.0 | Training loss: 0.1219
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 137.5 | Training loss: 0.1288
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.5 | Training loss: 0.1372
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1252
Epoch: 11 |   [Val] | Loss: 0.1066 | [CCC]:  0.3078 [' 0.3078'] | PCC: 0.3808 ['0.3808'] | RMSE: 0.5571 ['0.5571']
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 141.6 | Training loss: 0.1425
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 135.6 | Training loss: 0.1179
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 133.4 | Training loss: 0.1162
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1268
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1255
Epoch: 12 |   [Val] | Loss: 0.0898 | [CCC]:  0.3437 [' 0.3437'] | PCC: 0.3650 ['0.3650'] | RMSE: 0.4791 ['0.4791']
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 138.1 | Training loss: 0.1296
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 141.7 | Training loss: 0.1221
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 138.2 | Training loss: 0.1223
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1244
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1247
Epoch: 13 |   [Val] | Loss: 0.0774 | [CCC]:  0.4090 [' 0.4090'] | PCC: 0.4134 ['0.4134'] | RMSE: 0.4649 ['0.4649']
Epoch: 13 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_331_None_None].pth"!
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 137.4 | Training loss: 0.1112
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 131.8 | Training loss: 0.1012
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 139.0 | Training loss: 0.1057
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.0970
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1059
Epoch: 14 |   [Val] | Loss: 0.1099 | [CCC]:  0.2709 [' 0.2709'] | PCC: 0.4004 ['0.4004'] | RMSE: 0.4806 ['0.4806']
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 137.7 | Training loss: 0.1441
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 126.4 | Training loss: 0.1180
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 132.3 | Training loss: 0.1210
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1654
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1283
Epoch: 15 |   [Val] | Loss: 0.0879 | [CCC]:  0.3544 [' 0.3544'] | PCC: 0.3786 ['0.3786'] | RMSE: 0.4955 ['0.4955']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.1 | Training loss: 0.1205
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 128.9 | Training loss: 0.1090
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 130.0 | Training loss: 0.1250
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0643
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.1173
Epoch: 16 |   [Val] | Loss: 0.0849 | [CCC]:  0.3666 [' 0.3666'] | PCC: 0.4024 ['0.4024'] | RMSE: 0.4723 ['0.4723']
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.4 | Training loss: 0.1076
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 131.1 | Training loss: 0.1040
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 131.6 | Training loss: 0.1138
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1274
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.1088
Epoch: 17 |   [Val] | Loss: 0.0840 | [CCC]:  0.3665 [' 0.3665'] | PCC: 0.3859 ['0.3859'] | RMSE: 0.5037 ['0.5037']
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 140.1 | Training loss: 0.1079
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 133.6 | Training loss: 0.1138
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 124.5 | Training loss: 0.1018
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0752
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.1073
Epoch: 18 |   [Val] | Loss: 0.0839 | [CCC]:  0.3554 [' 0.3554'] | PCC: 0.4059 ['0.4059'] | RMSE: 0.4412 ['0.4412']
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 137.4 | Training loss: 0.1084
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 135.7 | Training loss: 0.0972
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 124.2 | Training loss: 0.1006
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0904
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.1019
Epoch: 19 |   [Val] | Loss: 0.0732 | [CCC]:  0.4173 [' 0.4173'] | PCC: 0.4192 ['0.4192'] | RMSE: 0.4631 ['0.4631']
Epoch: 19 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_331_None_None].pth"!
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.6 | Training loss: 0.0893
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 135.7 | Training loss: 0.0988
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 134.2 | Training loss: 0.0916
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0865
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0931
Epoch: 20 |   [Val] | Loss: 0.0763 | [CCC]:  0.3913 [' 0.3913'] | PCC: 0.4168 ['0.4168'] | RMSE: 0.4468 ['0.4468']
Epoch: 21 | Batch:   1 | Lr: 0.00500 | Time used(s): 135.3 | Training loss: 0.0965
Epoch: 21 | Batch:   2 | Lr: 0.00500 | Time used(s): 135.3 | Training loss: 0.0928
Epoch: 21 | Batch:   3 | Lr: 0.00500 | Time used(s): 125.1 | Training loss: 0.0882
Epoch: 21 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0554
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0919
Epoch: 21 |   [Val] | Loss: 0.0823 | [CCC]:  0.3872 [' 0.3872'] | PCC: 0.4096 ['0.4096'] | RMSE: 0.4806 ['0.4806']
Epoch: 22 | Batch:   1 | Lr: 0.00500 | Time used(s): 135.2 | Training loss: 0.1003
Epoch: 22 | Batch:   2 | Lr: 0.00500 | Time used(s): 135.5 | Training loss: 0.0862
Epoch: 22 | Batch:   3 | Lr: 0.00500 | Time used(s): 128.2 | Training loss: 0.0975
Epoch: 22 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0794
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0944
Epoch: 22 |   [Val] | Loss: 0.0769 | [CCC]:  0.3762 [' 0.3762'] | PCC: 0.4031 ['0.4031'] | RMSE: 0.4131 ['0.4131']
Epoch: 23 | Batch:   1 | Lr: 0.00500 | Time used(s): 135.6 | Training loss: 0.0877
Epoch: 23 | Batch:   2 | Lr: 0.00500 | Time used(s): 132.0 | Training loss: 0.1073
Epoch: 23 | Batch:   3 | Lr: 0.00500 | Time used(s): 125.3 | Training loss: 0.0826
Epoch: 23 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1061
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0927
Epoch: 23 |   [Val] | Loss: 0.0889 | [CCC]:  0.3567 [' 0.3567'] | PCC: 0.4145 ['0.4145'] | RMSE: 0.5064 ['0.5064']
Epoch: 24 | Batch:   1 | Lr: 0.00500 | Time used(s): 138.3 | Training loss: 0.1089
Epoch: 24 | Batch:   2 | Lr: 0.00500 | Time used(s): 134.6 | Training loss: 0.0874
Epoch: 24 | Batch:   3 | Lr: 0.00500 | Time used(s): 127.6 | Training loss: 0.0924
Epoch: 24 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0915
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0962
Epoch: 24 |   [Val] | Loss: 0.0691 | [CCC]:  0.4178 [' 0.4178'] | PCC: 0.4360 ['0.4360'] | RMSE: 0.4124 ['0.4124']
Epoch: 24 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_331_None_None].pth"!
Epoch: 25 | Batch:   1 | Lr: 0.00500 | Time used(s): 137.2 | Training loss: 0.0850
Epoch: 25 | Batch:   2 | Lr: 0.00500 | Time used(s): 136.7 | Training loss: 0.0949
Epoch: 25 | Batch:   3 | Lr: 0.00500 | Time used(s): 115.4 | Training loss: 0.0902
Epoch: 25 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0360
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0892
Epoch: 25 |   [Val] | Loss: 0.0917 | [CCC]:  0.3634 [' 0.3634'] | PCC: 0.3896 ['0.3896'] | RMSE: 0.5201 ['0.5201']
Epoch: 26 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.8 | Training loss: 0.1009
Epoch: 26 | Batch:   2 | Lr: 0.00500 | Time used(s): 134.3 | Training loss: 0.0970
Epoch: 26 | Batch:   3 | Lr: 0.00500 | Time used(s): 129.7 | Training loss: 0.0846
Epoch: 26 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0838
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0940
Epoch: 26 |   [Val] | Loss: 0.0802 | [CCC]:  0.3817 [' 0.3817'] | PCC: 0.4198 ['0.4198'] | RMSE: 0.4113 ['0.4113']
Epoch: 27 | Batch:   1 | Lr: 0.00500 | Time used(s): 133.2 | Training loss: 0.0912
Epoch: 27 | Batch:   2 | Lr: 0.00500 | Time used(s): 134.1 | Training loss: 0.0817
Epoch: 27 | Batch:   3 | Lr: 0.00500 | Time used(s): 129.1 | Training loss: 0.0852
Epoch: 27 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0983
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0862
Epoch: 27 |   [Val] | Loss: 0.0672 | [CCC]:  0.4261 [' 0.4261'] | PCC: 0.4321 ['0.4321'] | RMSE: 0.4546 ['0.4546']
Epoch: 27 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_331_None_None].pth"!
Epoch: 28 | Batch:   1 | Lr: 0.00500 | Time used(s): 135.6 | Training loss: 0.0834
Epoch: 28 | Batch:   2 | Lr: 0.00500 | Time used(s): 134.8 | Training loss: 0.0769
Epoch: 28 | Batch:   3 | Lr: 0.00500 | Time used(s): 137.5 | Training loss: 0.0862
Epoch: 28 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0672
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0819
Epoch: 28 |   [Val] | Loss: 0.0695 | [CCC]:  0.4267 [' 0.4267'] | PCC: 0.4305 ['0.4305'] | RMSE: 0.4311 ['0.4311']
Epoch: 28 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_331_None_None].pth"!
Epoch: 29 | Batch:   1 | Lr: 0.00500 | Time used(s): 130.4 | Training loss: 0.0691
Epoch: 29 | Batch:   2 | Lr: 0.00500 | Time used(s): 133.6 | Training loss: 0.0788
Epoch: 29 | Batch:   3 | Lr: 0.00500 | Time used(s): 125.5 | Training loss: 0.0813
Epoch: 29 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0741
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0764
Epoch: 29 |   [Val] | Loss: 0.0739 | [CCC]:  0.4161 [' 0.4161'] | PCC: 0.4259 ['0.4259'] | RMSE: 0.4386 ['0.4386']
Epoch: 30 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.1 | Training loss: 0.0689
Epoch: 30 | Batch:   2 | Lr: 0.00500 | Time used(s): 137.6 | Training loss: 0.0758
Epoch: 30 | Batch:   3 | Lr: 0.00500 | Time used(s): 129.5 | Training loss: 0.0755
Epoch: 30 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0518
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0731
Epoch: 30 |   [Val] | Loss: 0.0827 | [CCC]:  0.3883 [' 0.3883'] | PCC: 0.4183 ['0.4183'] | RMSE: 0.4644 ['0.4644']
Epoch: 31 | Batch:   1 | Lr: 0.00500 | Time used(s): 137.6 | Training loss: 0.0820
Epoch: 31 | Batch:   2 | Lr: 0.00500 | Time used(s): 133.5 | Training loss: 0.0760
Epoch: 31 | Batch:   3 | Lr: 0.00500 | Time used(s): 131.9 | Training loss: 0.0696
Epoch: 31 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0466
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0754
Epoch: 31 |   [Val] | Loss: 0.0851 | [CCC]:  0.4062 [' 0.4062'] | PCC: 0.4202 ['0.4202'] | RMSE: 0.4102 ['0.4102']
Epoch: 32 | Batch:   1 | Lr: 0.00500 | Time used(s): 132.1 | Training loss: 0.0790
Epoch: 32 | Batch:   2 | Lr: 0.00500 | Time used(s): 127.0 | Training loss: 0.0711
Epoch: 32 | Batch:   3 | Lr: 0.00500 | Time used(s): 125.9 | Training loss: 0.0716
Epoch: 32 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0611
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0737
Epoch: 32 |   [Val] | Loss: 0.0714 | [CCC]:  0.4030 [' 0.4030'] | PCC: 0.4037 ['0.4037'] | RMSE: 0.4338 ['0.4338']
Epoch: 33 | Batch:   1 | Lr: 0.00500 | Time used(s): 137.1 | Training loss: 0.0629
Epoch: 33 | Batch:   2 | Lr: 0.00500 | Time used(s): 132.9 | Training loss: 0.0739
Epoch: 33 | Batch:   3 | Lr: 0.00500 | Time used(s): 122.9 | Training loss: 0.0777
Epoch: 33 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0915
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0718
Epoch: 33 |   [Val] | Loss: 0.0747 | [CCC]:  0.4103 [' 0.4103'] | PCC: 0.4114 ['0.4114'] | RMSE: 0.4313 ['0.4313']
Epoch: 34 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.6 | Training loss: 0.0666
Epoch: 34 | Batch:   2 | Lr: 0.00500 | Time used(s): 134.9 | Training loss: 0.0701
Epoch: 34 | Batch:   3 | Lr: 0.00500 | Time used(s): 124.8 | Training loss: 0.0632
Epoch: 34 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0843
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0669
Epoch: 34 |   [Val] | Loss: 0.0945 | [CCC]:  0.3968 [' 0.3968'] | PCC: 0.4227 ['0.4227'] | RMSE: 0.4767 ['0.4767']
Epoch    34: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 35 | Batch:   1 | Lr: 0.00250 | Time used(s): 134.2 | Training loss: 0.0774
Epoch: 35 | Batch:   2 | Lr: 0.00250 | Time used(s): 132.3 | Training loss: 0.0726
Epoch: 35 | Batch:   3 | Lr: 0.00250 | Time used(s): 119.6 | Training loss: 0.0601
Epoch: 35 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0790
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0702
Epoch: 35 |   [Val] | Loss: 0.0708 | [CCC]:  0.3945 [' 0.3945'] | PCC: 0.4128 ['0.4128'] | RMSE: 0.4076 ['0.4076']
Epoch: 36 | Batch:   1 | Lr: 0.00250 | Time used(s): 133.6 | Training loss: 0.0646
Epoch: 36 | Batch:   2 | Lr: 0.00250 | Time used(s): 130.3 | Training loss: 0.0733
Epoch: 36 | Batch:   3 | Lr: 0.00250 | Time used(s): 120.4 | Training loss: 0.0580
Epoch: 36 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0835
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0656
Epoch: 36 |   [Val] | Loss: 0.0872 | [CCC]:  0.3757 [' 0.3757'] | PCC: 0.3960 ['0.3960'] | RMSE: 0.4609 ['0.4609']
Epoch: 37 | Batch:   1 | Lr: 0.00250 | Time used(s): 130.9 | Training loss: 0.0603
Epoch: 37 | Batch:   2 | Lr: 0.00250 | Time used(s): 129.4 | Training loss: 0.0679
Epoch: 37 | Batch:   3 | Lr: 0.00250 | Time used(s): 120.6 | Training loss: 0.0577
Epoch: 37 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0770
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0622
Epoch: 37 |   [Val] | Loss: 0.0724 | [CCC]:  0.3934 [' 0.3934'] | PCC: 0.3973 ['0.3973'] | RMSE: 0.4348 ['0.4348']
Epoch: 38 | Batch:   1 | Lr: 0.00250 | Time used(s): 138.4 | Training loss: 0.0556
Epoch: 38 | Batch:   2 | Lr: 0.00250 | Time used(s): 131.6 | Training loss: 0.0532
Epoch: 38 | Batch:   3 | Lr: 0.00250 | Time used(s): 121.7 | Training loss: 0.0613
Epoch: 38 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0743
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0570
Epoch: 38 |   [Val] | Loss: 0.0742 | [CCC]:  0.4106 [' 0.4106'] | PCC: 0.4124 ['0.4124'] | RMSE: 0.4248 ['0.4248']
Epoch: 39 | Batch:   1 | Lr: 0.00250 | Time used(s): 132.9 | Training loss: 0.0489
Epoch: 39 | Batch:   2 | Lr: 0.00250 | Time used(s): 133.5 | Training loss: 0.0562
Epoch: 39 | Batch:   3 | Lr: 0.00250 | Time used(s): 130.0 | Training loss: 0.0456
Epoch: 39 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.1139
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0513
Epoch: 39 |   [Val] | Loss: 0.0752 | [CCC]:  0.4113 [' 0.4113'] | PCC: 0.4236 ['0.4236'] | RMSE: 0.4328 ['0.4328']
Epoch: 40 | Batch:   1 | Lr: 0.00250 | Time used(s): 136.7 | Training loss: 0.0570
Epoch: 40 | Batch:   2 | Lr: 0.00250 | Time used(s): 129.4 | Training loss: 0.0489
Epoch: 40 | Batch:   3 | Lr: 0.00250 | Time used(s): 121.0 | Training loss: 0.0403
Epoch: 40 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0870
--------------------------------------------------
Epoch: 40 | [Train] | Loss: 0.0494
Epoch: 40 |   [Val] | Loss: 0.0696 | [CCC]:  0.4104 [' 0.4104'] | PCC: 0.4130 ['0.4130'] | RMSE: 0.4364 ['0.4364']
Epoch    40: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 41 | Batch:   1 | Lr: 0.00125 | Time used(s): 135.8 | Training loss: 0.0499
Epoch: 41 | Batch:   2 | Lr: 0.00125 | Time used(s): 130.4 | Training loss: 0.0467
Epoch: 41 | Batch:   3 | Lr: 0.00125 | Time used(s): 126.5 | Training loss: 0.0465
Epoch: 41 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0378
--------------------------------------------------
Epoch: 41 | [Train] | Loss: 0.0476
Epoch: 41 |   [Val] | Loss: 0.0746 | [CCC]:  0.4042 [' 0.4042'] | PCC: 0.4217 ['0.4217'] | RMSE: 0.4416 ['0.4416']
Epoch: 42 | Batch:   1 | Lr: 0.00125 | Time used(s): 133.1 | Training loss: 0.0527
Epoch: 42 | Batch:   2 | Lr: 0.00125 | Time used(s): 134.9 | Training loss: 0.0410
Epoch: 42 | Batch:   3 | Lr: 0.00125 | Time used(s): 121.6 | Training loss: 0.0442
Epoch: 42 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0357
--------------------------------------------------
Epoch: 42 | [Train] | Loss: 0.0458
Epoch: 42 |   [Val] | Loss: 0.0696 | [CCC]:  0.4179 [' 0.4179'] | PCC: 0.4193 ['0.4193'] | RMSE: 0.4278 ['0.4278']
Epoch: 43 | Batch:   1 | Lr: 0.00125 | Time used(s): 136.2 | Training loss: 0.0416
Epoch: 43 | Batch:   2 | Lr: 0.00125 | Time used(s): 133.3 | Training loss: 0.0500
Epoch: 43 | Batch:   3 | Lr: 0.00125 | Time used(s): 129.0 | Training loss: 0.0378
Epoch: 43 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0271
--------------------------------------------------
Epoch: 43 | [Train] | Loss: 0.0429
Epoch: 43 |   [Val] | Loss: 0.0722 | [CCC]:  0.4061 [' 0.4061'] | PCC: 0.4185 ['0.4185'] | RMSE: 0.4292 ['0.4292']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 331 | Best [Val CCC]: 0.4267 [' 0.4267']| Loss: 0.0695 | PCC: 0.4305 ['0.4305'] | RMSE: 0.4311 ['0.4311']
On Test: CCC  0.5646 | PCC  0.5796 | RMSE  0.3752
****************************************************************************************************
Seed "331" over!
****************************************************************************************************
****************************************************************************************************
Using seed "332"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 332]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 133.8 | Training loss: 0.2494
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 131.0 | Training loss: 0.2417
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 116.8 | Training loss: 0.2414
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2153
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2437
Epoch:  1 |   [Val] | Loss: 0.1995 | [CCC]:  0.1334 [' 0.1334'] | PCC: 0.2001 ['0.2001'] | RMSE: 0.4631 ['0.4631']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_332_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.2 | Training loss: 0.2260
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 125.2 | Training loss: 0.2095
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 117.2 | Training loss: 0.1922
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.1975
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2090
Epoch:  2 |   [Val] | Loss: 0.2203 | [CCC]:  0.0534 [' 0.0534'] | PCC: 0.1770 ['0.1770'] | RMSE: 0.8036 ['0.8036']
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 128.5 | Training loss: 0.2229
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 131.4 | Training loss: 0.2152
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 122.3 | Training loss: 0.2082
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2125
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.2154
Epoch:  3 |   [Val] | Loss: 0.1428 | [CCC]:  0.2375 [' 0.2375'] | PCC: 0.2418 ['0.2418'] | RMSE: 0.4815 ['0.4815']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_332_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 133.8 | Training loss: 0.1917
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 128.6 | Training loss: 0.1781
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 125.8 | Training loss: 0.1816
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.2000
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1841
Epoch:  4 |   [Val] | Loss: 0.1661 | [CCC]:  0.1754 [' 0.1754'] | PCC: 0.2660 ['0.2660'] | RMSE: 0.6128 ['0.6128']
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 134.9 | Training loss: 0.1940
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 130.8 | Training loss: 0.1611
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 126.0 | Training loss: 0.1592
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1533
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1711
Epoch:  5 |   [Val] | Loss: 0.1108 | [CCC]:  0.3012 [' 0.3012'] | PCC: 0.3061 ['0.3061'] | RMSE: 0.4796 ['0.4796']
Epoch:  5 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_332_None_None].pth"!
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 141.4 | Training loss: 0.1503
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 136.3 | Training loss: 0.1457
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 129.1 | Training loss: 0.1446
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1330
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1466
Epoch:  6 |   [Val] | Loss: 0.1159 | [CCC]:  0.2829 [' 0.2829'] | PCC: 0.2997 ['0.2997'] | RMSE: 0.4888 ['0.4888']
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 133.7 | Training loss: 0.1582
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 139.3 | Training loss: 0.1453
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 128.2 | Training loss: 0.1472
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1476
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1502
Epoch:  7 |   [Val] | Loss: 0.1011 | [CCC]:  0.3104 [' 0.3104'] | PCC: 0.3239 ['0.3239'] | RMSE: 0.4418 ['0.4418']
Epoch:  7 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_332_None_None].pth"!
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 141.1 | Training loss: 0.1400
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 140.2 | Training loss: 0.1429
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 119.7 | Training loss: 0.1441
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1260
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1421
Epoch:  8 |   [Val] | Loss: 0.1134 | [CCC]:  0.2966 [' 0.2966'] | PCC: 0.3408 ['0.3408'] | RMSE: 0.5506 ['0.5506']
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 139.7 | Training loss: 0.1376
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 135.8 | Training loss: 0.1283
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 130.7 | Training loss: 0.1323
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1341
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1327
Epoch:  9 |   [Val] | Loss: 0.1266 | [CCC]:  0.2500 [' 0.2500'] | PCC: 0.3281 ['0.3281'] | RMSE: 0.5751 ['0.5751']
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 141.3 | Training loss: 0.1477
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 134.0 | Training loss: 0.1411
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 134.9 | Training loss: 0.1268
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1271
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1384
Epoch: 10 |   [Val] | Loss: 0.0884 | [CCC]:  0.3517 [' 0.3517'] | PCC: 0.3726 ['0.3726'] | RMSE: 0.4069 ['0.4069']
Epoch: 10 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_332_None_None].pth"!
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 139.4 | Training loss: 0.1267
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 132.4 | Training loss: 0.1255
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 126.1 | Training loss: 0.1219
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1254
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1247
Epoch: 11 |   [Val] | Loss: 0.0837 | [CCC]:  0.3799 [' 0.3799'] | PCC: 0.3833 ['0.3833'] | RMSE: 0.4822 ['0.4822']
Epoch: 11 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_332_None_None].pth"!
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 138.1 | Training loss: 0.1215
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 130.1 | Training loss: 0.1155
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 124.4 | Training loss: 0.1142
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1025
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1168
Epoch: 12 |   [Val] | Loss: 0.0852 | [CCC]:  0.3915 [' 0.3915'] | PCC: 0.4019 ['0.4019'] | RMSE: 0.4630 ['0.4630']
Epoch: 12 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_332_None_None].pth"!
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 140.2 | Training loss: 0.1173
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 134.9 | Training loss: 0.1121
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 125.8 | Training loss: 0.1052
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1075
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1115
Epoch: 13 |   [Val] | Loss: 0.0811 | [CCC]:  0.4045 [' 0.4045'] | PCC: 0.4077 ['0.4077'] | RMSE: 0.4273 ['0.4273']
Epoch: 13 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_332_None_None].pth"!
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 140.6 | Training loss: 0.1071
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 138.0 | Training loss: 0.1082
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 133.0 | Training loss: 0.1051
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1072
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1068
Epoch: 14 |   [Val] | Loss: 0.0837 | [CCC]:  0.3919 [' 0.3919'] | PCC: 0.3936 ['0.3936'] | RMSE: 0.4324 ['0.4324']
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 137.2 | Training loss: 0.1096
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 137.0 | Training loss: 0.0942
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 129.1 | Training loss: 0.1031
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1051
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1023
Epoch: 15 |   [Val] | Loss: 0.0858 | [CCC]:  0.3715 [' 0.3715'] | PCC: 0.3971 ['0.3971'] | RMSE: 0.4554 ['0.4554']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 135.9 | Training loss: 0.0872
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 132.8 | Training loss: 0.0999
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 131.6 | Training loss: 0.1004
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1548
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.0968
Epoch: 16 |   [Val] | Loss: 0.0792 | [CCC]:  0.4002 [' 0.4002'] | PCC: 0.4018 ['0.4018'] | RMSE: 0.4458 ['0.4458']
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 137.1 | Training loss: 0.1014
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 136.5 | Training loss: 0.0854
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 132.5 | Training loss: 0.0899
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1128
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.0926
Epoch: 17 |   [Val] | Loss: 0.0755 | [CCC]:  0.4209 [' 0.4209'] | PCC: 0.4219 ['0.4219'] | RMSE: 0.4357 ['0.4357']
Epoch: 17 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_332_None_None].pth"!
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 138.0 | Training loss: 0.0916
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 142.9 | Training loss: 0.0989
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 124.2 | Training loss: 0.0934
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1161
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.0950
Epoch: 18 |   [Val] | Loss: 0.0807 | [CCC]:  0.3868 [' 0.3868'] | PCC: 0.4252 ['0.4252'] | RMSE: 0.4311 ['0.4311']
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 137.0 | Training loss: 0.0869
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 135.1 | Training loss: 0.0933
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 129.2 | Training loss: 0.1064
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0977
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.0956
Epoch: 19 |   [Val] | Loss: 0.0773 | [CCC]:  0.3983 [' 0.3983'] | PCC: 0.4214 ['0.4214'] | RMSE: 0.4840 ['0.4840']
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 132.6 | Training loss: 0.0893
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 128.7 | Training loss: 0.0913
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 133.7 | Training loss: 0.0942
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1092
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0919
Epoch: 20 |   [Val] | Loss: 0.0778 | [CCC]:  0.3685 [' 0.3685'] | PCC: 0.4001 ['0.4001'] | RMSE: 0.4153 ['0.4153']
Epoch: 21 | Batch:   1 | Lr: 0.00500 | Time used(s): 141.7 | Training loss: 0.0845
Epoch: 21 | Batch:   2 | Lr: 0.00500 | Time used(s): 134.7 | Training loss: 0.0909
Epoch: 21 | Batch:   3 | Lr: 0.00500 | Time used(s): 128.6 | Training loss: 0.0860
Epoch: 21 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0893
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0872
Epoch: 21 |   [Val] | Loss: 0.0747 | [CCC]:  0.4018 [' 0.4018'] | PCC: 0.4185 ['0.4185'] | RMSE: 0.4774 ['0.4774']
Epoch: 22 | Batch:   1 | Lr: 0.00500 | Time used(s): 139.7 | Training loss: 0.0788
Epoch: 22 | Batch:   2 | Lr: 0.00500 | Time used(s): 134.3 | Training loss: 0.0843
Epoch: 22 | Batch:   3 | Lr: 0.00500 | Time used(s): 129.8 | Training loss: 0.0884
Epoch: 22 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1157
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0844
Epoch: 22 |   [Val] | Loss: 0.0769 | [CCC]:  0.4088 [' 0.4088'] | PCC: 0.4203 ['0.4203'] | RMSE: 0.4426 ['0.4426']
Epoch: 23 | Batch:   1 | Lr: 0.00500 | Time used(s): 143.3 | Training loss: 0.0771
Epoch: 23 | Batch:   2 | Lr: 0.00500 | Time used(s): 136.1 | Training loss: 0.0842
Epoch: 23 | Batch:   3 | Lr: 0.00500 | Time used(s): 122.9 | Training loss: 0.0825
Epoch: 23 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0928
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0815
Epoch: 23 |   [Val] | Loss: 0.0732 | [CCC]:  0.3921 [' 0.3921'] | PCC: 0.3985 ['0.3985'] | RMSE: 0.4168 ['0.4168']
Epoch    23: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 24 | Batch:   1 | Lr: 0.00250 | Time used(s): 137.9 | Training loss: 0.0880
Epoch: 24 | Batch:   2 | Lr: 0.00250 | Time used(s): 129.9 | Training loss: 0.0776
Epoch: 24 | Batch:   3 | Lr: 0.00250 | Time used(s): 127.3 | Training loss: 0.0684
Epoch: 24 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0810
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0781
Epoch: 24 |   [Val] | Loss: 0.0750 | [CCC]:  0.4194 [' 0.4194'] | PCC: 0.4195 ['0.4195'] | RMSE: 0.4450 ['0.4450']
Epoch: 25 | Batch:   1 | Lr: 0.00250 | Time used(s): 138.4 | Training loss: 0.0766
Epoch: 25 | Batch:   2 | Lr: 0.00250 | Time used(s): 133.0 | Training loss: 0.0796
Epoch: 25 | Batch:   3 | Lr: 0.00250 | Time used(s): 127.6 | Training loss: 0.0690
Epoch: 25 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0750
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0750
Epoch: 25 |   [Val] | Loss: 0.0717 | [CCC]:  0.4313 [' 0.4313'] | PCC: 0.4316 ['0.4316'] | RMSE: 0.4382 ['0.4382']
Epoch: 25 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_332_None_None].pth"!
Epoch: 26 | Batch:   1 | Lr: 0.00250 | Time used(s): 142.0 | Training loss: 0.0675
Epoch: 26 | Batch:   2 | Lr: 0.00250 | Time used(s): 130.5 | Training loss: 0.0751
Epoch: 26 | Batch:   3 | Lr: 0.00250 | Time used(s): 130.1 | Training loss: 0.0706
Epoch: 26 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0690
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0710
Epoch: 26 |   [Val] | Loss: 0.0683 | [CCC]:  0.4129 [' 0.4129'] | PCC: 0.4136 ['0.4136'] | RMSE: 0.4307 ['0.4307']
Epoch: 27 | Batch:   1 | Lr: 0.00250 | Time used(s): 137.2 | Training loss: 0.0727
Epoch: 27 | Batch:   2 | Lr: 0.00250 | Time used(s): 122.3 | Training loss: 0.0771
Epoch: 27 | Batch:   3 | Lr: 0.00250 | Time used(s): 136.6 | Training loss: 0.0643
Epoch: 27 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0518
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0711
Epoch: 27 |   [Val] | Loss: 0.0791 | [CCC]:  0.4049 [' 0.4049'] | PCC: 0.4178 ['0.4178'] | RMSE: 0.4714 ['0.4714']
Epoch: 28 | Batch:   1 | Lr: 0.00250 | Time used(s): 139.5 | Training loss: 0.0686
Epoch: 28 | Batch:   2 | Lr: 0.00250 | Time used(s): 136.6 | Training loss: 0.0667
Epoch: 28 | Batch:   3 | Lr: 0.00250 | Time used(s): 137.5 | Training loss: 0.0735
Epoch: 28 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0474
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0693
Epoch: 28 |   [Val] | Loss: 0.0934 | [CCC]:  0.3373 [' 0.3373'] | PCC: 0.4086 ['0.4086'] | RMSE: 0.5148 ['0.5148']
Epoch: 29 | Batch:   1 | Lr: 0.00250 | Time used(s): 139.9 | Training loss: 0.0941
Epoch: 29 | Batch:   2 | Lr: 0.00250 | Time used(s): 135.8 | Training loss: 0.0761
Epoch: 29 | Batch:   3 | Lr: 0.00250 | Time used(s): 122.6 | Training loss: 0.0748
Epoch: 29 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0853
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0817
Epoch: 29 |   [Val] | Loss: 0.0767 | [CCC]:  0.4324 [' 0.4324'] | PCC: 0.4340 ['0.4340'] | RMSE: 0.4551 ['0.4551']
Epoch: 29 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_332_None_None].pth"!
Epoch: 30 | Batch:   1 | Lr: 0.00250 | Time used(s): 138.9 | Training loss: 0.0648
Epoch: 30 | Batch:   2 | Lr: 0.00250 | Time used(s): 128.3 | Training loss: 0.0861
Epoch: 30 | Batch:   3 | Lr: 0.00250 | Time used(s): 120.1 | Training loss: 0.0808
Epoch: 30 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0613
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0770
Epoch: 30 |   [Val] | Loss: 0.0734 | [CCC]:  0.4080 [' 0.4080'] | PCC: 0.4384 ['0.4384'] | RMSE: 0.4032 ['0.4032']
Epoch: 31 | Batch:   1 | Lr: 0.00250 | Time used(s): 141.7 | Training loss: 0.0749
Epoch: 31 | Batch:   2 | Lr: 0.00250 | Time used(s): 139.4 | Training loss: 0.0683
Epoch: 31 | Batch:   3 | Lr: 0.00250 | Time used(s): 127.4 | Training loss: 0.0663
Epoch: 31 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0835
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0701
Epoch: 31 |   [Val] | Loss: 0.0716 | [CCC]:  0.4218 [' 0.4218'] | PCC: 0.4293 ['0.4293'] | RMSE: 0.4353 ['0.4353']
Epoch: 32 | Batch:   1 | Lr: 0.00250 | Time used(s): 142.6 | Training loss: 0.0660
Epoch: 32 | Batch:   2 | Lr: 0.00250 | Time used(s): 136.7 | Training loss: 0.0616
Epoch: 32 | Batch:   3 | Lr: 0.00250 | Time used(s): 127.9 | Training loss: 0.0634
Epoch: 32 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0510
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0635
Epoch: 32 |   [Val] | Loss: 0.0714 | [CCC]:  0.4033 [' 0.4033'] | PCC: 0.4071 ['0.4071'] | RMSE: 0.4292 ['0.4292']
Epoch: 33 | Batch:   1 | Lr: 0.00250 | Time used(s): 140.0 | Training loss: 0.0603
Epoch: 33 | Batch:   2 | Lr: 0.00250 | Time used(s): 134.3 | Training loss: 0.0604
Epoch: 33 | Batch:   3 | Lr: 0.00250 | Time used(s): 117.4 | Training loss: 0.0591
Epoch: 33 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.1029
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0606
Epoch: 33 |   [Val] | Loss: 0.0761 | [CCC]:  0.4055 [' 0.4055'] | PCC: 0.4098 ['0.4098'] | RMSE: 0.4404 ['0.4404']
Epoch: 34 | Batch:   1 | Lr: 0.00250 | Time used(s): 134.6 | Training loss: 0.0550
Epoch: 34 | Batch:   2 | Lr: 0.00250 | Time used(s): 132.1 | Training loss: 0.0606
Epoch: 34 | Batch:   3 | Lr: 0.00250 | Time used(s): 131.3 | Training loss: 0.0630
Epoch: 34 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.1 | Training loss: 0.0606
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0596
Epoch: 34 |   [Val] | Loss: 0.0749 | [CCC]:  0.3947 [' 0.3947'] | PCC: 0.3948 ['0.3948'] | RMSE: 0.4497 ['0.4497']
Epoch: 35 | Batch:   1 | Lr: 0.00250 | Time used(s): 140.6 | Training loss: 0.0540
Epoch: 35 | Batch:   2 | Lr: 0.00250 | Time used(s): 140.7 | Training loss: 0.0559
Epoch: 35 | Batch:   3 | Lr: 0.00250 | Time used(s): 130.9 | Training loss: 0.0625
Epoch: 35 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0736
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0578
Epoch: 35 |   [Val] | Loss: 0.0712 | [CCC]:  0.3935 [' 0.3935'] | PCC: 0.3970 ['0.3970'] | RMSE: 0.4293 ['0.4293']
Epoch    35: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 36 | Batch:   1 | Lr: 0.00125 | Time used(s): 144.6 | Training loss: 0.0507
Epoch: 36 | Batch:   2 | Lr: 0.00125 | Time used(s): 124.5 | Training loss: 0.0541
Epoch: 36 | Batch:   3 | Lr: 0.00125 | Time used(s): 121.1 | Training loss: 0.0495
Epoch: 36 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0377
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0512
Epoch: 36 |   [Val] | Loss: 0.0741 | [CCC]:  0.4007 [' 0.4007'] | PCC: 0.4037 ['0.4037'] | RMSE: 0.4256 ['0.4256']
Epoch: 37 | Batch:   1 | Lr: 0.00125 | Time used(s): 140.4 | Training loss: 0.0456
Epoch: 37 | Batch:   2 | Lr: 0.00125 | Time used(s): 134.5 | Training loss: 0.0528
Epoch: 37 | Batch:   3 | Lr: 0.00125 | Time used(s): 126.9 | Training loss: 0.0553
Epoch: 37 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0345
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0510
Epoch: 37 |   [Val] | Loss: 0.0727 | [CCC]:  0.3980 [' 0.3980'] | PCC: 0.4003 ['0.4003'] | RMSE: 0.4449 ['0.4449']
Epoch: 38 | Batch:   1 | Lr: 0.00125 | Time used(s): 136.3 | Training loss: 0.0447
Epoch: 38 | Batch:   2 | Lr: 0.00125 | Time used(s): 127.8 | Training loss: 0.0539
Epoch: 38 | Batch:   3 | Lr: 0.00125 | Time used(s): 125.6 | Training loss: 0.0446
Epoch: 38 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0155
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0472
Epoch: 38 |   [Val] | Loss: 0.0702 | [CCC]:  0.4030 [' 0.4030'] | PCC: 0.4038 ['0.4038'] | RMSE: 0.4353 ['0.4353']
Epoch: 39 | Batch:   1 | Lr: 0.00125 | Time used(s): 139.5 | Training loss: 0.0453
Epoch: 39 | Batch:   2 | Lr: 0.00125 | Time used(s): 138.9 | Training loss: 0.0461
Epoch: 39 | Batch:   3 | Lr: 0.00125 | Time used(s): 130.9 | Training loss: 0.0444
Epoch: 39 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0264
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0449
Epoch: 39 |   [Val] | Loss: 0.0711 | [CCC]:  0.4060 [' 0.4060'] | PCC: 0.4108 ['0.4108'] | RMSE: 0.4402 ['0.4402']
Epoch: 40 | Batch:   1 | Lr: 0.00125 | Time used(s): 134.7 | Training loss: 0.0487
Epoch: 40 | Batch:   2 | Lr: 0.00125 | Time used(s): 141.5 | Training loss: 0.0363
Epoch: 40 | Batch:   3 | Lr: 0.00125 | Time used(s): 125.4 | Training loss: 0.0403
Epoch: 40 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0709
--------------------------------------------------
Epoch: 40 | [Train] | Loss: 0.0422
Epoch: 40 |   [Val] | Loss: 0.0696 | [CCC]:  0.4160 [' 0.4160'] | PCC: 0.4211 ['0.4211'] | RMSE: 0.4162 ['0.4162']
Epoch: 41 | Batch:   1 | Lr: 0.00125 | Time used(s): 133.3 | Training loss: 0.0416
Epoch: 41 | Batch:   2 | Lr: 0.00125 | Time used(s): 139.9 | Training loss: 0.0407
Epoch: 41 | Batch:   3 | Lr: 0.00125 | Time used(s): 119.9 | Training loss: 0.0362
Epoch: 41 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0771
--------------------------------------------------
Epoch: 41 | [Train] | Loss: 0.0401
Epoch: 41 |   [Val] | Loss: 0.0690 | [CCC]:  0.4084 [' 0.4084'] | PCC: 0.4149 ['0.4149'] | RMSE: 0.4437 ['0.4437']
Epoch    41: reducing learning rate of group 0 to 6.2500e-04.
Epoch: 42 | Batch:   1 | Lr: 0.00063 | Time used(s): 125.8 | Training loss: 0.0419
Epoch: 42 | Batch:   2 | Lr: 0.00063 | Time used(s): 137.0 | Training loss: 0.0399
Epoch: 42 | Batch:   3 | Lr: 0.00063 | Time used(s): 118.4 | Training loss: 0.0378
Epoch: 42 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.2 | Training loss: 0.0493
--------------------------------------------------
Epoch: 42 | [Train] | Loss: 0.0400
Epoch: 42 |   [Val] | Loss: 0.0704 | [CCC]:  0.4168 [' 0.4168'] | PCC: 0.4200 ['0.4200'] | RMSE: 0.4293 ['0.4293']
Epoch: 43 | Batch:   1 | Lr: 0.00063 | Time used(s): 141.1 | Training loss: 0.0447
Epoch: 43 | Batch:   2 | Lr: 0.00063 | Time used(s): 131.6 | Training loss: 0.0352
Epoch: 43 | Batch:   3 | Lr: 0.00063 | Time used(s): 128.0 | Training loss: 0.0322
Epoch: 43 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.2 | Training loss: 0.0436
--------------------------------------------------
Epoch: 43 | [Train] | Loss: 0.0375
Epoch: 43 |   [Val] | Loss: 0.0698 | [CCC]:  0.4173 [' 0.4173'] | PCC: 0.4204 ['0.4204'] | RMSE: 0.4436 ['0.4436']
Epoch: 44 | Batch:   1 | Lr: 0.00063 | Time used(s): 136.7 | Training loss: 0.0262
Epoch: 44 | Batch:   2 | Lr: 0.00063 | Time used(s): 137.1 | Training loss: 0.0460
Epoch: 44 | Batch:   3 | Lr: 0.00063 | Time used(s): 127.3 | Training loss: 0.0345
Epoch: 44 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.2 | Training loss: 0.0278
--------------------------------------------------
Epoch: 44 | [Train] | Loss: 0.0355
Epoch: 44 |   [Val] | Loss: 0.0694 | [CCC]:  0.4190 [' 0.4190'] | PCC: 0.4199 ['0.4199'] | RMSE: 0.4292 ['0.4292']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 332 | Best [Val CCC]: 0.4324 [' 0.4324']| Loss: 0.0767 | PCC: 0.4340 ['0.4340'] | RMSE: 0.4551 ['0.4551']
On Test: CCC  0.5695 | PCC  0.5786 | RMSE  0.3793
****************************************************************************************************
Seed "332" over!
****************************************************************************************************
****************************************************************************************************
Using seed "333"
****************************************************************************************************
Model(
  (proj): Linear(in_features=768, out_features=64, bias=False)
  (attn): SelfAttention(
    (layers): ModuleList(
      (0): SelfAttentionLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (rnn): RNNEncoder(
    (rnn): LSTM(64, 64, bidirectional=True)
  )
  (out): QuantileRegressor(
    (fc_1): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.0, inplace=False)
    )
    (fc_2): Linear(in_features=64, out_features=3, bias=True)
  )
)
==================================================
Training model... [seed 333]
Epoch:  1 | Batch:   1 | Lr: 0.00500 | Time used(s): 135.3 | Training loss: 0.2501
Epoch:  1 | Batch:   2 | Lr: 0.00500 | Time used(s): 133.7 | Training loss: 0.2452
Epoch:  1 | Batch:   3 | Lr: 0.00500 | Time used(s): 121.3 | Training loss: 0.2392
Epoch:  1 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2458
--------------------------------------------------
Epoch:  1 | [Train] | Loss: 0.2449
Epoch:  1 |   [Val] | Loss: 0.2385 | [CCC]:  0.0189 [' 0.0189'] | PCC: 0.1108 ['0.1108'] | RMSE: 0.6072 ['0.6072']
Epoch:  1 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_333_None_None].pth"!
Epoch:  2 | Batch:   1 | Lr: 0.00500 | Time used(s): 139.3 | Training loss: 0.2386
Epoch:  2 | Batch:   2 | Lr: 0.00500 | Time used(s): 132.8 | Training loss: 0.2164
Epoch:  2 | Batch:   3 | Lr: 0.00500 | Time used(s): 130.0 | Training loss: 0.2193
Epoch:  2 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.1 | Training loss: 0.2106
--------------------------------------------------
Epoch:  2 | [Train] | Loss: 0.2245
Epoch:  2 |   [Val] | Loss: 0.1624 | [CCC]:  0.2185 [' 0.2185'] | PCC: 0.2220 ['0.2220'] | RMSE: 0.5407 ['0.5407']
Epoch:  2 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_333_None_None].pth"!
Epoch:  3 | Batch:   1 | Lr: 0.00500 | Time used(s): 133.4 | Training loss: 0.1961
Epoch:  3 | Batch:   2 | Lr: 0.00500 | Time used(s): 131.1 | Training loss: 0.2038
Epoch:  3 | Batch:   3 | Lr: 0.00500 | Time used(s): 125.7 | Training loss: 0.1872
Epoch:  3 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1432
--------------------------------------------------
Epoch:  3 | [Train] | Loss: 0.1948
Epoch:  3 |   [Val] | Loss: 0.1328 | [CCC]:  0.2840 [' 0.2840'] | PCC: 0.2890 ['0.2890'] | RMSE: 0.5005 ['0.5005']
Epoch:  3 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_333_None_None].pth"!
Epoch:  4 | Batch:   1 | Lr: 0.00500 | Time used(s): 137.1 | Training loss: 0.1675
Epoch:  4 | Batch:   2 | Lr: 0.00500 | Time used(s): 130.4 | Training loss: 0.1683
Epoch:  4 | Batch:   3 | Lr: 0.00500 | Time used(s): 123.2 | Training loss: 0.1827
Epoch:  4 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1862
--------------------------------------------------
Epoch:  4 | [Train] | Loss: 0.1730
Epoch:  4 |   [Val] | Loss: 0.1362 | [CCC]:  0.2554 [' 0.2554'] | PCC: 0.2914 ['0.2914'] | RMSE: 0.4637 ['0.4637']
Epoch:  5 | Batch:   1 | Lr: 0.00500 | Time used(s): 140.2 | Training loss: 0.1670
Epoch:  5 | Batch:   2 | Lr: 0.00500 | Time used(s): 138.2 | Training loss: 0.1517
Epoch:  5 | Batch:   3 | Lr: 0.00500 | Time used(s): 134.2 | Training loss: 0.1536
Epoch:  5 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1549
--------------------------------------------------
Epoch:  5 | [Train] | Loss: 0.1574
Epoch:  5 |   [Val] | Loss: 0.1083 | [CCC]:  0.3014 [' 0.3014'] | PCC: 0.3099 ['0.3099'] | RMSE: 0.5254 ['0.5254']
Epoch:  5 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_333_None_None].pth"!
Epoch:  6 | Batch:   1 | Lr: 0.00500 | Time used(s): 142.2 | Training loss: 0.1427
Epoch:  6 | Batch:   2 | Lr: 0.00500 | Time used(s): 133.7 | Training loss: 0.1453
Epoch:  6 | Batch:   3 | Lr: 0.00500 | Time used(s): 114.2 | Training loss: 0.1423
Epoch:  6 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1567
--------------------------------------------------
Epoch:  6 | [Train] | Loss: 0.1436
Epoch:  6 |   [Val] | Loss: 0.1369 | [CCC]:  0.2359 [' 0.2359'] | PCC: 0.3068 ['0.3068'] | RMSE: 0.5804 ['0.5804']
Epoch:  7 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.0 | Training loss: 0.1655
Epoch:  7 | Batch:   2 | Lr: 0.00500 | Time used(s): 142.0 | Training loss: 0.1296
Epoch:  7 | Batch:   3 | Lr: 0.00500 | Time used(s): 133.9 | Training loss: 0.1637
Epoch:  7 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1422
--------------------------------------------------
Epoch:  7 | [Train] | Loss: 0.1527
Epoch:  7 |   [Val] | Loss: 0.1091 | [CCC]:  0.3031 [' 0.3031'] | PCC: 0.3154 ['0.3154'] | RMSE: 0.5214 ['0.5214']
Epoch:  7 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_333_None_None].pth"!
Epoch:  8 | Batch:   1 | Lr: 0.00500 | Time used(s): 140.8 | Training loss: 0.1438
Epoch:  8 | Batch:   2 | Lr: 0.00500 | Time used(s): 136.4 | Training loss: 0.1534
Epoch:  8 | Batch:   3 | Lr: 0.00500 | Time used(s): 138.6 | Training loss: 0.1407
Epoch:  8 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1641
--------------------------------------------------
Epoch:  8 | [Train] | Loss: 0.1462
Epoch:  8 |   [Val] | Loss: 0.1133 | [CCC]:  0.2896 [' 0.2896'] | PCC: 0.3124 ['0.3124'] | RMSE: 0.4631 ['0.4631']
Epoch:  9 | Batch:   1 | Lr: 0.00500 | Time used(s): 138.2 | Training loss: 0.1417
Epoch:  9 | Batch:   2 | Lr: 0.00500 | Time used(s): 130.5 | Training loss: 0.1296
Epoch:  9 | Batch:   3 | Lr: 0.00500 | Time used(s): 135.8 | Training loss: 0.1345
Epoch:  9 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1532
--------------------------------------------------
Epoch:  9 | [Train] | Loss: 0.1356
Epoch:  9 |   [Val] | Loss: 0.1186 | [CCC]:  0.2903 [' 0.2903'] | PCC: 0.3405 ['0.3405'] | RMSE: 0.4772 ['0.4772']
Epoch: 10 | Batch:   1 | Lr: 0.00500 | Time used(s): 138.5 | Training loss: 0.1364
Epoch: 10 | Batch:   2 | Lr: 0.00500 | Time used(s): 134.5 | Training loss: 0.1237
Epoch: 10 | Batch:   3 | Lr: 0.00500 | Time used(s): 137.9 | Training loss: 0.1342
Epoch: 10 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1255
--------------------------------------------------
Epoch: 10 | [Train] | Loss: 0.1313
Epoch: 10 |   [Val] | Loss: 0.0926 | [CCC]:  0.3460 [' 0.3460'] | PCC: 0.3476 ['0.3476'] | RMSE: 0.4479 ['0.4479']
Epoch: 10 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_333_None_None].pth"!
Epoch: 11 | Batch:   1 | Lr: 0.00500 | Time used(s): 139.4 | Training loss: 0.1141
Epoch: 11 | Batch:   2 | Lr: 0.00500 | Time used(s): 139.5 | Training loss: 0.1219
Epoch: 11 | Batch:   3 | Lr: 0.00500 | Time used(s): 128.0 | Training loss: 0.1139
Epoch: 11 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1409
--------------------------------------------------
Epoch: 11 | [Train] | Loss: 0.1170
Epoch: 11 |   [Val] | Loss: 0.0902 | [CCC]:  0.3455 [' 0.3455'] | PCC: 0.3648 ['0.3648'] | RMSE: 0.4762 ['0.4762']
Epoch: 12 | Batch:   1 | Lr: 0.00500 | Time used(s): 141.8 | Training loss: 0.1155
Epoch: 12 | Batch:   2 | Lr: 0.00500 | Time used(s): 132.2 | Training loss: 0.1097
Epoch: 12 | Batch:   3 | Lr: 0.00500 | Time used(s): 135.1 | Training loss: 0.1140
Epoch: 12 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1048
--------------------------------------------------
Epoch: 12 | [Train] | Loss: 0.1129
Epoch: 12 |   [Val] | Loss: 0.1022 | [CCC]:  0.3218 [' 0.3218'] | PCC: 0.3625 ['0.3625'] | RMSE: 0.5615 ['0.5615']
Epoch: 13 | Batch:   1 | Lr: 0.00500 | Time used(s): 139.4 | Training loss: 0.1277
Epoch: 13 | Batch:   2 | Lr: 0.00500 | Time used(s): 128.5 | Training loss: 0.0961
Epoch: 13 | Batch:   3 | Lr: 0.00500 | Time used(s): 129.1 | Training loss: 0.1181
Epoch: 13 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1375
--------------------------------------------------
Epoch: 13 | [Train] | Loss: 0.1143
Epoch: 13 |   [Val] | Loss: 0.0812 | [CCC]:  0.3771 [' 0.3771'] | PCC: 0.3908 ['0.3908'] | RMSE: 0.4390 ['0.4390']
Epoch: 13 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_333_None_None].pth"!
Epoch: 14 | Batch:   1 | Lr: 0.00500 | Time used(s): 140.8 | Training loss: 0.1108
Epoch: 14 | Batch:   2 | Lr: 0.00500 | Time used(s): 126.1 | Training loss: 0.1074
Epoch: 14 | Batch:   3 | Lr: 0.00500 | Time used(s): 133.9 | Training loss: 0.1093
Epoch: 14 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1410
--------------------------------------------------
Epoch: 14 | [Train] | Loss: 0.1097
Epoch: 14 |   [Val] | Loss: 0.0845 | [CCC]:  0.3721 [' 0.3721'] | PCC: 0.3973 ['0.3973'] | RMSE: 0.4549 ['0.4549']
Epoch: 15 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.0 | Training loss: 0.1138
Epoch: 15 | Batch:   2 | Lr: 0.00500 | Time used(s): 138.4 | Training loss: 0.1143
Epoch: 15 | Batch:   3 | Lr: 0.00500 | Time used(s): 125.9 | Training loss: 0.1034
Epoch: 15 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1348
--------------------------------------------------
Epoch: 15 | [Train] | Loss: 0.1109
Epoch: 15 |   [Val] | Loss: 0.0922 | [CCC]:  0.3507 [' 0.3507'] | PCC: 0.3874 ['0.3874'] | RMSE: 0.5407 ['0.5407']
Epoch: 16 | Batch:   1 | Lr: 0.00500 | Time used(s): 139.9 | Training loss: 0.1113
Epoch: 16 | Batch:   2 | Lr: 0.00500 | Time used(s): 133.6 | Training loss: 0.1051
Epoch: 16 | Batch:   3 | Lr: 0.00500 | Time used(s): 122.9 | Training loss: 0.1112
Epoch: 16 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0996
--------------------------------------------------
Epoch: 16 | [Train] | Loss: 0.1091
Epoch: 16 |   [Val] | Loss: 0.0792 | [CCC]:  0.3915 [' 0.3915'] | PCC: 0.3943 ['0.3943'] | RMSE: 0.4265 ['0.4265']
Epoch: 16 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_333_None_None].pth"!
Epoch: 17 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.9 | Training loss: 0.0997
Epoch: 17 | Batch:   2 | Lr: 0.00500 | Time used(s): 140.6 | Training loss: 0.1050
Epoch: 17 | Batch:   3 | Lr: 0.00500 | Time used(s): 129.5 | Training loss: 0.0985
Epoch: 17 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.1316
--------------------------------------------------
Epoch: 17 | [Train] | Loss: 0.1016
Epoch: 17 |   [Val] | Loss: 0.0876 | [CCC]:  0.3655 [' 0.3655'] | PCC: 0.3865 ['0.3865'] | RMSE: 0.4413 ['0.4413']
Epoch: 18 | Batch:   1 | Lr: 0.00500 | Time used(s): 135.3 | Training loss: 0.0957
Epoch: 18 | Batch:   2 | Lr: 0.00500 | Time used(s): 128.4 | Training loss: 0.1019
Epoch: 18 | Batch:   3 | Lr: 0.00500 | Time used(s): 132.1 | Training loss: 0.1055
Epoch: 18 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1030
--------------------------------------------------
Epoch: 18 | [Train] | Loss: 0.1011
Epoch: 18 |   [Val] | Loss: 0.0935 | [CCC]:  0.3605 [' 0.3605'] | PCC: 0.3982 ['0.3982'] | RMSE: 0.4543 ['0.4543']
Epoch: 19 | Batch:   1 | Lr: 0.00500 | Time used(s): 139.0 | Training loss: 0.0968
Epoch: 19 | Batch:   2 | Lr: 0.00500 | Time used(s): 138.0 | Training loss: 0.1000
Epoch: 19 | Batch:   3 | Lr: 0.00500 | Time used(s): 126.0 | Training loss: 0.0987
Epoch: 19 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1361
--------------------------------------------------
Epoch: 19 | [Train] | Loss: 0.0991
Epoch: 19 |   [Val] | Loss: 0.0756 | [CCC]:  0.3936 [' 0.3936'] | PCC: 0.4063 ['0.4063'] | RMSE: 0.4110 ['0.4110']
Epoch: 19 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_333_None_None].pth"!
Epoch: 20 | Batch:   1 | Lr: 0.00500 | Time used(s): 135.4 | Training loss: 0.0907
Epoch: 20 | Batch:   2 | Lr: 0.00500 | Time used(s): 133.6 | Training loss: 0.0951
Epoch: 20 | Batch:   3 | Lr: 0.00500 | Time used(s): 130.1 | Training loss: 0.0900
Epoch: 20 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0952
--------------------------------------------------
Epoch: 20 | [Train] | Loss: 0.0920
Epoch: 20 |   [Val] | Loss: 0.0857 | [CCC]:  0.3861 [' 0.3861'] | PCC: 0.3964 ['0.3964'] | RMSE: 0.4814 ['0.4814']
Epoch: 21 | Batch:   1 | Lr: 0.00500 | Time used(s): 140.3 | Training loss: 0.0890
Epoch: 21 | Batch:   2 | Lr: 0.00500 | Time used(s): 131.7 | Training loss: 0.0943
Epoch: 21 | Batch:   3 | Lr: 0.00500 | Time used(s): 127.5 | Training loss: 0.0872
Epoch: 21 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.4 | Training loss: 0.0554
--------------------------------------------------
Epoch: 21 | [Train] | Loss: 0.0896
Epoch: 21 |   [Val] | Loss: 0.0823 | [CCC]:  0.3907 [' 0.3907'] | PCC: 0.3911 ['0.3911'] | RMSE: 0.4522 ['0.4522']
Epoch: 22 | Batch:   1 | Lr: 0.00500 | Time used(s): 138.7 | Training loss: 0.0828
Epoch: 22 | Batch:   2 | Lr: 0.00500 | Time used(s): 138.0 | Training loss: 0.0860
Epoch: 22 | Batch:   3 | Lr: 0.00500 | Time used(s): 132.6 | Training loss: 0.0797
Epoch: 22 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0872
--------------------------------------------------
Epoch: 22 | [Train] | Loss: 0.0829
Epoch: 22 |   [Val] | Loss: 0.0835 | [CCC]:  0.4012 [' 0.4012'] | PCC: 0.4032 ['0.4032'] | RMSE: 0.4344 ['0.4344']
Epoch: 22 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_333_None_None].pth"!
Epoch: 23 | Batch:   1 | Lr: 0.00500 | Time used(s): 139.4 | Training loss: 0.0765
Epoch: 23 | Batch:   2 | Lr: 0.00500 | Time used(s): 140.2 | Training loss: 0.0869
Epoch: 23 | Batch:   3 | Lr: 0.00500 | Time used(s): 129.4 | Training loss: 0.0770
Epoch: 23 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0574
--------------------------------------------------
Epoch: 23 | [Train] | Loss: 0.0798
Epoch: 23 |   [Val] | Loss: 0.0723 | [CCC]:  0.3938 [' 0.3938'] | PCC: 0.4033 ['0.4033'] | RMSE: 0.4313 ['0.4313']
Epoch: 24 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.4 | Training loss: 0.0768
Epoch: 24 | Batch:   2 | Lr: 0.00500 | Time used(s): 136.6 | Training loss: 0.0736
Epoch: 24 | Batch:   3 | Lr: 0.00500 | Time used(s): 133.1 | Training loss: 0.0784
Epoch: 24 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0359
--------------------------------------------------
Epoch: 24 | [Train] | Loss: 0.0756
Epoch: 24 |   [Val] | Loss: 0.0804 | [CCC]:  0.4152 [' 0.4152'] | PCC: 0.4205 ['0.4205'] | RMSE: 0.4571 ['0.4571']
Epoch: 24 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_333_None_None].pth"!
Epoch: 25 | Batch:   1 | Lr: 0.00500 | Time used(s): 139.0 | Training loss: 0.0764
Epoch: 25 | Batch:   2 | Lr: 0.00500 | Time used(s): 136.7 | Training loss: 0.0746
Epoch: 25 | Batch:   3 | Lr: 0.00500 | Time used(s): 135.3 | Training loss: 0.0624
Epoch: 25 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1055
--------------------------------------------------
Epoch: 25 | [Train] | Loss: 0.0717
Epoch: 25 |   [Val] | Loss: 0.0966 | [CCC]:  0.3380 [' 0.3380'] | PCC: 0.3826 ['0.3826'] | RMSE: 0.5397 ['0.5397']
Epoch: 26 | Batch:   1 | Lr: 0.00500 | Time used(s): 141.6 | Training loss: 0.0882
Epoch: 26 | Batch:   2 | Lr: 0.00500 | Time used(s): 134.8 | Training loss: 0.0722
Epoch: 26 | Batch:   3 | Lr: 0.00500 | Time used(s): 134.8 | Training loss: 0.0824
Epoch: 26 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0703
--------------------------------------------------
Epoch: 26 | [Train] | Loss: 0.0808
Epoch: 26 |   [Val] | Loss: 0.0759 | [CCC]:  0.3965 [' 0.3965'] | PCC: 0.4254 ['0.4254'] | RMSE: 0.4513 ['0.4513']
Epoch: 27 | Batch:   1 | Lr: 0.00500 | Time used(s): 140.5 | Training loss: 0.0758
Epoch: 27 | Batch:   2 | Lr: 0.00500 | Time used(s): 135.8 | Training loss: 0.0759
Epoch: 27 | Batch:   3 | Lr: 0.00500 | Time used(s): 129.4 | Training loss: 0.0738
Epoch: 27 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0441
--------------------------------------------------
Epoch: 27 | [Train] | Loss: 0.0747
Epoch: 27 |   [Val] | Loss: 0.0759 | [CCC]:  0.4250 [' 0.4250'] | PCC: 0.4275 ['0.4275'] | RMSE: 0.4285 ['0.4285']
Epoch: 27 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_333_None_None].pth"!
Epoch: 28 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.9 | Training loss: 0.0602
Epoch: 28 | Batch:   2 | Lr: 0.00500 | Time used(s): 135.8 | Training loss: 0.0798
Epoch: 28 | Batch:   3 | Lr: 0.00500 | Time used(s): 137.1 | Training loss: 0.0725
Epoch: 28 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0762
--------------------------------------------------
Epoch: 28 | [Train] | Loss: 0.0709
Epoch: 28 |   [Val] | Loss: 0.0767 | [CCC]:  0.4042 [' 0.4042'] | PCC: 0.4058 ['0.4058'] | RMSE: 0.4398 ['0.4398']
Epoch: 29 | Batch:   1 | Lr: 0.00500 | Time used(s): 124.3 | Training loss: 0.0683
Epoch: 29 | Batch:   2 | Lr: 0.00500 | Time used(s): 132.3 | Training loss: 0.0687
Epoch: 29 | Batch:   3 | Lr: 0.00500 | Time used(s): 129.0 | Training loss: 0.0643
Epoch: 29 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.1021
--------------------------------------------------
Epoch: 29 | [Train] | Loss: 0.0677
Epoch: 29 |   [Val] | Loss: 0.0731 | [CCC]:  0.3976 [' 0.3976'] | PCC: 0.4158 ['0.4158'] | RMSE: 0.4097 ['0.4097']
Epoch: 30 | Batch:   1 | Lr: 0.00500 | Time used(s): 136.9 | Training loss: 0.0651
Epoch: 30 | Batch:   2 | Lr: 0.00500 | Time used(s): 134.0 | Training loss: 0.0798
Epoch: 30 | Batch:   3 | Lr: 0.00500 | Time used(s): 136.6 | Training loss: 0.0622
Epoch: 30 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0573
--------------------------------------------------
Epoch: 30 | [Train] | Loss: 0.0689
Epoch: 30 |   [Val] | Loss: 0.0953 | [CCC]:  0.3588 [' 0.3588'] | PCC: 0.3688 ['0.3688'] | RMSE: 0.5111 ['0.5111']
Epoch: 31 | Batch:   1 | Lr: 0.00500 | Time used(s): 137.2 | Training loss: 0.0771
Epoch: 31 | Batch:   2 | Lr: 0.00500 | Time used(s): 135.9 | Training loss: 0.0734
Epoch: 31 | Batch:   3 | Lr: 0.00500 | Time used(s): 130.9 | Training loss: 0.0690
Epoch: 31 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.0923
--------------------------------------------------
Epoch: 31 | [Train] | Loss: 0.0734
Epoch: 31 |   [Val] | Loss: 0.0779 | [CCC]:  0.4142 [' 0.4142'] | PCC: 0.4214 ['0.4214'] | RMSE: 0.4257 ['0.4257']
Epoch: 32 | Batch:   1 | Lr: 0.00500 | Time used(s): 133.5 | Training loss: 0.0739
Epoch: 32 | Batch:   2 | Lr: 0.00500 | Time used(s): 135.3 | Training loss: 0.0628
Epoch: 32 | Batch:   3 | Lr: 0.00500 | Time used(s): 134.7 | Training loss: 0.0560
Epoch: 32 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.3 | Training loss: 0.1121
--------------------------------------------------
Epoch: 32 | [Train] | Loss: 0.0650
Epoch: 32 |   [Val] | Loss: 0.0752 | [CCC]:  0.4035 [' 0.4035'] | PCC: 0.4083 ['0.4083'] | RMSE: 0.4178 ['0.4178']
Epoch: 33 | Batch:   1 | Lr: 0.00500 | Time used(s): 133.9 | Training loss: 0.0598
Epoch: 33 | Batch:   2 | Lr: 0.00500 | Time used(s): 127.4 | Training loss: 0.0629
Epoch: 33 | Batch:   3 | Lr: 0.00500 | Time used(s): 133.2 | Training loss: 0.0573
Epoch: 33 | Batch:   4 | Lr: 0.00500 | Time used(s): 2.2 | Training loss: 0.0699
--------------------------------------------------
Epoch: 33 | [Train] | Loss: 0.0602
Epoch: 33 |   [Val] | Loss: 0.0775 | [CCC]:  0.3904 [' 0.3904'] | PCC: 0.3976 ['0.3976'] | RMSE: 0.4274 ['0.4274']
Epoch    33: reducing learning rate of group 0 to 2.5000e-03.
Epoch: 34 | Batch:   1 | Lr: 0.00250 | Time used(s): 135.6 | Training loss: 0.0551
Epoch: 34 | Batch:   2 | Lr: 0.00250 | Time used(s): 136.7 | Training loss: 0.0543
Epoch: 34 | Batch:   3 | Lr: 0.00250 | Time used(s): 134.9 | Training loss: 0.0510
Epoch: 34 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0764
--------------------------------------------------
Epoch: 34 | [Train] | Loss: 0.0539
Epoch: 34 |   [Val] | Loss: 0.0757 | [CCC]:  0.4134 [' 0.4134'] | PCC: 0.4135 ['0.4135'] | RMSE: 0.4451 ['0.4451']
Epoch: 35 | Batch:   1 | Lr: 0.00250 | Time used(s): 136.5 | Training loss: 0.0492
Epoch: 35 | Batch:   2 | Lr: 0.00250 | Time used(s): 136.7 | Training loss: 0.0488
Epoch: 35 | Batch:   3 | Lr: 0.00250 | Time used(s): 131.2 | Training loss: 0.0472
Epoch: 35 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0456
--------------------------------------------------
Epoch: 35 | [Train] | Loss: 0.0483
Epoch: 35 |   [Val] | Loss: 0.0788 | [CCC]:  0.4138 [' 0.4138'] | PCC: 0.4273 ['0.4273'] | RMSE: 0.4261 ['0.4261']
Epoch: 36 | Batch:   1 | Lr: 0.00250 | Time used(s): 136.1 | Training loss: 0.0526
Epoch: 36 | Batch:   2 | Lr: 0.00250 | Time used(s): 137.8 | Training loss: 0.0514
Epoch: 36 | Batch:   3 | Lr: 0.00250 | Time used(s): 139.3 | Training loss: 0.0444
Epoch: 36 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0469
--------------------------------------------------
Epoch: 36 | [Train] | Loss: 0.0494
Epoch: 36 |   [Val] | Loss: 0.0777 | [CCC]:  0.4082 [' 0.4082'] | PCC: 0.4089 ['0.4089'] | RMSE: 0.4342 ['0.4342']
Epoch: 37 | Batch:   1 | Lr: 0.00250 | Time used(s): 138.1 | Training loss: 0.0433
Epoch: 37 | Batch:   2 | Lr: 0.00250 | Time used(s): 128.2 | Training loss: 0.0496
Epoch: 37 | Batch:   3 | Lr: 0.00250 | Time used(s): 129.8 | Training loss: 0.0436
Epoch: 37 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0750
--------------------------------------------------
Epoch: 37 | [Train] | Loss: 0.0460
Epoch: 37 |   [Val] | Loss: 0.0784 | [CCC]:  0.4225 [' 0.4225'] | PCC: 0.4226 ['0.4226'] | RMSE: 0.4354 ['0.4354']
Epoch: 38 | Batch:   1 | Lr: 0.00250 | Time used(s): 136.6 | Training loss: 0.0426
Epoch: 38 | Batch:   2 | Lr: 0.00250 | Time used(s): 136.6 | Training loss: 0.0402
Epoch: 38 | Batch:   3 | Lr: 0.00250 | Time used(s): 135.6 | Training loss: 0.0405
Epoch: 38 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.0304
--------------------------------------------------
Epoch: 38 | [Train] | Loss: 0.0409
Epoch: 38 |   [Val] | Loss: 0.0811 | [CCC]:  0.4389 [' 0.4389'] | PCC: 0.4395 ['0.4395'] | RMSE: 0.4317 ['0.4317']
Epoch: 38 | Save best model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_333_None_None].pth"!
Epoch: 39 | Batch:   1 | Lr: 0.00250 | Time used(s): 140.1 | Training loss: 0.0368
Epoch: 39 | Batch:   2 | Lr: 0.00250 | Time used(s): 129.2 | Training loss: 0.0363
Epoch: 39 | Batch:   3 | Lr: 0.00250 | Time used(s): 127.8 | Training loss: 0.0488
Epoch: 39 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0039
--------------------------------------------------
Epoch: 39 | [Train] | Loss: 0.0401
Epoch: 39 |   [Val] | Loss: 0.0782 | [CCC]:  0.4264 [' 0.4264'] | PCC: 0.4432 ['0.4432'] | RMSE: 0.4279 ['0.4279']
Epoch: 40 | Batch:   1 | Lr: 0.00250 | Time used(s): 139.2 | Training loss: 0.0486
Epoch: 40 | Batch:   2 | Lr: 0.00250 | Time used(s): 132.8 | Training loss: 0.0403
Epoch: 40 | Batch:   3 | Lr: 0.00250 | Time used(s): 132.4 | Training loss: 0.0440
Epoch: 40 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0273
--------------------------------------------------
Epoch: 40 | [Train] | Loss: 0.0440
Epoch: 40 |   [Val] | Loss: 0.0838 | [CCC]:  0.4205 [' 0.4205'] | PCC: 0.4218 ['0.4218'] | RMSE: 0.4504 ['0.4504']
Epoch: 41 | Batch:   1 | Lr: 0.00250 | Time used(s): 139.6 | Training loss: 0.0383
Epoch: 41 | Batch:   2 | Lr: 0.00250 | Time used(s): 136.8 | Training loss: 0.0407
Epoch: 41 | Batch:   3 | Lr: 0.00250 | Time used(s): 136.1 | Training loss: 0.0419
Epoch: 41 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.3 | Training loss: 0.0065
--------------------------------------------------
Epoch: 41 | [Train] | Loss: 0.0398
Epoch: 41 |   [Val] | Loss: 0.0863 | [CCC]:  0.4097 [' 0.4097'] | PCC: 0.4111 ['0.4111'] | RMSE: 0.4604 ['0.4604']
Epoch: 42 | Batch:   1 | Lr: 0.00250 | Time used(s): 137.6 | Training loss: 0.0373
Epoch: 42 | Batch:   2 | Lr: 0.00250 | Time used(s): 127.4 | Training loss: 0.0419
Epoch: 42 | Batch:   3 | Lr: 0.00250 | Time used(s): 124.7 | Training loss: 0.0439
Epoch: 42 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0249
--------------------------------------------------
Epoch: 42 | [Train] | Loss: 0.0408
Epoch: 42 |   [Val] | Loss: 0.0818 | [CCC]:  0.4085 [' 0.4085'] | PCC: 0.4242 ['0.4242'] | RMSE: 0.4341 ['0.4341']
Epoch: 43 | Batch:   1 | Lr: 0.00250 | Time used(s): 140.8 | Training loss: 0.0358
Epoch: 43 | Batch:   2 | Lr: 0.00250 | Time used(s): 128.8 | Training loss: 0.0288
Epoch: 43 | Batch:   3 | Lr: 0.00250 | Time used(s): 128.5 | Training loss: 0.0313
Epoch: 43 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0291
--------------------------------------------------
Epoch: 43 | [Train] | Loss: 0.0319
Epoch: 43 |   [Val] | Loss: 0.0830 | [CCC]:  0.4214 [' 0.4214'] | PCC: 0.4375 ['0.4375'] | RMSE: 0.4320 ['0.4320']
Epoch: 44 | Batch:   1 | Lr: 0.00250 | Time used(s): 136.5 | Training loss: 0.0369
Epoch: 44 | Batch:   2 | Lr: 0.00250 | Time used(s): 135.5 | Training loss: 0.0244
Epoch: 44 | Batch:   3 | Lr: 0.00250 | Time used(s): 136.3 | Training loss: 0.0249
Epoch: 44 | Batch:   4 | Lr: 0.00250 | Time used(s): 2.2 | Training loss: 0.0333
--------------------------------------------------
Epoch: 44 | [Train] | Loss: 0.0288
Epoch: 44 |   [Val] | Loss: 0.0874 | [CCC]:  0.4180 [' 0.4180'] | PCC: 0.4353 ['0.4353'] | RMSE: 0.4430 ['0.4430']
Epoch    44: reducing learning rate of group 0 to 1.2500e-03.
Epoch: 45 | Batch:   1 | Lr: 0.00125 | Time used(s): 136.3 | Training loss: 0.0316
Epoch: 45 | Batch:   2 | Lr: 0.00125 | Time used(s): 131.5 | Training loss: 0.0328
Epoch: 45 | Batch:   3 | Lr: 0.00125 | Time used(s): 127.2 | Training loss: 0.0235
Epoch: 45 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0147
--------------------------------------------------
Epoch: 45 | [Train] | Loss: 0.0291
Epoch: 45 |   [Val] | Loss: 0.0812 | [CCC]:  0.4259 [' 0.4259'] | PCC: 0.4327 ['0.4327'] | RMSE: 0.4212 ['0.4212']
Epoch: 46 | Batch:   1 | Lr: 0.00125 | Time used(s): 133.7 | Training loss: 0.0280
Epoch: 46 | Batch:   2 | Lr: 0.00125 | Time used(s): 129.0 | Training loss: 0.0288
Epoch: 46 | Batch:   3 | Lr: 0.00125 | Time used(s): 133.5 | Training loss: 0.0249
Epoch: 46 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0499
--------------------------------------------------
Epoch: 46 | [Train] | Loss: 0.0276
Epoch: 46 |   [Val] | Loss: 0.0803 | [CCC]:  0.4328 [' 0.4328'] | PCC: 0.4448 ['0.4448'] | RMSE: 0.4335 ['0.4335']
Epoch: 47 | Batch:   1 | Lr: 0.00125 | Time used(s): 136.1 | Training loss: 0.0297
Epoch: 47 | Batch:   2 | Lr: 0.00125 | Time used(s): 136.3 | Training loss: 0.0249
Epoch: 47 | Batch:   3 | Lr: 0.00125 | Time used(s): 127.6 | Training loss: 0.0147
Epoch: 47 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0113
--------------------------------------------------
Epoch: 47 | [Train] | Loss: 0.0229
Epoch: 47 |   [Val] | Loss: 0.0824 | [CCC]:  0.4388 [' 0.4388'] | PCC: 0.4395 ['0.4395'] | RMSE: 0.4350 ['0.4350']
Epoch: 48 | Batch:   1 | Lr: 0.00125 | Time used(s): 139.3 | Training loss: 0.0240
Epoch: 48 | Batch:   2 | Lr: 0.00125 | Time used(s): 131.9 | Training loss: 0.0181
Epoch: 48 | Batch:   3 | Lr: 0.00125 | Time used(s): 129.9 | Training loss: 0.0168
Epoch: 48 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0343
--------------------------------------------------
Epoch: 48 | [Train] | Loss: 0.0199
Epoch: 48 |   [Val] | Loss: 0.0839 | [CCC]:  0.4236 [' 0.4236'] | PCC: 0.4334 ['0.4334'] | RMSE: 0.4217 ['0.4217']
Epoch: 49 | Batch:   1 | Lr: 0.00125 | Time used(s): 138.8 | Training loss: 0.0175
Epoch: 49 | Batch:   2 | Lr: 0.00125 | Time used(s): 132.5 | Training loss: 0.0128
Epoch: 49 | Batch:   3 | Lr: 0.00125 | Time used(s): 128.4 | Training loss: 0.0222
Epoch: 49 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.3 | Training loss: 0.0154
--------------------------------------------------
Epoch: 49 | [Train] | Loss: 0.0175
Epoch: 49 |   [Val] | Loss: 0.0856 | [CCC]:  0.4206 [' 0.4206'] | PCC: 0.4219 ['0.4219'] | RMSE: 0.4580 ['0.4580']
Epoch: 50 | Batch:   1 | Lr: 0.00125 | Time used(s): 139.9 | Training loss: 0.0202
Epoch: 50 | Batch:   2 | Lr: 0.00125 | Time used(s): 135.7 | Training loss: 0.0208
Epoch: 50 | Batch:   3 | Lr: 0.00125 | Time used(s): 125.5 | Training loss: 0.0092
Epoch: 50 | Batch:   4 | Lr: 0.00125 | Time used(s): 2.2 | Training loss: 0.0107
--------------------------------------------------
Epoch: 50 | [Train] | Loss: 0.0166
Epoch: 50 |   [Val] | Loss: 0.0814 | [CCC]:  0.4025 [' 0.4025'] | PCC: 0.4192 ['0.4192'] | RMSE: 0.4245 ['0.4245']
Epoch    50: reducing learning rate of group 0 to 6.2500e-04.
Epoch: 51 | Batch:   1 | Lr: 0.00063 | Time used(s): 138.0 | Training loss: 0.0190
Epoch: 51 | Batch:   2 | Lr: 0.00063 | Time used(s): 134.1 | Training loss: 0.0096
Epoch: 51 | Batch:   3 | Lr: 0.00063 | Time used(s): 127.0 | Training loss: 0.0153
Epoch: 51 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.2 | Training loss: 0.0122
--------------------------------------------------
Epoch: 51 | [Train] | Loss: 0.0146
Epoch: 51 |   [Val] | Loss: 0.0867 | [CCC]:  0.4188 [' 0.4188'] | PCC: 0.4196 ['0.4196'] | RMSE: 0.4318 ['0.4318']
Epoch: 52 | Batch:   1 | Lr: 0.00063 | Time used(s): 135.3 | Training loss: 0.0133
Epoch: 52 | Batch:   2 | Lr: 0.00063 | Time used(s): 127.9 | Training loss: 0.0081
Epoch: 52 | Batch:   3 | Lr: 0.00063 | Time used(s): 120.2 | Training loss: 0.0113
Epoch: 52 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.2 | Training loss: 0.0092
--------------------------------------------------
Epoch: 52 | [Train] | Loss: 0.0109
Epoch: 52 |   [Val] | Loss: 0.0898 | [CCC]:  0.4132 [' 0.4132'] | PCC: 0.4215 ['0.4215'] | RMSE: 0.4482 ['0.4482']
Epoch: 53 | Batch:   1 | Lr: 0.00063 | Time used(s): 140.4 | Training loss: 0.0174
Epoch: 53 | Batch:   2 | Lr: 0.00063 | Time used(s): 133.5 | Training loss: 0.0109
Epoch: 53 | Batch:   3 | Lr: 0.00063 | Time used(s): 136.0 | Training loss: 0.0025
Epoch: 53 | Batch:   4 | Lr: 0.00063 | Time used(s): 2.3 | Training loss: 0.0014
--------------------------------------------------
Epoch: 53 | [Train] | Loss: 0.0101
Epoch: 53 |   [Val] | Loss: 0.0888 | [CCC]:  0.4196 [' 0.4196'] | PCC: 0.4205 ['0.4205'] | RMSE: 0.4424 ['0.4424']
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 333 | Best [Val CCC]: 0.4389 [' 0.4389']| Loss: 0.0811 | PCC: 0.4395 ['0.4395'] | RMSE: 0.4317 ['0.4317']
On Test: CCC  0.5785 | PCC  0.5794 | RMSE  0.3942
==================================================
****************************************************************************************************
Seed "333" over!
****************************************************************************************************
On ground-truth labels:	Best	[Val CCC] for seed "321":	 0.4529
On ground-truth labels:		[Test CCC] for seed "321":	 0.5521
----------------------------------------------------------------------------------------------------
Predict val & test videos...
...done.
Delete model "MuSe-LSTM-Attention-baseline-model/output/model/2021-03-24-16-27_[bert-4]_[valence]_[NOSEG]_[lstm_64_1_True]_[True_1_4]_[0.005_1024_0.0_0.0_0.0]_None_[20_321_None_None].pth".
