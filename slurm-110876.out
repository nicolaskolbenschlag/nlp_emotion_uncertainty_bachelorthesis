MuSe-LSTM-Attention-baseline-model/emotion_recognition/main_uncertainty.py --feature_set bert-4 --emo_dim_set valence --epochs 100 --refresh --n_seeds 1 --seed 314 --predict --attn --rnn_bi --loss ccc --uncertainty_approach monte_carlo_dropout --attn_dr .5 --out_dr .5 --rnn_n_layers 2 --rnn_dr .5
Constructing dataset and data loader ...
Constructing data from scratch ...
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.6726, max 1.0).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -1.0, max 1.0).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.7353, max 0.80704).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.9549, max 1.0).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.9706, max 0.9218999999999999).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.39030000000000004, max 0.4687).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.98725, max 1.0).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.8137000000000001, max 1.0).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 9: 9
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 10: 10
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 11: 11
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 12: 12
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 13: 13
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 14: 14
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.58476, max 1.0).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 16: 16
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 17: 17
Samples in partitions: (3132, 62, 64)
Input feature dim: 768.
==================================================
Training model... [seed 314]
Note: target can not be optimized for 15 consecutive epochs, early stop the training process!
Seed 314 | Best [Val CCC]: 0.4312 [' 0.4312']| Loss: 0.6175 | PCC: 0.4429 ['0.4429'] | RMSE: 0.1844 ['0.1844']
(1402, 1)
torch.Size([1, 1402, 1])
torch.Size([1, 1402, 768])

(1538, 1)
torch.Size([1, 1538, 1])
torch.Size([1, 1538, 768])

(1706, 1)
torch.Size([1, 1706, 1])
torch.Size([1, 1706, 768])

(1053, 1)
torch.Size([1, 1053, 1])
torch.Size([1, 1053, 768])

(687, 1)
torch.Size([1, 687, 1])
torch.Size([1, 687, 768])

(796, 1)
torch.Size([1, 796, 1])
torch.Size([1, 796, 768])

(2670, 1)
torch.Size([1, 2670, 1])
torch.Size([1, 2670, 768])

(1725, 1)
torch.Size([1, 1725, 1])
torch.Size([1, 1725, 768])

(2321, 1)
torch.Size([1, 2321, 1])
torch.Size([1, 2321, 768])

(2107, 1)
torch.Size([1, 2107, 1])
torch.Size([1, 2107, 768])

(1592, 1)
torch.Size([1, 1592, 1])
torch.Size([1, 1592, 768])

(2153, 1)
torch.Size([1, 2153, 1])
torch.Size([1, 2153, 768])

(596, 1)
torch.Size([1, 596, 1])
torch.Size([1, 596, 768])

(2283, 1)
torch.Size([1, 2283, 1])
torch.Size([1, 2283, 768])

(2257, 1)
torch.Size([1, 2257, 1])
torch.Size([1, 2257, 768])

(2117, 1)
torch.Size([1, 2117, 1])
torch.Size([1, 2117, 768])

(1508, 1)
torch.Size([1, 1508, 1])
torch.Size([1, 1508, 768])

(1413, 1)
torch.Size([1, 1413, 1])
torch.Size([1, 1413, 768])

(1046, 1)
torch.Size([1, 1046, 1])
torch.Size([1, 1046, 768])

(1652, 1)
torch.Size([1, 1652, 1])
torch.Size([1, 1652, 768])

(1602, 1)
torch.Size([1, 1602, 1])
torch.Size([1, 1602, 768])

(2492, 1)
torch.Size([1, 2492, 1])
torch.Size([1, 2492, 768])

(1230, 1)
torch.Size([1, 1230, 1])
torch.Size([1, 1230, 768])

(585, 1)
torch.Size([1, 585, 1])
torch.Size([1, 585, 768])

(2189, 1)
torch.Size([1, 2189, 1])
torch.Size([1, 2189, 768])

(505, 1)
torch.Size([1, 505, 1])
torch.Size([1, 505, 768])

(1289, 1)
torch.Size([1, 1289, 1])
torch.Size([1, 1289, 768])

(1590, 1)
torch.Size([1, 1590, 1])
torch.Size([1, 1590, 768])

(2331, 1)
torch.Size([1, 2331, 1])
torch.Size([1, 2331, 768])

(1606, 1)
torch.Size([1, 1606, 1])
torch.Size([1, 1606, 768])

(772, 1)
torch.Size([1, 772, 1])
torch.Size([1, 772, 768])

(2066, 1)
torch.Size([1, 2066, 1])
torch.Size([1, 2066, 768])

(550, 1)
torch.Size([1, 550, 1])
torch.Size([1, 550, 768])

(598, 1)
torch.Size([1, 598, 1])
torch.Size([1, 598, 768])

(529, 1)
torch.Size([1, 529, 1])
torch.Size([1, 529, 768])

(1740, 1)
torch.Size([1, 1740, 1])
torch.Size([1, 1740, 768])

(1659, 1)
torch.Size([1, 1659, 1])
torch.Size([1, 1659, 768])

(2454, 1)
torch.Size([1, 2454, 1])
torch.Size([1, 2454, 768])

(1701, 1)
torch.Size([1, 1701, 1])
torch.Size([1, 1701, 768])

(1781, 1)
torch.Size([1, 1781, 1])
torch.Size([1, 1781, 768])

(1543, 1)
torch.Size([1, 1543, 1])
torch.Size([1, 1543, 768])

(723, 1)
torch.Size([1, 723, 1])
torch.Size([1, 723, 768])

(773, 1)
torch.Size([1, 773, 1])
torch.Size([1, 773, 768])

(618, 1)
torch.Size([1, 618, 1])
torch.Size([1, 618, 768])

(600, 1)
torch.Size([1, 600, 1])
torch.Size([1, 600, 768])

(1651, 1)
torch.Size([1, 1651, 1])
torch.Size([1, 1651, 768])

(2378, 1)
torch.Size([1, 2378, 1])
torch.Size([1, 2378, 768])

(481, 1)
torch.Size([1, 481, 1])
torch.Size([1, 481, 768])

(778, 1)
torch.Size([1, 778, 1])
torch.Size([1, 778, 768])

(788, 1)
torch.Size([1, 788, 1])
torch.Size([1, 788, 768])

(1265, 1)
torch.Size([1, 1265, 1])
torch.Size([1, 1265, 768])

(1799, 1)
torch.Size([1, 1799, 1])
torch.Size([1, 1799, 768])

(1339, 1)
torch.Size([1, 1339, 1])
torch.Size([1, 1339, 768])

(619, 1)
torch.Size([1, 619, 1])
torch.Size([1, 619, 768])

(332, 1)
torch.Size([1, 332, 1])
torch.Size([1, 332, 768])

(608, 1)
torch.Size([1, 608, 1])
torch.Size([1, 608, 768])

(1880, 1)
torch.Size([1, 1880, 1])
torch.Size([1, 1880, 768])

(606, 1)
torch.Size([1, 606, 1])
torch.Size([1, 606, 768])

(1568, 1)
torch.Size([1, 1568, 1])
torch.Size([1, 1568, 768])

(604, 1)
torch.Size([1, 604, 1])
torch.Size([1, 604, 768])

(1375, 1)
torch.Size([1, 1375, 1])
torch.Size([1, 1375, 768])

(1194, 1)
torch.Size([1, 1194, 1])
torch.Size([1, 1194, 768])

(948, 1)
torch.Size([1, 948, 1])
torch.Size([1, 948, 768])

(601, 1)
torch.Size([1, 601, 1])
torch.Size([1, 601, 768])

(1544, 1)
torch.Size([1, 1544, 1])
torch.Size([1, 1544, 768])

(991, 1)
torch.Size([1, 991, 1])
torch.Size([1, 991, 768])

(1574, 1)
torch.Size([1, 1574, 1])
torch.Size([1, 1574, 768])

(1356, 1)
torch.Size([1, 1356, 1])
torch.Size([1, 1356, 768])

(1068, 1)
torch.Size([1, 1068, 1])
torch.Size([1, 1068, 768])

(1542, 1)
torch.Size([1, 1542, 1])
torch.Size([1, 1542, 768])

(1268, 1)
torch.Size([1, 1268, 1])
torch.Size([1, 1268, 768])

(1603, 1)
torch.Size([1, 1603, 1])
torch.Size([1, 1603, 768])

(1356, 1)
torch.Size([1, 1356, 1])
torch.Size([1, 1356, 768])

(964, 1)
torch.Size([1, 964, 1])
torch.Size([1, 964, 768])

(1437, 1)
torch.Size([1, 1437, 1])
torch.Size([1, 1437, 768])

(1521, 1)
torch.Size([1, 1521, 1])
torch.Size([1, 1521, 768])

(1232, 1)
torch.Size([1, 1232, 1])
torch.Size([1, 1232, 768])

(1469, 1)
torch.Size([1, 1469, 1])
torch.Size([1, 1469, 768])

(739, 1)
torch.Size([1, 739, 1])
torch.Size([1, 739, 768])

(1531, 1)
torch.Size([1, 1531, 1])
torch.Size([1, 1531, 768])

(1099, 1)
torch.Size([1, 1099, 1])
torch.Size([1, 1099, 768])

(1272, 1)
torch.Size([1, 1272, 1])
torch.Size([1, 1272, 768])

(1052, 1)
torch.Size([1, 1052, 1])
torch.Size([1, 1052, 768])

(1231, 1)
torch.Size([1, 1231, 1])
torch.Size([1, 1231, 768])

(1516, 1)
torch.Size([1, 1516, 1])
torch.Size([1, 1516, 768])

(1300, 1)
torch.Size([1, 1300, 1])
torch.Size([1, 1300, 768])

(1078, 1)
torch.Size([1, 1078, 1])
torch.Size([1, 1078, 768])

(1193, 1)
torch.Size([1, 1193, 1])
torch.Size([1, 1193, 768])

(1075, 1)
torch.Size([1, 1075, 1])
torch.Size([1, 1075, 768])

(896, 1)
torch.Size([1, 896, 1])
torch.Size([1, 896, 768])

(1603, 1)
torch.Size([1, 1603, 1])
torch.Size([1, 1603, 768])

(1667, 1)
torch.Size([1, 1667, 1])
torch.Size([1, 1667, 768])

(1482, 1)
torch.Size([1, 1482, 1])
torch.Size([1, 1482, 768])

(1126, 1)
torch.Size([1, 1126, 1])
torch.Size([1, 1126, 768])

(1227, 1)
torch.Size([1, 1227, 1])
torch.Size([1, 1227, 768])

(1026, 1)
torch.Size([1, 1026, 1])
torch.Size([1, 1026, 768])

(1077, 1)
torch.Size([1, 1077, 1])
torch.Size([1, 1077, 768])

(1099, 1)
torch.Size([1, 1099, 1])
torch.Size([1, 1099, 768])

(631, 1)
torch.Size([1, 631, 1])
torch.Size([1, 631, 768])

(1455, 1)
torch.Size([1, 1455, 1])
torch.Size([1, 1455, 768])

(1309, 1)
torch.Size([1, 1309, 1])
torch.Size([1, 1309, 768])

(1095, 1)
torch.Size([1, 1095, 1])
torch.Size([1, 1095, 768])

(2013, 1)
torch.Size([1, 2013, 1])
torch.Size([1, 2013, 768])

(2812, 1)
torch.Size([1, 2812, 1])
torch.Size([1, 2812, 768])

(2378, 1)
torch.Size([1, 2378, 1])
torch.Size([1, 2378, 768])

(2238, 1)
torch.Size([1, 2238, 1])
torch.Size([1, 2238, 768])

(491, 1)
torch.Size([1, 491, 1])
torch.Size([1, 491, 768])

(1613, 1)
torch.Size([1, 1613, 1])
torch.Size([1, 1613, 768])

(3802, 1)
torch.Size([1, 3802, 1])
torch.Size([1, 3802, 768])

(4090, 1)
torch.Size([1, 4090, 1])
torch.Size([1, 4090, 768])

(1610, 1)
torch.Size([1, 1610, 1])
torch.Size([1, 1610, 768])

(1793, 1)
torch.Size([1, 1793, 1])
torch.Size([1, 1793, 768])

(1337, 1)
torch.Size([1, 1337, 1])
torch.Size([1, 1337, 768])

(2233, 1)
torch.Size([1, 2233, 1])
torch.Size([1, 2233, 768])

(4972, 1)
torch.Size([1, 4972, 1])
torch.Size([1, 4972, 768])

(2200, 1)
torch.Size([1, 2200, 1])
torch.Size([1, 2200, 768])

(858, 1)
torch.Size([1, 858, 1])
torch.Size([1, 858, 768])

(2605, 1)
torch.Size([1, 2605, 1])
torch.Size([1, 2605, 768])

(2483, 1)
torch.Size([1, 2483, 1])
torch.Size([1, 2483, 768])

(1212, 1)
torch.Size([1, 1212, 1])
torch.Size([1, 1212, 768])

(1872, 1)
torch.Size([1, 1872, 1])
torch.Size([1, 1872, 768])

(1968, 1)
torch.Size([1, 1968, 1])
torch.Size([1, 1968, 768])

(1187, 1)
torch.Size([1, 1187, 1])
torch.Size([1, 1187, 768])

(1909, 1)
torch.Size([1, 1909, 1])
torch.Size([1, 1909, 768])

(1899, 1)
torch.Size([1, 1899, 1])
torch.Size([1, 1899, 768])

(1059, 1)
torch.Size([1, 1059, 1])
torch.Size([1, 1059, 768])

On Test (uncal.): sbUME 0.3023 | pebUME(3) 0.4214 | pebUME(5) 0.4051 | pebUME(7) 0.4101 | pebUME(10) 0.4210 | Cv 0.3143
On Test (cal.): sbUME 0.4662 | pebUME(3) 0.5464 | pebUME(5) 0.5608 | pebUME(7) 0.5726 | pebUME(10) 0.5864 | Cv 0.0479
On Test: CCC  0.6050 | PCC  0.6196 | RMSE  0.1611
==================================================
On ground-truth labels:	Best	[Val CCC] for seed "314":	 0.4312
On ground-truth labels:		[Test CCC] for seed "314":	 0.6050
----------------------------------------------------------------------------------------------------
Predict val & test videos...
...done.
Delete model "MuSe-LSTM-Attention-baseline-model/output/model/2021-05-15-19-40_[bert-4]_[valence]_[NOSEG]_[lstm_64_2_True]_[True_1_4]_[0.005_1024_0.5_0.5_0.5]_None_[1_314_None_None].pth".
MuSe-LSTM-Attention-baseline-model/emotion_recognition/main_uncertainty.py --feature_set bert-4 --emo_dim_set valence --epochs 100 --refresh --n_seeds 2 --seed 314 --predict --attn --rnn_bi --uncertainty_approach quantile_regression --loss tiltedCCC
Constructing dataset and data loader ...
Constructing data from scratch ...
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.6726, max 1.0).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -1.0, max 1.0).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.7353, max 0.80704).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.9549, max 1.0).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.9706, max 0.9218999999999999).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.39030000000000004, max 0.4687).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.98725, max 1.0).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.8137000000000001, max 1.0).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 9: 9
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 10: 10
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 11: 11
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 12: 12
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 13: 13
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 14: 14
Found cached annotator 2 video mapping.
Constructing data from scratch ...
No label preprocessing (min -0.58476, max 1.0).
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 16: 16
Found cached annotator 2 video mapping.
Constructing data from scratch ...
Exception for annotator 17: 17
Samples in partitions: (3132, 62, 64)
Input feature dim: 768.
==================================================
Training model... [seed 314]
Traceback (most recent call last):
  File "MuSe-LSTM-Attention-baseline-model/emotion_recognition/main_uncertainty.py", line 327, in <module>
    main(params)
  File "MuSe-LSTM-Attention-baseline-model/emotion_recognition/main_uncertainty.py", line 210, in main
    train_model(model, data_loader, params)
  File "/nas/student/NicolasKolbenschlag/emotion_uncertainty_bachelorarbeit/MuSe-LSTM-Attention-baseline-model/emotion_recognition/train.py", line 48, in train_model
    train_loss = train(model, train_loader, criterion, optimizer, epoch, params)
  File "/nas/student/NicolasKolbenschlag/emotion_uncertainty_bachelorarbeit/MuSe-LSTM-Attention-baseline-model/emotion_recognition/train.py", line 119, in train
    branch_loss = criterion(preds, labels[:, :, i], feature_lens, params.label_smooth)
  File "/nas/student/NicolasKolbenschlag/emotion_uncertainty_bachelorarbeit/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nas/student/NicolasKolbenschlag/emotion_uncertainty_bachelorarbeit/MuSe-LSTM-Attention-baseline-model/emotion_recognition/utils.py", line 602, in forward
    rolling_correlation = torch.tensor([uncertainty_utilities.rolling_correlation_coefficient(yt, yp[:,i], window) for yt, yp in zip(y_true, y_pred)], dtype="float")
  File "/nas/student/NicolasKolbenschlag/emotion_uncertainty_bachelorarbeit/MuSe-LSTM-Attention-baseline-model/emotion_recognition/utils.py", line 602, in <listcomp>
    rolling_correlation = torch.tensor([uncertainty_utilities.rolling_correlation_coefficient(yt, yp[:,i], window) for yt, yp in zip(y_true, y_pred)], dtype="float")
  File "/nas/student/NicolasKolbenschlag/emotion_uncertainty_bachelorarbeit/MuSe-LSTM-Attention-baseline-model/emotion_recognition/uncertainty_utilities.py", line 267, in rolling_correlation_coefficient
    for i in range(rolling_window, len(y_true) + 1)
  File "/nas/student/NicolasKolbenschlag/emotion_uncertainty_bachelorarbeit/MuSe-LSTM-Attention-baseline-model/emotion_recognition/uncertainty_utilities.py", line 267, in <listcomp>
    for i in range(rolling_window, len(y_true) + 1)
  File "/nas/student/NicolasKolbenschlag/emotion_uncertainty_bachelorarbeit/venv/lib/python3.7/site-packages/pandas/core/series.py", line 327, in __init__
    data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True)
  File "/nas/student/NicolasKolbenschlag/emotion_uncertainty_bachelorarbeit/venv/lib/python3.7/site-packages/pandas/core/construction.py", line 463, in sanitize_array
    subarr = _try_cast(data, dtype, copy, raise_cast_failure)
  File "/nas/student/NicolasKolbenschlag/emotion_uncertainty_bachelorarbeit/venv/lib/python3.7/site-packages/pandas/core/construction.py", line 558, in _try_cast
    subarr = maybe_cast_to_datetime(arr, dtype)
  File "/nas/student/NicolasKolbenschlag/emotion_uncertainty_bachelorarbeit/venv/lib/python3.7/site-packages/pandas/core/dtypes/cast.py", line 1443, in maybe_cast_to_datetime
    value = maybe_infer_to_datetimelike(value)
  File "/nas/student/NicolasKolbenschlag/emotion_uncertainty_bachelorarbeit/venv/lib/python3.7/site-packages/pandas/core/dtypes/cast.py", line 1231, in maybe_infer_to_datetimelike
    v = np.array(v, copy=False)
  File "/nas/student/NicolasKolbenschlag/emotion_uncertainty_bachelorarbeit/venv/lib/python3.7/site-packages/torch/tensor.py", line 630, in __array__
    return self.numpy()
RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.
slurmstepd: Exceeded step memory limit at some point.
